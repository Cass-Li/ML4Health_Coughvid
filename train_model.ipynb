{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaf48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub # faster than librosa\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "import time\n",
    "\n",
    "from pytorch.coughvid_dataset import CoughvidDataset\n",
    "from pytorch.test_coughvid_dataset import TestCoughvidDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.models import resnet50, resnet18\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd6fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask file C:/COUGHVID_public_dataset/public_dataset/ does not exist. Calculating masks on the fly.\n",
      "8997 records ready to load across 2 groups.\n",
      "699 samples in the minority class.\n"
     ]
    }
   ],
   "source": [
    "dir = 'C:/COUGHVID_public_dataset/public_dataset/'\n",
    "\n",
    "full_dataset = CoughvidDataset(dir, 'metadata_compiled.csv', get_features=True)\n",
    "\n",
    "dataframe = full_dataset.dataframe\n",
    "\n",
    "minority_class_count = len(dataframe[dataframe['status']==2])\n",
    "\n",
    "print(f'{minority_class_count} samples in the minority class.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef46aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask file C:/COUGHVID_public_dataset/public_dataset/ does not exist. Calculating masks on the fly.\n",
      "800 records ready to load across 2 groups.\n"
     ]
    }
   ],
   "source": [
    "sample_dataset = CoughvidDataset(dir, 'metadata_compiled.csv', get_features=True, samples_per_class=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf60937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test samples\n",
    "train_indices, test_indices = train_test_split(np.arange(0,len(sample_dataset)-1), test_size=0.25)\n",
    "\n",
    "batch_size  = 1\n",
    "num_workers = 2\n",
    "\n",
    "train_loader  = DataLoader(sample_dataset, \n",
    "                           num_workers=num_workers,\n",
    "                           sampler=SubsetRandomSampler(train_indices)\n",
    "                          )\n",
    "\n",
    "test_loader   = DataLoader(sample_dataset, \n",
    "                           num_workers=num_workers,\n",
    "                           sampler=SubsetRandomSampler(test_indices)\n",
    "                          )\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a011ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and change output shape for binary prediction\n",
    "model = resnet18()\n",
    "\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(\n",
    "        in_features=512, #2048 for resnet50\n",
    "        out_features=1\n",
    "    ),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=True)\n",
    "\n",
    "model.double()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa02e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=4050, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "EPOCHS = int(n_iters / (len(sample_dataset) / batch_size))\n",
    "\n",
    "lr_rate = 0.00001    \n",
    "    \n",
    "model = LogisticRegression(4050,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "criterion = torch.nn.BCELoss()\n",
    "model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16812b10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7916]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.9275]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.9908]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.8096]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8780]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.9287]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7076]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3364]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.1472]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.1972]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3574]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5835]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7704]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8747]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.8635]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8644]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8738]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8881]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.8532]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8283]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8168]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8185]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7696]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7386]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6561]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6079]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5981]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6215]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5938]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6004]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6350]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6883]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6890]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6465]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5643]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4527]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3331]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2307]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.1950]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.1604]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.1632]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.1596]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.1865]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2462]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3445]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4776]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5664]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6627]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7040]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7506]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "1:50 - loss: 0.9272177331408951, acc: 0.54\n",
      "tensor([[0.7558]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7287]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7159]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6729]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6487]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6436]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6094]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5499]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5183]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4657]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3988]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3257]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2552]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2271]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2288]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2560]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3088]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3880]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4888]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5577]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5933]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5996]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6215]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6552]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6594]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6758]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7019]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7342]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7373]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7153]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6692]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6001]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5119]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4139]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3541]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3270]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2932]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2561]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2186]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.1834]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.1743]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.1857]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.1918]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2183]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2675]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3419]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4402]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5131]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5566]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5728]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "1:100 - loss: 0.9016122013070244, acc: 0.47\n",
      "tensor([[0.6038]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6452]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6572]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6798]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6757]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6836]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7012]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7263]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7559]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7872]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7923]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7758]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7378]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6770]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5935]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5297]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4897]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4728]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4412]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3983]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3815]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3877]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4141]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4239]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4530]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4988]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5576]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6238]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6595]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7004]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7143]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7058]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7067]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7161]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7323]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7262]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7283]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7378]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7259]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7227]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6991]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6861]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6837]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6605]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6498]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6192]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5702]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5396]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4936]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4695]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "1:150 - loss: 0.8548711975419627, acc: 0.48\n",
      "tensor([[0.4324]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3857]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3646]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3657]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3866]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4257]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4475]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4856]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5039]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5367]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5483]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5734]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5773]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5943]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5904]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6001]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5896]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5932]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6094]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6046]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6128]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6008]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6026]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5850]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5824]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5612]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5562]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5662]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5568]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5300]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5213]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5291]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5188]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4924]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4853]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4959]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4891]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4670]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4645]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4799]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4779]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4604]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4624]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4819]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5166]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5308]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5589]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5657]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5859]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6169]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "1:200 - loss: 0.822744242264378, acc: 0.475\n",
      "tensor([[0.6249]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6436]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6406]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6489]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6365]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6365]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6166]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6106]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5857]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5763]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5816]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5997]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5967]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5749]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5688]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5447]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5379]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5140]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5085]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4868]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4841]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4988]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5285]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5705]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6209]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6456]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6478]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6300]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6253]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6013]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5601]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5366]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5305]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5404]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5316]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5064]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4672]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4494]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4192]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3796]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3638]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3688]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3927]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4338]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4898]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5567]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5980]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6461]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6684]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6972]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "1:250 - loss: 0.8087710619696075, acc: 0.46\n",
      "tensor([[0.7030]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7162]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7089]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7101]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7188]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7076]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6777]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6296]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5955]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5453]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4820]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4419]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3930]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3394]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3128]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2817]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2485]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2384]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2472]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2493]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2694]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3076]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3353]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3515]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3855]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4052]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4107]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4339]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4727]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5247]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5546]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5949]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6420]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6921]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7156]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7429]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7719]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8008]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.8083]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8187]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.8117]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8092]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8108]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7956]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7637]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7135]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6718]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6414]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6238]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6187]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "1:300 - loss: 0.8020636107195943, acc: 0.47\n",
      "tensor([[0.5959]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5873]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5918]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5782]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5483]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5043]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4799]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4435]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4280]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4313]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4214]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4297]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4243]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4367]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4650]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4764]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4721]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4844]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4806]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4931]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5197]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5581]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6049]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6567]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6829]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6877]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6737]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6698]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6479]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6377]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6104]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5675]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5410]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5307]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5056]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4978]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5058]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5277]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5317]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5194]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4929]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4546]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4364]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4077]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3711]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3297]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2864]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2443]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2257]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2255]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "1:350 - loss: 0.7920016008773717, acc: 0.4742857142857143\n",
      "tensor([[0.2207]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2324]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2602]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3047]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3400]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3915]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4287]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4795]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5114]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5543]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5761]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5789]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5646]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5351]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5223]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4954]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4862]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4637]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4595]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4716]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4686]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4521]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4242]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4160]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4256]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4510]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4901]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5400]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5687]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6060]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6217]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6184]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5982]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5627]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5430]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5385]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5187]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4859]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4715]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4453]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4378]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4471]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4427]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4546]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4810]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5197]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5395]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5703]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5815]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6032]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "1:400 - loss: 0.7905318986101486, acc: 0.4575\n",
      "tensor([[0.6330]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6686]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7071]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7461]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7624]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7813]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7819]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7869]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7954]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8066]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8198]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8341]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8326]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8341]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8381]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.8279]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.8214]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.8003]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7635]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7322]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7085]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6682]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6391]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6222]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6172]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6235]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6394]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6361]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6429]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6586]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6550]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6342]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6252]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6273]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6118]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5806]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5639]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5613]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5715]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5926]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5949]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5802]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5504]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5075]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4830]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4474]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4314]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4330]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4504]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4534]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "1:450 - loss: 0.7799212299553491, acc: 0.47555555555555556\n",
      "tensor([[0.4433]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4499]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4432]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4528]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4487]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4325]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4338]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4229]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4015]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3988]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3856]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3636]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3350]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3267]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3113]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3146]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3347]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3707]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4215]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4569]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5043]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5326]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5709]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5890]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6162]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6237]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6137]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5881]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5760]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5767]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5615]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5320]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5185]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5198]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5345]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5328]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5443]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5394]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5199]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4877]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4452]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4228]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4187]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4036]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4062]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3976]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4061]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4299]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4673]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5160]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "1:500 - loss: 0.7767813416329024, acc: 0.47\n",
      "tensor([[0.5454]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5838]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6021]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6287]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6358]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6514]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6738]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7010]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7310]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7409]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7335]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7327]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7155]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7062]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6808]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6396]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6104]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5667]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5107]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4740]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4280]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4034]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3708]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3329]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2924]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2522]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2352]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2366]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2540]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.2872]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3363]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3741]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3987]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4098]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4365]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.4768]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5284]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5590]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5700]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5633]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5413]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5347]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5424]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5623]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5643]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.5783]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6024]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6344]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.6716]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7113]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "1:550 - loss: 0.7738022320178887, acc: 0.4763636363636364\n",
      "tensor([[0.7509]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7675]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7658]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7692]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7770]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7680]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7648]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7668]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7734]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7838]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.7970]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7933]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7743]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.7396]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6882]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.6200]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.5375]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.4471]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3846]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.3477]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.3074]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2664]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.2272]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.1913]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.1769]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n",
      "tensor([[0.1790]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([0], dtype=torch.int32)\n",
      "tensor([[0.1775]], dtype=torch.float64, grad_fn=<SigmoidBackward0>) tensor([1], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17416/3882819675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"train\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''RESNET training code adapted from https://www.kaggle.com/gxkok21/resnet50-with-pytorch'''\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    model = model.cuda()\n",
    "#    print('Using GPU.')\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        samples = 0\n",
    "        loss_sum = 0\n",
    "        correct_sum = 0\n",
    "        for j, batch in enumerate(dataloaders[phase]):\n",
    "            X,labels = batch\n",
    "            #print(labels)\n",
    "            #print(len(X),len(X[0]),len(X[1]))\n",
    "            #print(labels[None,...].double())\n",
    "            #X = torch.Tensor(X)\n",
    "            #print(X.shape)\n",
    "            #labels = 1-labels\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                y = model(X[None,...].double())\n",
    "                #y = model(X.flatten().double())\n",
    "                print(y,labels)\n",
    "                loss = criterion(\n",
    "                    y,\n",
    "                    #torch.clip(y,0,1), \n",
    "                    labels[None,...].double()\n",
    "                    #labels.double()\n",
    "                )\n",
    "                #print(loss.item())\n",
    "                #print(labels[None,...].double())\n",
    "                \n",
    "                #writer.add_scalar(\"Loss/train\", loss, i)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                loss_sum += loss.item() * X.shape[0] # We need to multiple by batch size as loss is the mean loss of the samples in the batch\n",
    "                samples += X.shape[0]\n",
    "                num_corrects = torch.sum((y >= 0.5).float() == labels[0].float())\n",
    "                correct_sum += num_corrects\n",
    "                \n",
    "                # Print batch statistics every 50 batches\n",
    "                if j % 50 == 49 and phase == \"train\":\n",
    "                    print(\"{}:{} - loss: {}, acc: {}\".format(\n",
    "                        i + 1, \n",
    "                        j + 1, \n",
    "                        float(loss_sum) / float(samples), \n",
    "                        float(correct_sum) / float(samples)\n",
    "                    ))\n",
    "                \n",
    "        # Print epoch statistics\n",
    "        epoch_acc = float(correct_sum) / float(samples)\n",
    "        epoch_loss = float(loss_sum) / float(samples)\n",
    "        print(\"epoch: {} - {} loss: {}, {} acc: {}\".format(i + 1, phase, epoch_loss, phase, epoch_acc))\n",
    "        \n",
    "        # Deep copy the model\n",
    "        if phase == \"test\" and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, \"resnet50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cd7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

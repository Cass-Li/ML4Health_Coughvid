{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaf48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub # faster than librosa\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "import time\n",
    "\n",
    "from coughvid.pytorch.coughvid_dataset import CoughvidDataset\n",
    "from coughvid.pytorch.coughvid_dataset import CoughvidDataset\n",
    "from coughvid.pytorch.coswara_dataset import CoswaraDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.models import resnet50, resnet18\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "import wandb\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd6fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask file C:/COUGHVID_public_dataset/public_dataset/ does not exist. Calculating masks on the fly.\n",
      "8997 records ready to load across 2 groups.\n",
      "699 samples in the minority class.\n",
      "Mask file C:/COUGHVID_public_dataset/public_dataset/ does not exist. Calculating masks on the fly.\n",
      "1398 records ready to load across 2 groups.\n"
     ]
    }
   ],
   "source": [
    "dir = 'C:/COUGHVID_public_dataset/public_dataset/'\n",
    "\n",
    "full_dataset = CoughvidDataset(dir, 'metadata_compiled.csv', get_features=True)\n",
    "\n",
    "dataframe = full_dataset.dataframe\n",
    "\n",
    "minority_class_count = len(dataframe[dataframe['status']==1])\n",
    "\n",
    "print(f'{minority_class_count} samples in the minority class.')\n",
    "\n",
    "sample_dataset = CoughvidDataset(dir, 'metadata_compiled.csv', get_features=True, samples_per_class=minority_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef46aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1731 records ready to load across 2 groups.\n",
      "378 samples in the minority class.\n",
      "756 records ready to load across 2 groups.\n"
     ]
    }
   ],
   "source": [
    "dir = './data/coswara/'\n",
    "\n",
    "full_dataset = CoswaraDataset(dir, 'filtered_data.csv', get_features=True)\n",
    "\n",
    "dataframe = full_dataset.dataframe\n",
    "\n",
    "minority_class_count = len(dataframe[dataframe['covid_status']==1])\n",
    "\n",
    "print(f'{minority_class_count} samples in the minority class.')\n",
    "\n",
    "sample_dataset = CoswaraDataset(dir, 'filtered_data.csv', get_features=True, samples_per_class=minority_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf60937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test samples\n",
    "train_indices, test_indices = train_test_split(np.arange(0,len(sample_dataset)-1), test_size=0.25)\n",
    "\n",
    "batch_size  = 1\n",
    "num_workers = 2\n",
    "\n",
    "train_loader  = DataLoader(sample_dataset, \n",
    "                           num_workers=num_workers,\n",
    "                           sampler=SubsetRandomSampler(train_indices)\n",
    "                          )\n",
    "\n",
    "test_loader   = DataLoader(sample_dataset, \n",
    "                           num_workers=num_workers,\n",
    "                           sampler=SubsetRandomSampler(test_indices)\n",
    "                          )\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a011ecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=True)\n",
       "    (1): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model and change output shape for binary prediction\n",
    "model = resnet18()\n",
    "\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.10,inplace=True),\n",
    "    torch.nn.Linear(\n",
    "        in_features=512, #2048 for resnet50\n",
    "        out_features=1\n",
    "    ),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=True)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa02e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=4050, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "EPOCHS = int(n_iters / (len(sample_dataset) / batch_size))\n",
    "\n",
    "lr_rate = 0.00001    \n",
    "    \n",
    "model = LogisticRegression(4050,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "criterion = torch.nn.BCELoss()\n",
    "model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16812b10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Filip/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/filipmiscevic/uncategorized/runs/1tntbrye\" target=\"_blank\">still-bird-1</a></strong> to <a href=\"https://wandb.ai/filipmiscevic/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:50 - loss: 0.8624618969590682, acc: 0.48\n",
      "1:100 - loss: 0.8373175373250368, acc: 0.51\n",
      "1:150 - loss: 0.7727786192988627, acc: 0.5866666666666667\n",
      "1:200 - loss: 0.7546751775830042, acc: 0.61\n",
      "1:250 - loss: 0.7561352259109202, acc: 0.592\n",
      "1:300 - loss: 0.7497283396721769, acc: 0.5833333333333334\n",
      "1:350 - loss: 0.7458537453787911, acc: 0.58\n",
      "1:400 - loss: 0.7447015140363002, acc: 0.575\n",
      "1:450 - loss: 0.7413862589910749, acc: 0.5755555555555556\n",
      "1:500 - loss: 0.7436514174517306, acc: 0.568\n",
      "1:550 - loss: 0.7357599998003134, acc: 0.5672727272727273\n",
      "epoch: 1 - train loss: 0.7389071931930815, train acc: 0.5653710247349824\n",
      "epoch: 1 - test loss: 0.698726289652009, test acc: 0.5396825396825397\n",
      "2:50 - loss: 0.7100536460617322, acc: 0.62\n",
      "2:100 - loss: 0.6979814334310923, acc: 0.59\n",
      "2:150 - loss: 0.7031320844274089, acc: 0.5666666666666667\n",
      "2:200 - loss: 0.7151235594452889, acc: 0.555\n",
      "2:250 - loss: 0.7172521625923268, acc: 0.568\n",
      "2:300 - loss: 0.7118804708077459, acc: 0.5733333333333334\n",
      "2:350 - loss: 0.7203773946623435, acc: 0.56\n",
      "2:400 - loss: 0.7231682804132943, acc: 0.565\n",
      "2:450 - loss: 0.7249175890727978, acc: 0.5577777777777778\n",
      "2:500 - loss: 0.7259377046675735, acc: 0.554\n",
      "2:550 - loss: 0.7180012308351753, acc: 0.5636363636363636\n",
      "epoch: 2 - train loss: 0.714365086162253, train acc: 0.5671378091872792\n",
      "epoch: 2 - test loss: 0.9570818736760939, test acc: 0.5291005291005291\n",
      "3:50 - loss: 0.7007276077407478, acc: 0.56\n",
      "3:100 - loss: 0.7028514415510201, acc: 0.58\n",
      "3:150 - loss: 0.708419823744663, acc: 0.5333333333333333\n",
      "3:200 - loss: 0.7063033845309987, acc: 0.52\n",
      "3:250 - loss: 0.7092636806059919, acc: 0.5\n",
      "3:300 - loss: 0.7157425092400882, acc: 0.5\n",
      "3:350 - loss: 0.7163982682511508, acc: 0.5085714285714286\n",
      "3:400 - loss: 0.7074781305132845, acc: 0.535\n",
      "3:450 - loss: 0.7007186321138344, acc: 0.54\n",
      "3:500 - loss: 0.6989366968060939, acc: 0.548\n",
      "3:550 - loss: 0.6974719643458608, acc: 0.5527272727272727\n",
      "epoch: 3 - train loss: 0.6952708378232965, train acc: 0.5565371024734982\n",
      "epoch: 3 - test loss: 0.6584519033848089, test acc: 0.5608465608465608\n",
      "4:50 - loss: 0.8367008353962737, acc: 0.46\n",
      "4:100 - loss: 0.7497295490338943, acc: 0.53\n",
      "4:150 - loss: 0.6967065657657735, acc: 0.56\n",
      "4:200 - loss: 0.6838413655312013, acc: 0.585\n",
      "4:250 - loss: 0.6846761618556501, acc: 0.588\n",
      "4:300 - loss: 0.68441526995308, acc: 0.5966666666666667\n",
      "4:350 - loss: 0.6857151308014147, acc: 0.6028571428571429\n",
      "4:400 - loss: 0.6742862655134276, acc: 0.6075\n",
      "4:450 - loss: 0.6647449301490328, acc: 0.6155555555555555\n",
      "4:500 - loss: 0.6714405586178464, acc: 0.608\n",
      "4:550 - loss: 0.6782322637274738, acc: 0.6054545454545455\n",
      "epoch: 4 - train loss: 0.6764744616774899, train acc: 0.6095406360424028\n",
      "epoch: 4 - test loss: 0.6801939469371338, test acc: 0.5661375661375662\n",
      "5:50 - loss: 0.6432875538394511, acc: 0.68\n",
      "5:100 - loss: 0.6054929058007652, acc: 0.72\n",
      "5:150 - loss: 0.5959805571876812, acc: 0.6866666666666666\n",
      "5:200 - loss: 0.5981738937252962, acc: 0.68\n",
      "5:250 - loss: 0.6041725838535963, acc: 0.664\n",
      "5:300 - loss: 0.6346800852837078, acc: 0.6266666666666667\n",
      "5:350 - loss: 0.6377407176667942, acc: 0.6285714285714286\n",
      "5:400 - loss: 0.645910499252688, acc: 0.6225\n",
      "5:450 - loss: 0.6454588322006177, acc: 0.6266666666666667\n",
      "5:500 - loss: 0.6379097412448432, acc: 0.638\n",
      "5:550 - loss: 0.6330394954253981, acc: 0.6454545454545455\n",
      "epoch: 5 - train loss: 0.6265870581318839, train acc: 0.6519434628975265\n",
      "epoch: 5 - test loss: 0.6417142220354795, test acc: 0.6507936507936508\n",
      "6:50 - loss: 0.5325549887147626, acc: 0.7\n",
      "6:100 - loss: 0.6298869052172388, acc: 0.68\n",
      "6:150 - loss: 0.6055143964406731, acc: 0.7\n",
      "6:200 - loss: 0.6054403626383039, acc: 0.71\n",
      "6:250 - loss: 0.5971318880977547, acc: 0.716\n",
      "6:300 - loss: 0.5913398871507297, acc: 0.7133333333333334\n",
      "6:350 - loss: 0.5941768810108956, acc: 0.7028571428571428\n",
      "6:400 - loss: 0.597670658074237, acc: 0.69\n",
      "6:450 - loss: 0.6010923074924261, acc: 0.6844444444444444\n",
      "6:500 - loss: 0.6082344325119111, acc: 0.678\n",
      "6:550 - loss: 0.6059727759801079, acc: 0.6854545454545454\n",
      "epoch: 6 - train loss: 0.6054526958806867, train acc: 0.6837455830388692\n",
      "epoch: 6 - test loss: 0.6765754791693773, test acc: 0.5978835978835979\n",
      "7:50 - loss: 0.5861657539707282, acc: 0.66\n",
      "7:100 - loss: 0.605404013848455, acc: 0.68\n",
      "7:150 - loss: 0.6105958095836687, acc: 0.66\n",
      "7:200 - loss: 0.5982828346087665, acc: 0.66\n",
      "7:250 - loss: 0.5668592330690976, acc: 0.688\n",
      "7:300 - loss: 0.575763998851443, acc: 0.6866666666666666\n",
      "7:350 - loss: 0.5663928450552876, acc: 0.7\n",
      "7:400 - loss: 0.5715173285549021, acc: 0.7\n",
      "7:450 - loss: 0.5693996898776892, acc: 0.6955555555555556\n",
      "7:500 - loss: 0.569647948275465, acc: 0.698\n",
      "7:550 - loss: 0.558432594136303, acc: 0.7018181818181818\n",
      "epoch: 7 - train loss: 0.5588943926847578, train acc: 0.7031802120141343\n",
      "epoch: 7 - test loss: 0.6439349714247069, test acc: 0.6243386243386243\n",
      "8:50 - loss: 0.5690931927355862, acc: 0.68\n",
      "8:100 - loss: 0.5576392062616565, acc: 0.7\n",
      "8:150 - loss: 0.5451982496711693, acc: 0.7333333333333333\n",
      "8:200 - loss: 0.5372432576237005, acc: 0.74\n",
      "8:250 - loss: 0.5327433454330236, acc: 0.744\n",
      "8:300 - loss: 0.5226518535510991, acc: 0.7566666666666667\n",
      "8:350 - loss: 0.5405771911057952, acc: 0.7428571428571429\n",
      "8:400 - loss: 0.54824867597432, acc: 0.735\n",
      "8:450 - loss: 0.5568095308853731, acc: 0.7311111111111112\n",
      "8:500 - loss: 0.5400008107881106, acc: 0.744\n",
      "8:550 - loss: 0.527272293694312, acc: 0.7545454545454545\n",
      "epoch: 8 - train loss: 0.5288588553946132, train acc: 0.7544169611307421\n",
      "epoch: 8 - test loss: 0.6631394586696164, test acc: 0.582010582010582\n",
      "9:50 - loss: 0.5314521746940406, acc: 0.72\n",
      "9:100 - loss: 0.42298757346173615, acc: 0.81\n",
      "9:150 - loss: 0.48306601604742494, acc: 0.7666666666666667\n",
      "9:200 - loss: 0.496587101224757, acc: 0.77\n",
      "9:250 - loss: 0.4829481555126729, acc: 0.776\n",
      "9:300 - loss: 0.4864089922784082, acc: 0.7833333333333333\n",
      "9:350 - loss: 0.5030464659199793, acc: 0.7714285714285715\n",
      "9:400 - loss: 0.5082788783512404, acc: 0.765\n",
      "9:450 - loss: 0.517539570288345, acc: 0.76\n",
      "9:500 - loss: 0.5277170851636723, acc: 0.752\n",
      "9:550 - loss: 0.5268681312767753, acc: 0.7527272727272727\n",
      "epoch: 9 - train loss: 0.5270981506316552, train acc: 0.7508833922261484\n",
      "epoch: 9 - test loss: 0.6455355298723525, test acc: 0.6666666666666666\n",
      "10:50 - loss: 0.4605081425050272, acc: 0.82\n",
      "10:100 - loss: 0.4384388523344444, acc: 0.79\n",
      "10:150 - loss: 0.4540065761109243, acc: 0.7733333333333333\n",
      "10:200 - loss: 0.44994192591426246, acc: 0.785\n",
      "10:250 - loss: 0.4639819894502721, acc: 0.78\n",
      "10:300 - loss: 0.4924222740030092, acc: 0.76\n",
      "10:350 - loss: 0.49724530493441155, acc: 0.7571428571428571\n",
      "10:400 - loss: 0.5134530449990471, acc: 0.7375\n",
      "10:450 - loss: 0.508955112435357, acc: 0.7444444444444445\n",
      "10:500 - loss: 0.5038990378538372, acc: 0.748\n",
      "10:550 - loss: 0.4977159504846026, acc: 0.7581818181818182\n",
      "epoch: 10 - train loss: 0.5031935028578373, train acc: 0.7526501766784452\n",
      "epoch: 10 - test loss: 0.6599692286182468, test acc: 0.6455026455026455\n",
      "11:50 - loss: 0.47307054937684795, acc: 0.76\n",
      "11:100 - loss: 0.46619740259672876, acc: 0.76\n",
      "11:150 - loss: 0.4797811499222605, acc: 0.7666666666666667\n",
      "11:200 - loss: 0.4714469858624175, acc: 0.775\n",
      "11:250 - loss: 0.479171515220226, acc: 0.768\n",
      "11:300 - loss: 0.4772949109830075, acc: 0.7633333333333333\n",
      "11:350 - loss: 0.46515413188062105, acc: 0.7771428571428571\n",
      "11:400 - loss: 0.4513814563170592, acc: 0.785\n",
      "11:450 - loss: 0.44297167626372513, acc: 0.7955555555555556\n",
      "11:500 - loss: 0.4330396141494097, acc: 0.8\n",
      "11:550 - loss: 0.4374599914189981, acc: 0.8018181818181818\n",
      "epoch: 11 - train loss: 0.44379484581033457, train acc: 0.7985865724381626\n",
      "epoch: 11 - test loss: 0.6697626373410466, test acc: 0.5661375661375662\n",
      "12:50 - loss: 0.3415774681165478, acc: 0.86\n",
      "12:100 - loss: 0.3673260996239398, acc: 0.84\n",
      "12:150 - loss: 0.4000704022823732, acc: 0.82\n",
      "12:200 - loss: 0.43718281670777465, acc: 0.8\n",
      "12:250 - loss: 0.441479134024956, acc: 0.8\n",
      "12:300 - loss: 0.4370973577189441, acc: 0.8033333333333333\n",
      "12:350 - loss: 0.43721141951251435, acc: 0.7942857142857143\n",
      "12:400 - loss: 0.4328420070420191, acc: 0.8\n",
      "12:450 - loss: 0.4349862730040966, acc: 0.7933333333333333\n",
      "12:500 - loss: 0.423655816129376, acc: 0.806\n",
      "12:550 - loss: 0.4268257285689399, acc: 0.8090909090909091\n",
      "epoch: 12 - train loss: 0.4293507898067826, train acc: 0.8056537102473498\n",
      "epoch: 12 - test loss: 0.6625978729262293, test acc: 0.6613756613756614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:50 - loss: 0.4179910606746964, acc: 0.82\n",
      "13:100 - loss: 0.42122347699153345, acc: 0.84\n",
      "13:150 - loss: 0.4015827196869574, acc: 0.8466666666666667\n",
      "13:200 - loss: 0.4288778221361615, acc: 0.82\n",
      "13:250 - loss: 0.40923014936281343, acc: 0.828\n",
      "13:300 - loss: 0.4119845491555215, acc: 0.8233333333333334\n",
      "13:350 - loss: 0.41706842425518376, acc: 0.8257142857142857\n",
      "13:400 - loss: 0.4424216069534062, acc: 0.815\n",
      "13:450 - loss: 0.4384338979901176, acc: 0.8133333333333334\n",
      "13:500 - loss: 0.42143083221594296, acc: 0.82\n",
      "13:550 - loss: 0.43411544290707904, acc: 0.8127272727272727\n",
      "epoch: 13 - train loss: 0.4373052689441792, train acc: 0.8091872791519434\n",
      "epoch: 13 - test loss: 0.6925760081907205, test acc: 0.582010582010582\n",
      "14:50 - loss: 0.2793846610487734, acc: 0.92\n",
      "14:100 - loss: 0.3135116515942994, acc: 0.9\n",
      "14:150 - loss: 0.41510140428806497, acc: 0.8466666666666667\n",
      "14:200 - loss: 0.381228559941418, acc: 0.845\n",
      "14:250 - loss: 0.37061850765215026, acc: 0.852\n",
      "14:300 - loss: 0.3615465247066777, acc: 0.8566666666666667\n",
      "14:350 - loss: 0.3819556932376108, acc: 0.8514285714285714\n",
      "14:400 - loss: 0.3678731486013778, acc: 0.8575\n",
      "14:450 - loss: 0.3720686799202297, acc: 0.8533333333333334\n",
      "14:500 - loss: 0.375814614990863, acc: 0.852\n",
      "14:550 - loss: 0.36978010372559056, acc: 0.8527272727272728\n",
      "epoch: 14 - train loss: 0.3718221867125095, train acc: 0.8498233215547704\n",
      "epoch: 14 - test loss: 0.6908522189805425, test acc: 0.5978835978835979\n",
      "15:50 - loss: 0.291045771120146, acc: 0.88\n",
      "15:100 - loss: 0.37049202926872304, acc: 0.83\n",
      "15:150 - loss: 0.3377535611134335, acc: 0.86\n",
      "15:200 - loss: 0.29580953739741966, acc: 0.885\n",
      "15:250 - loss: 0.31004054401352743, acc: 0.884\n",
      "15:300 - loss: 0.32918461920586994, acc: 0.88\n",
      "15:350 - loss: 0.3334402558410021, acc: 0.8685714285714285\n",
      "15:400 - loss: 0.32661937649740813, acc: 0.87\n",
      "15:450 - loss: 0.33403942883132925, acc: 0.8711111111111111\n",
      "15:500 - loss: 0.33993008437775946, acc: 0.866\n",
      "15:550 - loss: 0.34459503100161065, acc: 0.8636363636363636\n",
      "epoch: 15 - train loss: 0.3518702125211333, train acc: 0.8568904593639576\n",
      "epoch: 15 - test loss: 0.7108660832308151, test acc: 0.544973544973545\n",
      "16:50 - loss: 0.264871323496684, acc: 0.94\n",
      "16:100 - loss: 0.3378397511282384, acc: 0.9\n",
      "16:150 - loss: 0.3029939120547046, acc: 0.9133333333333333\n",
      "16:200 - loss: 0.3041511985580747, acc: 0.91\n",
      "16:250 - loss: 0.31207962758761615, acc: 0.9\n",
      "16:300 - loss: 0.3331343770052662, acc: 0.88\n",
      "16:350 - loss: 0.3356481799532305, acc: 0.8714285714285714\n",
      "16:400 - loss: 0.34043816328361537, acc: 0.87\n",
      "16:450 - loss: 0.3424891702291287, acc: 0.8688888888888889\n",
      "16:500 - loss: 0.33472340323590777, acc: 0.87\n",
      "16:550 - loss: 0.3292913296731333, acc: 0.8727272727272727\n",
      "epoch: 16 - train loss: 0.329040256595402, train acc: 0.872791519434629\n",
      "epoch: 16 - test loss: 0.7077470586319259, test acc: 0.5873015873015873\n",
      "17:50 - loss: 0.28964792720296884, acc: 0.92\n",
      "17:100 - loss: 0.30927161031157097, acc: 0.91\n",
      "17:150 - loss: 0.3016868652093723, acc: 0.9133333333333333\n",
      "17:200 - loss: 0.3069246471056323, acc: 0.9\n",
      "17:250 - loss: 0.29589403435213674, acc: 0.908\n",
      "17:300 - loss: 0.2939133440719942, acc: 0.9\n",
      "17:350 - loss: 0.3107430516569768, acc: 0.8914285714285715\n",
      "17:400 - loss: 0.29467671996311806, acc: 0.9\n",
      "17:450 - loss: 0.3000444401497747, acc: 0.8977777777777778\n",
      "17:500 - loss: 0.2894825256019557, acc: 0.902\n",
      "17:550 - loss: 0.29509538922049894, acc: 0.8981818181818182\n",
      "epoch: 17 - train loss: 0.2961334188706909, train acc: 0.8975265017667845\n",
      "epoch: 17 - test loss: 0.7233208058450635, test acc: 0.5396825396825397\n",
      "18:50 - loss: 0.22308695879208437, acc: 0.94\n",
      "18:100 - loss: 0.22474656960245173, acc: 0.92\n",
      "18:150 - loss: 0.23857547418236702, acc: 0.9066666666666666\n",
      "18:200 - loss: 0.23933797470399104, acc: 0.905\n",
      "18:250 - loss: 0.21954023564978348, acc: 0.916\n",
      "18:300 - loss: 0.2133395761319825, acc: 0.92\n",
      "18:350 - loss: 0.2362906840361736, acc: 0.9142857142857143\n",
      "18:400 - loss: 0.23483429561837443, acc: 0.91\n",
      "18:450 - loss: 0.23854903685939016, acc: 0.9088888888888889\n",
      "18:500 - loss: 0.2517267632009195, acc: 0.904\n",
      "18:550 - loss: 0.2511463852459392, acc: 0.9036363636363637\n",
      "epoch: 18 - train loss: 0.25566861424148013, train acc: 0.901060070671378\n",
      "epoch: 18 - test loss: 0.7744146530560896, test acc: 0.5978835978835979\n",
      "19:50 - loss: 0.22168276458250713, acc: 0.9\n",
      "19:100 - loss: 0.27130363543988834, acc: 0.9\n",
      "19:150 - loss: 0.271639680335156, acc: 0.8933333333333333\n",
      "19:200 - loss: 0.26665016821733006, acc: 0.895\n",
      "19:250 - loss: 0.23353589528365234, acc: 0.912\n",
      "19:300 - loss: 0.21914294608041307, acc: 0.9166666666666666\n",
      "19:350 - loss: 0.2001617517947971, acc: 0.9285714285714286\n",
      "19:400 - loss: 0.2211166335307719, acc: 0.9225\n",
      "19:450 - loss: 0.22207416430788968, acc: 0.9155555555555556\n",
      "19:500 - loss: 0.23579217751808235, acc: 0.914\n",
      "19:550 - loss: 0.23020578971300182, acc: 0.9181818181818182\n",
      "epoch: 19 - train loss: 0.2275925503410639, train acc: 0.9187279151943463\n",
      "epoch: 19 - test loss: 0.8550883887351342, test acc: 0.5925925925925926\n",
      "20:50 - loss: 0.25003288419640435, acc: 0.94\n",
      "20:100 - loss: 0.23228384737309782, acc: 0.94\n",
      "20:150 - loss: 0.23026965205762026, acc: 0.9333333333333333\n",
      "20:200 - loss: 0.22459053916096328, acc: 0.94\n",
      "20:250 - loss: 0.23161500690391904, acc: 0.936\n",
      "20:300 - loss: 0.21748935516790618, acc: 0.94\n",
      "20:350 - loss: 0.2365118275629953, acc: 0.9257142857142857\n",
      "20:400 - loss: 0.24141397466432601, acc: 0.9225\n",
      "20:450 - loss: 0.2620629648530191, acc: 0.9111111111111111\n",
      "20:500 - loss: 0.2618401929191086, acc: 0.91\n",
      "20:550 - loss: 0.2711017187498272, acc: 0.9054545454545454\n",
      "epoch: 20 - train loss: 0.2740587120108083, train acc: 0.9028268551236749\n",
      "epoch: 20 - test loss: 0.7068908936326957, test acc: 0.5978835978835979\n",
      "21:50 - loss: 0.31803452321774806, acc: 0.86\n",
      "21:100 - loss: 0.30328226376366885, acc: 0.87\n",
      "21:150 - loss: 0.2472153365549274, acc: 0.8933333333333333\n",
      "21:200 - loss: 0.21057442659585182, acc: 0.915\n",
      "21:250 - loss: 0.21082711651469976, acc: 0.912\n",
      "21:300 - loss: 0.20399301931736044, acc: 0.9166666666666666\n",
      "21:350 - loss: 0.18977959268672498, acc: 0.9228571428571428\n",
      "21:400 - loss: 0.1850455134430807, acc: 0.925\n",
      "21:450 - loss: 0.17802256483153195, acc: 0.9266666666666666\n",
      "21:500 - loss: 0.19452040109446997, acc: 0.92\n",
      "21:550 - loss: 0.20370304311041168, acc: 0.92\n",
      "epoch: 21 - train loss: 0.2094725731354252, train acc: 0.9169611307420494\n",
      "epoch: 21 - test loss: 0.8584703856008052, test acc: 0.5767195767195767\n",
      "22:50 - loss: 0.12320118602871956, acc: 0.98\n",
      "22:100 - loss: 0.13530914321501059, acc: 0.97\n",
      "22:150 - loss: 0.14739594713152027, acc: 0.9533333333333334\n",
      "22:200 - loss: 0.14598617985519727, acc: 0.955\n",
      "22:250 - loss: 0.13581053216016134, acc: 0.96\n",
      "22:300 - loss: 0.13052489635994752, acc: 0.96\n",
      "22:350 - loss: 0.12091337777987521, acc: 0.9628571428571429\n",
      "22:400 - loss: 0.13121032588707598, acc: 0.96\n",
      "22:450 - loss: 0.14978173530017297, acc: 0.9466666666666667\n",
      "22:500 - loss: 0.16071872609858373, acc: 0.944\n",
      "22:550 - loss: 0.16585889171199655, acc: 0.9418181818181818\n",
      "epoch: 22 - train loss: 0.16643773133925263, train acc: 0.941696113074205\n",
      "epoch: 22 - test loss: 0.7633823594946617, test acc: 0.6349206349206349\n",
      "23:50 - loss: 0.1476724799111864, acc: 0.92\n",
      "23:100 - loss: 0.09230300719029637, acc: 0.96\n",
      "23:150 - loss: 0.10535354012225785, acc: 0.9533333333333334\n",
      "23:200 - loss: 0.1323622593026206, acc: 0.95\n",
      "23:250 - loss: 0.11618891004979702, acc: 0.956\n",
      "23:300 - loss: 0.10864959895165034, acc: 0.96\n",
      "23:350 - loss: 0.10572500617066487, acc: 0.9628571428571429\n",
      "23:400 - loss: 0.12033268637157399, acc: 0.9575\n",
      "23:450 - loss: 0.13034445238515308, acc: 0.9555555555555556\n",
      "23:500 - loss: 0.15057147893563078, acc: 0.946\n",
      "23:550 - loss: 0.16224734559333323, acc: 0.9418181818181818\n",
      "epoch: 23 - train loss: 0.1654788435247376, train acc: 0.9399293286219081\n",
      "epoch: 23 - test loss: 0.9450213017812114, test acc: 0.5925925925925926\n",
      "24:50 - loss: 0.1115624963657378, acc: 0.98\n",
      "24:100 - loss: 0.09314054980543479, acc: 0.97\n",
      "24:150 - loss: 0.10266676622708464, acc: 0.96\n",
      "24:200 - loss: 0.103673361153177, acc: 0.95\n",
      "24:250 - loss: 0.10723789078839331, acc: 0.956\n",
      "24:300 - loss: 0.11557115269102933, acc: 0.9533333333333334\n",
      "24:350 - loss: 0.1131857730915521, acc: 0.9514285714285714\n",
      "24:400 - loss: 0.12371087142852394, acc: 0.9475\n",
      "24:450 - loss: 0.12370637164654412, acc: 0.9511111111111111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24:500 - loss: 0.12024366944794482, acc: 0.952\n",
      "24:550 - loss: 0.13971310858362376, acc: 0.9436363636363636\n",
      "epoch: 24 - train loss: 0.14768722306439386, train acc: 0.941696113074205\n",
      "epoch: 24 - test loss: 0.9501713014690478, test acc: 0.6402116402116402\n",
      "25:50 - loss: 0.30614721112801774, acc: 0.92\n",
      "25:100 - loss: 0.20043696406291323, acc: 0.93\n",
      "25:150 - loss: 0.14258665017012392, acc: 0.9533333333333334\n",
      "25:200 - loss: 0.16207525730623376, acc: 0.945\n",
      "25:250 - loss: 0.1623763848457987, acc: 0.948\n",
      "25:300 - loss: 0.14240686956025186, acc: 0.9533333333333334\n",
      "25:350 - loss: 0.13057889200028347, acc: 0.9571428571428572\n",
      "25:400 - loss: 0.1194938555862346, acc: 0.96\n",
      "25:450 - loss: 0.1203735046413102, acc: 0.9577777777777777\n",
      "25:500 - loss: 0.13103107318848758, acc: 0.956\n",
      "25:550 - loss: 0.12590438257335373, acc: 0.9563636363636364\n",
      "epoch: 25 - train loss: 0.1275710552285709, train acc: 0.9558303886925795\n",
      "epoch: 25 - test loss: 1.0120781838283304, test acc: 0.5608465608465608\n",
      "26:50 - loss: 0.14356077854402802, acc: 0.92\n",
      "26:100 - loss: 0.09324119079453067, acc: 0.95\n",
      "26:150 - loss: 0.09813580875894809, acc: 0.9533333333333334\n",
      "26:200 - loss: 0.09517049333038688, acc: 0.955\n",
      "26:250 - loss: 0.08327440856422606, acc: 0.96\n",
      "26:300 - loss: 0.09850242392077696, acc: 0.9566666666666667\n",
      "26:350 - loss: 0.08904939728361903, acc: 0.9628571428571429\n",
      "26:400 - loss: 0.09267973419158898, acc: 0.9625\n",
      "26:450 - loss: 0.0869091103476759, acc: 0.9644444444444444\n",
      "26:500 - loss: 0.11974039197502896, acc: 0.946\n",
      "26:550 - loss: 0.1380300170928124, acc: 0.94\n",
      "epoch: 26 - train loss: 0.1400089355450308, train acc: 0.9381625441696113\n",
      "epoch: 26 - test loss: 0.8333224738697097, test acc: 0.5714285714285714\n",
      "27:50 - loss: 0.08350294867825087, acc: 0.96\n",
      "27:100 - loss: 0.1662256709052582, acc: 0.95\n",
      "27:150 - loss: 0.16142829958890556, acc: 0.9533333333333334\n",
      "27:200 - loss: 0.1403118468418911, acc: 0.955\n",
      "27:250 - loss: 0.15017995435787818, acc: 0.944\n",
      "27:300 - loss: 0.15510877948938484, acc: 0.9433333333333334\n",
      "27:350 - loss: 0.14286599809171233, acc: 0.9485714285714286\n",
      "27:400 - loss: 0.12877478333460352, acc: 0.955\n",
      "27:450 - loss: 0.12461755632078528, acc: 0.9555555555555556\n",
      "27:500 - loss: 0.1182128241100012, acc: 0.958\n",
      "27:550 - loss: 0.10977594887823931, acc: 0.9618181818181818\n",
      "epoch: 27 - train loss: 0.1101987126550019, train acc: 0.9611307420494699\n",
      "epoch: 27 - test loss: 0.8684318457385238, test acc: 0.5767195767195767\n",
      "28:50 - loss: 0.027941585797415222, acc: 0.98\n",
      "28:100 - loss: 0.03699986104721784, acc: 0.98\n",
      "28:150 - loss: 0.028301236831441326, acc: 0.9866666666666667\n",
      "28:200 - loss: 0.03073346219598091, acc: 0.99\n",
      "28:250 - loss: 0.0255898504373962, acc: 0.992\n",
      "28:300 - loss: 0.043402657797474625, acc: 0.99\n",
      "28:350 - loss: 0.052486553397008806, acc: 0.9828571428571429\n",
      "28:400 - loss: 0.04820514770237704, acc: 0.985\n",
      "28:450 - loss: 0.045064324674121255, acc: 0.9866666666666667\n",
      "28:500 - loss: 0.06424627204388478, acc: 0.98\n",
      "28:550 - loss: 0.06790982114273661, acc: 0.9763636363636363\n",
      "epoch: 28 - train loss: 0.06882953176178326, train acc: 0.9752650176678446\n",
      "epoch: 28 - test loss: 0.9155737491332508, test acc: 0.6190476190476191\n",
      "29:50 - loss: 0.17438601270153226, acc: 0.94\n",
      "29:100 - loss: 0.11214711091042648, acc: 0.96\n",
      "29:150 - loss: 0.14759898243658875, acc: 0.9466666666666667\n",
      "29:200 - loss: 0.1458746632140015, acc: 0.94\n",
      "29:250 - loss: 0.1638053952351496, acc: 0.932\n",
      "29:300 - loss: 0.1906987570908411, acc: 0.9266666666666666\n",
      "29:350 - loss: 0.1873325664546188, acc: 0.9257142857142857\n",
      "29:400 - loss: 0.1808333946252445, acc: 0.93\n",
      "29:450 - loss: 0.1632498041499874, acc: 0.9377777777777778\n",
      "29:500 - loss: 0.15990956830667444, acc: 0.942\n",
      "29:550 - loss: 0.14763335508362538, acc: 0.9472727272727273\n",
      "epoch: 29 - train loss: 0.14370801758879584, train acc: 0.9487632508833922\n",
      "epoch: 29 - test loss: 0.8406289693636949, test acc: 0.582010582010582\n",
      "30:50 - loss: 0.01740268725429385, acc: 1.0\n",
      "30:100 - loss: 0.012948801744080998, acc: 1.0\n",
      "30:150 - loss: 0.02844432683627962, acc: 0.9933333333333333\n",
      "30:200 - loss: 0.05267109166227447, acc: 0.985\n",
      "30:250 - loss: 0.0544731249843935, acc: 0.984\n",
      "30:300 - loss: 0.0498748318211268, acc: 0.9866666666666667\n",
      "30:350 - loss: 0.05632364984054637, acc: 0.9857142857142858\n",
      "30:400 - loss: 0.06810797653167683, acc: 0.9775\n",
      "30:450 - loss: 0.0629619565431774, acc: 0.98\n",
      "30:500 - loss: 0.0599460153773221, acc: 0.98\n",
      "30:550 - loss: 0.059276868234108174, acc: 0.98\n",
      "epoch: 30 - train loss: 0.058295317566569395, train acc: 0.980565371024735\n",
      "epoch: 30 - test loss: 1.1772413495536105, test acc: 0.5925925925925926\n",
      "31:50 - loss: 0.052450105279809034, acc: 0.98\n",
      "31:100 - loss: 0.027370211712807645, acc: 0.99\n",
      "31:150 - loss: 0.02278738482450072, acc: 0.9933333333333333\n",
      "31:200 - loss: 0.019393910534706113, acc: 0.995\n",
      "31:250 - loss: 0.017063174015289773, acc: 0.996\n",
      "31:300 - loss: 0.0329444270156551, acc: 0.9933333333333333\n",
      "31:350 - loss: 0.04080761519425367, acc: 0.9857142857142858\n",
      "31:400 - loss: 0.03637752468803466, acc: 0.9875\n",
      "31:450 - loss: 0.035219685175009716, acc: 0.9866666666666667\n",
      "31:500 - loss: 0.034398427842199715, acc: 0.986\n",
      "31:550 - loss: 0.03390683546026096, acc: 0.9854545454545455\n",
      "epoch: 31 - train loss: 0.03576453656789805, train acc: 0.9840989399293286\n",
      "epoch: 31 - test loss: 1.2588813894831616, test acc: 0.5978835978835979\n",
      "32:50 - loss: 0.005348855942790154, acc: 1.0\n",
      "32:100 - loss: 0.003624035615464264, acc: 1.0\n",
      "32:150 - loss: 0.0111373406711496, acc: 0.9933333333333333\n",
      "32:200 - loss: 0.009406665189959796, acc: 0.995\n",
      "32:250 - loss: 0.02262846199591245, acc: 0.992\n",
      "32:300 - loss: 0.019893169611987758, acc: 0.9933333333333333\n",
      "32:350 - loss: 0.017271347150349732, acc: 0.9942857142857143\n",
      "32:400 - loss: 0.015262584249314884, acc: 0.995\n",
      "32:450 - loss: 0.01682694781195736, acc: 0.9933333333333333\n",
      "32:500 - loss: 0.016703473290152347, acc: 0.994\n",
      "32:550 - loss: 0.02101408830520886, acc: 0.9927272727272727\n",
      "epoch: 32 - train loss: 0.022093954609814503, train acc: 0.9929328621908127\n",
      "epoch: 32 - test loss: 1.074802129103217, test acc: 0.5608465608465608\n",
      "33:50 - loss: 0.20437128292256632, acc: 0.9\n",
      "33:100 - loss: 0.20370536339205242, acc: 0.92\n",
      "33:150 - loss: 0.2136209994619923, acc: 0.92\n",
      "33:200 - loss: 0.17270329675978346, acc: 0.935\n",
      "33:250 - loss: 0.1581040753500259, acc: 0.944\n",
      "33:300 - loss: 0.13841185164314587, acc: 0.9533333333333334\n",
      "33:350 - loss: 0.13027072707916101, acc: 0.9571428571428572\n",
      "33:400 - loss: 0.14212693208923124, acc: 0.9475\n",
      "33:450 - loss: 0.1569082557031189, acc: 0.9422222222222222\n",
      "33:500 - loss: 0.1636538190528655, acc: 0.936\n",
      "33:550 - loss: 0.15758959146829524, acc: 0.94\n",
      "epoch: 33 - train loss: 0.16266769599935074, train acc: 0.9399293286219081\n",
      "epoch: 33 - test loss: 0.9559785991472868, test acc: 0.544973544973545\n",
      "34:50 - loss: 0.14313844515442084, acc: 0.92\n",
      "34:100 - loss: 0.10597156857401044, acc: 0.95\n",
      "34:150 - loss: 0.1533230315975406, acc: 0.9266666666666666\n",
      "34:200 - loss: 0.14042158714482889, acc: 0.925\n",
      "34:250 - loss: 0.11568621141095123, acc: 0.94\n",
      "34:300 - loss: 0.12093194196496854, acc: 0.94\n",
      "34:350 - loss: 0.12916278190703412, acc: 0.9428571428571428\n",
      "34:400 - loss: 0.11660737262933896, acc: 0.9475\n",
      "34:450 - loss: 0.10576970581433039, acc: 0.9533333333333334\n",
      "34:500 - loss: 0.10267481696110416, acc: 0.954\n",
      "34:550 - loss: 0.1272050811005391, acc: 0.9490909090909091\n",
      "epoch: 34 - train loss: 0.12889676131883906, train acc: 0.9452296819787986\n",
      "epoch: 34 - test loss: 0.9402729109194613, test acc: 0.5767195767195767\n",
      "35:50 - loss: 0.08164161401049734, acc: 0.96\n",
      "35:100 - loss: 0.09028935541433297, acc: 0.96\n",
      "35:150 - loss: 0.10437134655131367, acc: 0.9466666666666667\n",
      "35:200 - loss: 0.09870002232570264, acc: 0.95\n",
      "35:250 - loss: 0.08607563871686111, acc: 0.956\n",
      "35:300 - loss: 0.08924530175007094, acc: 0.9566666666666667\n",
      "35:350 - loss: 0.08184699694864703, acc: 0.96\n",
      "35:400 - loss: 0.07519634125768515, acc: 0.9625\n",
      "35:450 - loss: 0.07067630422529335, acc: 0.9666666666666667\n",
      "35:500 - loss: 0.08602011471000691, acc: 0.962\n",
      "35:550 - loss: 0.08036095808264872, acc: 0.9654545454545455\n",
      "epoch: 35 - train loss: 0.07819257227721081, train acc: 0.9664310954063604\n",
      "epoch: 35 - test loss: 0.9064500859454657, test acc: 0.6296296296296297\n",
      "36:50 - loss: 0.018314089234873047, acc: 1.0\n",
      "36:100 - loss: 0.0247914427391291, acc: 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36:150 - loss: 0.021350201537202917, acc: 0.9933333333333333\n",
      "36:200 - loss: 0.020628298796876995, acc: 0.99\n",
      "36:250 - loss: 0.026928510347799545, acc: 0.988\n",
      "36:300 - loss: 0.02479475011712546, acc: 0.99\n",
      "36:350 - loss: 0.026405872782392176, acc: 0.9885714285714285\n",
      "36:400 - loss: 0.0239384311852725, acc: 0.99\n",
      "36:450 - loss: 0.02298468311744587, acc: 0.9911111111111112\n",
      "36:500 - loss: 0.027254661935102606, acc: 0.99\n",
      "36:550 - loss: 0.026431150240106064, acc: 0.9890909090909091\n",
      "epoch: 36 - train loss: 0.027259324247879704, train acc: 0.9876325088339223\n",
      "epoch: 36 - test loss: 1.1902356303120596, test acc: 0.5343915343915344\n",
      "37:50 - loss: 0.044388475294096466, acc: 0.98\n",
      "37:100 - loss: 0.13857050910707913, acc: 0.95\n",
      "37:150 - loss: 0.18499229907158468, acc: 0.9333333333333333\n",
      "37:200 - loss: 0.16690830073869928, acc: 0.925\n",
      "37:250 - loss: 0.14754448522394387, acc: 0.936\n",
      "37:300 - loss: 0.1352178895251545, acc: 0.94\n",
      "37:350 - loss: 0.1363464326562921, acc: 0.9371428571428572\n",
      "37:400 - loss: 0.13359420387118573, acc: 0.94\n",
      "37:450 - loss: 0.1417246802912564, acc: 0.9333333333333333\n",
      "37:500 - loss: 0.13388668614679788, acc: 0.936\n",
      "37:550 - loss: 0.12370002449560996, acc: 0.9418181818181818\n",
      "epoch: 37 - train loss: 0.1224413061043375, train acc: 0.941696113074205\n",
      "epoch: 37 - test loss: 0.85794787192325, test acc: 0.6084656084656085\n",
      "38:50 - loss: 0.008131603616300867, acc: 1.0\n",
      "38:100 - loss: 0.05177644721895792, acc: 0.97\n",
      "38:150 - loss: 0.03667991813222803, acc: 0.98\n",
      "38:200 - loss: 0.03736275264288874, acc: 0.98\n",
      "38:250 - loss: 0.05918476108190892, acc: 0.972\n",
      "38:300 - loss: 0.1018389062625067, acc: 0.9666666666666667\n",
      "38:350 - loss: 0.1060895011783839, acc: 0.9628571428571429\n",
      "38:400 - loss: 0.1115022080956705, acc: 0.9625\n",
      "38:450 - loss: 0.11062865060587045, acc: 0.9622222222222222\n",
      "38:500 - loss: 0.10224657190251127, acc: 0.966\n",
      "38:550 - loss: 0.0961396427020109, acc: 0.9672727272727273\n",
      "epoch: 38 - train loss: 0.09358277046059874, train acc: 0.9681978798586572\n",
      "epoch: 38 - test loss: 0.858354353273495, test acc: 0.5978835978835979\n",
      "39:50 - loss: 0.0093918989549018, acc: 1.0\n",
      "39:100 - loss: 0.010990809945274089, acc: 1.0\n",
      "39:150 - loss: 0.009071733802187355, acc: 1.0\n",
      "39:200 - loss: 0.007962725167148538, acc: 1.0\n",
      "39:250 - loss: 0.019134490892248095, acc: 0.996\n",
      "39:300 - loss: 0.019177386408051222, acc: 0.9933333333333333\n",
      "39:350 - loss: 0.0201535297289678, acc: 0.9914285714285714\n",
      "39:400 - loss: 0.01810871670204564, acc: 0.9925\n",
      "39:450 - loss: 0.017294063648458424, acc: 0.9933333333333333\n",
      "39:500 - loss: 0.020389490160463417, acc: 0.992\n",
      "39:550 - loss: 0.01972119158233654, acc: 0.9927272727272727\n",
      "epoch: 39 - train loss: 0.019351715262102005, train acc: 0.9929328621908127\n",
      "epoch: 39 - test loss: 1.0611335469135468, test acc: 0.6031746031746031\n",
      "40:50 - loss: 0.002613746459013409, acc: 1.0\n",
      "40:100 - loss: 0.0020156649897398494, acc: 1.0\n",
      "40:150 - loss: 0.004241738844194479, acc: 1.0\n",
      "40:200 - loss: 0.004784258595825943, acc: 1.0\n",
      "40:250 - loss: 0.00433634948810365, acc: 1.0\n",
      "40:300 - loss: 0.01664746395097881, acc: 0.9933333333333333\n",
      "40:350 - loss: 0.014603410797831354, acc: 0.9942857142857143\n",
      "40:400 - loss: 0.01366649579867369, acc: 0.995\n",
      "40:450 - loss: 0.012464989531282273, acc: 0.9955555555555555\n",
      "40:500 - loss: 0.017364651224067818, acc: 0.994\n",
      "40:550 - loss: 0.023293680753452246, acc: 0.990909090909091\n",
      "epoch: 40 - train loss: 0.022675328140311146, train acc: 0.991166077738516\n",
      "epoch: 40 - test loss: 1.2285829421024512, test acc: 0.5661375661375662\n",
      "41:50 - loss: 0.04911438052270807, acc: 0.98\n",
      "41:100 - loss: 0.025478703772763633, acc: 0.99\n",
      "41:150 - loss: 0.019837788801426937, acc: 0.9933333333333333\n",
      "41:200 - loss: 0.015189277370551433, acc: 0.995\n",
      "41:250 - loss: 0.01953641029151837, acc: 0.988\n",
      "41:300 - loss: 0.016429467606764822, acc: 0.99\n",
      "41:350 - loss: 0.01875429931722111, acc: 0.9885714285714285\n",
      "41:400 - loss: 0.021122065470941028, acc: 0.9875\n",
      "41:450 - loss: 0.024709947048561238, acc: 0.9866666666666667\n",
      "41:500 - loss: 0.025260681284872778, acc: 0.986\n",
      "41:550 - loss: 0.023082496995551007, acc: 0.9872727272727273\n",
      "epoch: 41 - train loss: 0.022472505000876587, train acc: 0.9876325088339223\n",
      "epoch: 41 - test loss: 0.9240363483313504, test acc: 0.5978835978835979\n",
      "42:50 - loss: 0.0013072085828326485, acc: 1.0\n",
      "42:100 - loss: 0.018190003412612957, acc: 0.99\n",
      "42:150 - loss: 0.026348643558785436, acc: 0.9866666666666667\n",
      "42:200 - loss: 0.03657287825053138, acc: 0.98\n",
      "42:250 - loss: 0.029408802715941165, acc: 0.984\n",
      "42:300 - loss: 0.02462886479491129, acc: 0.9866666666666667\n",
      "42:350 - loss: 0.02680297411703585, acc: 0.9857142857142858\n",
      "42:400 - loss: 0.02353794591089111, acc: 0.9875\n",
      "42:450 - loss: 0.022483983869153144, acc: 0.9888888888888889\n",
      "42:500 - loss: 0.022588436139006098, acc: 0.988\n",
      "42:550 - loss: 0.020615465558814116, acc: 0.9890909090909091\n",
      "epoch: 42 - train loss: 0.020051040244549054, train acc: 0.9893992932862191\n",
      "epoch: 42 - test loss: 1.1645274695235954, test acc: 0.5978835978835979\n",
      "43:50 - loss: 0.02559620034368838, acc: 0.98\n",
      "43:100 - loss: 0.015178325885224664, acc: 0.99\n",
      "43:150 - loss: 0.011170786737114413, acc: 0.9933333333333333\n",
      "43:200 - loss: 0.020567732209530502, acc: 0.99\n",
      "43:250 - loss: 0.016620024196946328, acc: 0.992\n",
      "43:300 - loss: 0.01814810548247854, acc: 0.99\n",
      "43:350 - loss: 0.015623461412861006, acc: 0.9914285714285714\n",
      "43:400 - loss: 0.019253395048961026, acc: 0.99\n",
      "43:450 - loss: 0.017161609389179603, acc: 0.9911111111111112\n",
      "43:500 - loss: 0.017656495713467557, acc: 0.992\n",
      "43:550 - loss: 0.016195758844026523, acc: 0.9927272727272727\n",
      "epoch: 43 - train loss: 0.020501063888228636, train acc: 0.991166077738516\n",
      "epoch: 43 - test loss: 0.9825456025021081, test acc: 0.5767195767195767\n",
      "44:50 - loss: 0.01391605122532063, acc: 1.0\n",
      "44:100 - loss: 0.03403551667859611, acc: 0.99\n",
      "44:150 - loss: 0.02292184187299721, acc: 0.9933333333333333\n",
      "44:200 - loss: 0.017461636747044856, acc: 0.995\n",
      "44:250 - loss: 0.0245458383231475, acc: 0.992\n",
      "44:300 - loss: 0.021042707632436566, acc: 0.9933333333333333\n",
      "44:350 - loss: 0.018195545048873166, acc: 0.9942857142857143\n",
      "44:400 - loss: 0.01604755838851015, acc: 0.995\n",
      "44:450 - loss: 0.02100981166772986, acc: 0.9933333333333333\n",
      "44:500 - loss: 0.022776205910597185, acc: 0.992\n",
      "44:550 - loss: 0.02476439661439232, acc: 0.990909090909091\n",
      "epoch: 44 - train loss: 0.024075645790487113, train acc: 0.991166077738516\n",
      "epoch: 44 - test loss: 1.2292598957616117, test acc: 0.5767195767195767\n",
      "45:50 - loss: 0.020942349592022094, acc: 0.98\n",
      "45:100 - loss: 0.010645741644115574, acc: 0.99\n",
      "45:150 - loss: 0.007404104664744104, acc: 0.9933333333333333\n",
      "45:200 - loss: 0.006567000103455355, acc: 0.995\n",
      "45:250 - loss: 0.006198850361181567, acc: 0.996\n",
      "45:300 - loss: 0.01281291481899781, acc: 0.9933333333333333\n",
      "45:350 - loss: 0.012996145269120468, acc: 0.9942857142857143\n",
      "45:400 - loss: 0.015853934850585717, acc: 0.9925\n",
      "45:450 - loss: 0.01412600622262589, acc: 0.9933333333333333\n",
      "45:500 - loss: 0.015885781954039217, acc: 0.992\n",
      "45:550 - loss: 0.01696012303268503, acc: 0.990909090909091\n",
      "epoch: 45 - train loss: 0.019681849568205304, train acc: 0.9893992932862191\n",
      "epoch: 45 - test loss: 1.0283089010783975, test acc: 0.6031746031746031\n",
      "46:50 - loss: 0.007845972822365129, acc: 1.0\n",
      "46:100 - loss: 0.043268866590457075, acc: 0.98\n",
      "46:150 - loss: 0.029091208017659506, acc: 0.9866666666666667\n",
      "46:200 - loss: 0.021929179143447195, acc: 0.99\n",
      "46:250 - loss: 0.025338265747037333, acc: 0.988\n",
      "46:300 - loss: 0.034224736557213926, acc: 0.9866666666666667\n",
      "46:350 - loss: 0.029602791626705184, acc: 0.9885714285714285\n",
      "46:400 - loss: 0.02724070975090054, acc: 0.99\n",
      "46:450 - loss: 0.024268579321841517, acc: 0.9911111111111112\n",
      "46:500 - loss: 0.021898081597638386, acc: 0.992\n",
      "46:550 - loss: 0.024350598932254112, acc: 0.990909090909091\n",
      "epoch: 46 - train loss: 0.02368881455427625, train acc: 0.991166077738516\n",
      "epoch: 46 - test loss: 1.171481149886334, test acc: 0.5714285714285714\n",
      "47:50 - loss: 0.05070069044482416, acc: 0.96\n",
      "47:100 - loss: 0.035618155704122925, acc: 0.97\n",
      "47:150 - loss: 0.02385744923225924, acc: 0.98\n",
      "47:200 - loss: 0.017961444388743225, acc: 0.985\n",
      "47:250 - loss: 0.02039709876424652, acc: 0.984\n",
      "47:300 - loss: 0.021598321963027876, acc: 0.9833333333333333\n",
      "47:350 - loss: 0.018554251522707618, acc: 0.9857142857142858\n",
      "47:400 - loss: 0.01706374459484237, acc: 0.9875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47:450 - loss: 0.015195056090219585, acc: 0.9888888888888889\n",
      "47:500 - loss: 0.021132529652216098, acc: 0.986\n",
      "47:550 - loss: 0.02284699402236588, acc: 0.9854545454545455\n",
      "epoch: 47 - train loss: 0.02221770014573934, train acc: 0.9858657243816255\n",
      "epoch: 47 - test loss: 0.9736134101871254, test acc: 0.5925925925925926\n",
      "48:50 - loss: 0.0005312403178785842, acc: 1.0\n",
      "48:100 - loss: 0.017554080465044912, acc: 0.99\n",
      "48:150 - loss: 0.02141813491385966, acc: 0.9866666666666667\n",
      "48:200 - loss: 0.02782613374945586, acc: 0.985\n",
      "48:250 - loss: 0.022521000186786086, acc: 0.988\n",
      "48:300 - loss: 0.01881529949479953, acc: 0.99\n",
      "48:350 - loss: 0.016162941568849485, acc: 0.9914285714285714\n",
      "48:400 - loss: 0.014168408141160178, acc: 0.9925\n",
      "48:450 - loss: 0.013780737668645416, acc: 0.9933333333333333\n",
      "48:500 - loss: 0.019581519704044238, acc: 0.99\n",
      "48:550 - loss: 0.01803910780663822, acc: 0.990909090909091\n",
      "epoch: 48 - train loss: 0.01753186536181098, train acc: 0.991166077738516\n",
      "epoch: 48 - test loss: 1.30431130815657, test acc: 0.5396825396825397\n",
      "49:50 - loss: 0.04515182365131521, acc: 0.98\n",
      "49:100 - loss: 0.04657710888578845, acc: 0.97\n",
      "49:150 - loss: 0.03112827526961451, acc: 0.98\n",
      "49:200 - loss: 0.025526342769087395, acc: 0.985\n",
      "49:250 - loss: 0.027753934200446816, acc: 0.984\n",
      "49:300 - loss: 0.023182310464586773, acc: 0.9866666666666667\n",
      "49:350 - loss: 0.019905445326807696, acc: 0.9885714285714285\n",
      "49:400 - loss: 0.019266146274614364, acc: 0.9875\n",
      "49:450 - loss: 0.021888607505615013, acc: 0.9866666666666667\n",
      "49:500 - loss: 0.02095653594848798, acc: 0.988\n",
      "49:550 - loss: 0.019064271301700852, acc: 0.9890909090909091\n",
      "epoch: 49 - train loss: 0.01852843010394271, train acc: 0.9893992932862191\n",
      "epoch: 49 - test loss: 1.490817233328165, test acc: 0.544973544973545\n",
      "50:50 - loss: 0.06571488485962725, acc: 0.98\n",
      "50:100 - loss: 0.03297950625571963, acc: 0.99\n",
      "50:150 - loss: 0.029277434352226493, acc: 0.9866666666666667\n",
      "50:200 - loss: 0.022023431981633222, acc: 0.99\n",
      "50:250 - loss: 0.017673931115874317, acc: 0.992\n",
      "50:300 - loss: 0.01835081807439337, acc: 0.99\n",
      "50:350 - loss: 0.02017646446764053, acc: 0.9885714285714285\n",
      "50:400 - loss: 0.017686999283003664, acc: 0.99\n",
      "50:450 - loss: 0.019202961207385086, acc: 0.9888888888888889\n",
      "50:500 - loss: 0.02295293437928685, acc: 0.986\n",
      "50:550 - loss: 0.022753116246977335, acc: 0.9854545454545455\n",
      "epoch: 50 - train loss: 0.022115131637005313, train acc: 0.9858657243816255\n",
      "epoch: 50 - test loss: 1.1502823785370304, test acc: 0.5873015873015873\n",
      "51:50 - loss: 0.00012660974840880451, acc: 1.0\n",
      "51:100 - loss: 0.02079862564461664, acc: 0.98\n",
      "51:150 - loss: 0.026458677960545907, acc: 0.98\n",
      "51:200 - loss: 0.03165066340904407, acc: 0.98\n",
      "51:250 - loss: 0.025356048635854497, acc: 0.984\n",
      "51:300 - loss: 0.021181365649686684, acc: 0.9866666666666667\n",
      "51:350 - loss: 0.028651502423295762, acc: 0.9857142857142858\n",
      "51:400 - loss: 0.028078697310089006, acc: 0.9875\n",
      "51:450 - loss: 0.024978718984203008, acc: 0.9888888888888889\n",
      "51:500 - loss: 0.022489751265526166, acc: 0.99\n",
      "51:550 - loss: 0.020673027753901497, acc: 0.990909090909091\n",
      "epoch: 51 - train loss: 0.020092138382174923, train acc: 0.991166077738516\n",
      "epoch: 51 - test loss: 1.1553621332990887, test acc: 0.582010582010582\n",
      "52:50 - loss: 0.0850687778446152, acc: 0.96\n",
      "52:100 - loss: 0.06742198192139731, acc: 0.97\n",
      "52:150 - loss: 0.050052323482147075, acc: 0.9733333333333334\n",
      "52:200 - loss: 0.03901357502686906, acc: 0.98\n",
      "52:250 - loss: 0.03126310012685677, acc: 0.984\n",
      "52:300 - loss: 0.026066372417129707, acc: 0.9866666666666667\n",
      "52:350 - loss: 0.029144037904375277, acc: 0.9857142857142858\n",
      "52:400 - loss: 0.027472219831312504, acc: 0.985\n",
      "52:450 - loss: 0.028198406901005897, acc: 0.9844444444444445\n",
      "52:500 - loss: 0.02540965094617303, acc: 0.986\n",
      "52:550 - loss: 0.024959384110149307, acc: 0.9854545454545455\n",
      "epoch: 52 - train loss: 0.024256916443830694, train acc: 0.9858657243816255\n",
      "epoch: 52 - test loss: 1.3656557026499936, test acc: 0.5608465608465608\n",
      "53:50 - loss: 0.026053732551475795, acc: 0.98\n",
      "53:100 - loss: 0.022386850442286343, acc: 0.98\n",
      "53:150 - loss: 0.01726948367993153, acc: 0.9866666666666667\n",
      "53:200 - loss: 0.0192872149074313, acc: 0.985\n",
      "53:250 - loss: 0.01547327423863103, acc: 0.988\n",
      "53:300 - loss: 0.01292715728943737, acc: 0.99\n",
      "53:350 - loss: 0.024742799045754275, acc: 0.98\n",
      "53:400 - loss: 0.024803592023157246, acc: 0.98\n",
      "53:450 - loss: 0.022073740579220357, acc: 0.9822222222222222\n",
      "53:500 - loss: 0.019882367372914996, acc: 0.984\n",
      "53:550 - loss: 0.019636223411392338, acc: 0.9836363636363636\n",
      "epoch: 53 - train loss: 0.02155311213408756, train acc: 0.9823321554770318\n",
      "epoch: 53 - test loss: 1.2645437402973578, test acc: 0.5978835978835979\n",
      "54:50 - loss: 0.010459324952994018, acc: 1.0\n",
      "54:100 - loss: 0.006435916817112838, acc: 1.0\n",
      "54:150 - loss: 0.02514389080131922, acc: 0.9933333333333333\n",
      "54:200 - loss: 0.018887934818234205, acc: 0.995\n",
      "54:250 - loss: 0.015135972679194776, acc: 0.996\n",
      "54:300 - loss: 0.016436471873460336, acc: 0.9933333333333333\n",
      "54:350 - loss: 0.015346996853873586, acc: 0.9942857142857143\n",
      "54:400 - loss: 0.013476169777329017, acc: 0.995\n",
      "54:450 - loss: 0.012014703189908533, acc: 0.9955555555555555\n",
      "54:500 - loss: 0.01117966679629943, acc: 0.996\n",
      "54:550 - loss: 0.01406518362559251, acc: 0.9945454545454545\n",
      "epoch: 54 - train loss: 0.01496323661454711, train acc: 0.9929328621908127\n",
      "epoch: 54 - test loss: 1.0269191778507543, test acc: 0.6084656084656085\n",
      "55:50 - loss: 0.0001782389024182631, acc: 1.0\n",
      "55:100 - loss: 0.00015213356712645576, acc: 1.0\n",
      "55:150 - loss: 0.022997087821960274, acc: 0.9866666666666667\n",
      "55:200 - loss: 0.017922619696290207, acc: 0.99\n",
      "55:250 - loss: 0.014348564672807885, acc: 0.992\n",
      "55:300 - loss: 0.02070906487392956, acc: 0.99\n",
      "55:350 - loss: 0.017783293518632167, acc: 0.9914285714285714\n",
      "55:400 - loss: 0.021000685027922378, acc: 0.9875\n",
      "55:450 - loss: 0.021210002534452533, acc: 0.9866666666666667\n",
      "55:500 - loss: 0.019094259292130333, acc: 0.988\n",
      "55:550 - loss: 0.019856515129494646, acc: 0.9872727272727273\n",
      "epoch: 55 - train loss: 0.01929801041620983, train acc: 0.9876325088339223\n",
      "epoch: 55 - test loss: 1.015343081977279, test acc: 0.544973544973545\n",
      "56:50 - loss: 7.19946578342396e-05, acc: 1.0\n",
      "56:100 - loss: 0.00783682402931174, acc: 0.99\n",
      "56:150 - loss: 0.006515695915015134, acc: 0.9933333333333333\n",
      "56:200 - loss: 0.01666211517072492, acc: 0.985\n",
      "56:250 - loss: 0.03284007824886096, acc: 0.976\n",
      "56:300 - loss: 0.02738930590076277, acc: 0.98\n",
      "56:350 - loss: 0.02749587045362934, acc: 0.98\n",
      "56:400 - loss: 0.02406793538751793, acc: 0.9825\n",
      "56:450 - loss: 0.021401442962200992, acc: 0.9844444444444445\n",
      "56:500 - loss: 0.019363802602274507, acc: 0.986\n",
      "56:550 - loss: 0.017612942393123485, acc: 0.9872727272727273\n",
      "epoch: 56 - train loss: 0.017134322779597082, train acc: 0.9876325088339223\n",
      "epoch: 56 - test loss: 1.8273599192293202, test acc: 0.582010582010582\n",
      "57:50 - loss: 0.618719483202755, acc: 0.82\n",
      "57:100 - loss: 0.47123742896117365, acc: 0.81\n",
      "57:150 - loss: 0.4434840939556316, acc: 0.8066666666666666\n",
      "57:200 - loss: 0.39982012536311407, acc: 0.83\n",
      "57:250 - loss: 0.4328707112606125, acc: 0.824\n",
      "57:300 - loss: 0.41610746216134636, acc: 0.84\n",
      "57:350 - loss: 0.3798685488685754, acc: 0.8571428571428571\n",
      "57:400 - loss: 0.3620325698590971, acc: 0.8675\n",
      "57:450 - loss: 0.35483798730228805, acc: 0.8711111111111111\n",
      "57:500 - loss: 0.35371830486015937, acc: 0.864\n",
      "57:550 - loss: 0.3584454620774178, acc: 0.86\n",
      "epoch: 57 - train loss: 0.3503461212733323, train acc: 0.8639575971731449\n",
      "epoch: 57 - test loss: 1.0017353918573448, test acc: 0.544973544973545\n",
      "58:50 - loss: 0.18835302283059996, acc: 0.92\n",
      "58:100 - loss: 0.18619387536822213, acc: 0.92\n",
      "58:150 - loss: 0.1569894594629937, acc: 0.9333333333333333\n",
      "58:200 - loss: 0.1448328500857893, acc: 0.935\n",
      "58:250 - loss: 0.1690107568791891, acc: 0.932\n",
      "58:300 - loss: 0.15853102533339372, acc: 0.9366666666666666\n",
      "58:350 - loss: 0.14161012088544955, acc: 0.9457142857142857\n",
      "58:400 - loss: 0.1344083905361096, acc: 0.9475\n",
      "58:450 - loss: 0.1269709430926013, acc: 0.9488888888888889\n",
      "58:500 - loss: 0.12064169412935163, acc: 0.95\n",
      "58:550 - loss: 0.11461032189605025, acc: 0.9527272727272728\n",
      "epoch: 58 - train loss: 0.11742874691474468, train acc: 0.9522968197879859\n",
      "epoch: 58 - test loss: 0.8818378191929461, test acc: 0.544973544973545\n",
      "59:50 - loss: 0.03968930391865695, acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59:100 - loss: 0.02497564619311543, acc: 1.0\n",
      "59:150 - loss: 0.03363956270306649, acc: 0.9866666666666667\n",
      "59:200 - loss: 0.12836455365763436, acc: 0.965\n",
      "59:250 - loss: 0.10991769969049883, acc: 0.968\n",
      "59:300 - loss: 0.10020432284828262, acc: 0.97\n",
      "59:350 - loss: 0.09097771681648704, acc: 0.9742857142857143\n",
      "59:400 - loss: 0.08510611278915964, acc: 0.975\n",
      "59:450 - loss: 0.07767803722935582, acc: 0.9777777777777777\n",
      "59:500 - loss: 0.07125799552013574, acc: 0.98\n",
      "59:550 - loss: 0.06938488226329663, acc: 0.98\n",
      "epoch: 59 - train loss: 0.06851129507618256, train acc: 0.980565371024735\n",
      "epoch: 59 - test loss: 1.0000642805368838, test acc: 0.5661375661375662\n",
      "60:50 - loss: 0.024338321139015778, acc: 0.98\n",
      "60:100 - loss: 0.04877823226194966, acc: 0.96\n",
      "60:150 - loss: 0.03957108397248469, acc: 0.9666666666666667\n",
      "60:200 - loss: 0.032486940013374876, acc: 0.975\n",
      "60:250 - loss: 0.02681491111727964, acc: 0.98\n",
      "60:300 - loss: 0.024376285873888584, acc: 0.9833333333333333\n",
      "60:350 - loss: 0.05568526259587778, acc: 0.9771428571428571\n",
      "60:400 - loss: 0.06057114726037195, acc: 0.975\n",
      "60:450 - loss: 0.058817357820191096, acc: 0.9755555555555555\n",
      "60:500 - loss: 0.06088872022388819, acc: 0.976\n",
      "60:550 - loss: 0.061201366795008726, acc: 0.9745454545454545\n",
      "epoch: 60 - train loss: 0.062273680947533015, train acc: 0.9734982332155477\n",
      "epoch: 60 - test loss: 0.8217387080781324, test acc: 0.6190476190476191\n",
      "61:50 - loss: 0.03497371798003678, acc: 0.98\n",
      "61:100 - loss: 0.026894874927513395, acc: 0.99\n",
      "61:150 - loss: 0.03157351821959306, acc: 0.98\n",
      "61:200 - loss: 0.03042967394173436, acc: 0.98\n",
      "61:250 - loss: 0.02799821083047727, acc: 0.98\n",
      "61:300 - loss: 0.023659225769909236, acc: 0.9833333333333333\n",
      "61:350 - loss: 0.02204035762749511, acc: 0.9857142857142858\n",
      "61:400 - loss: 0.024192227127352025, acc: 0.985\n",
      "61:450 - loss: 0.023007350877161537, acc: 0.9866666666666667\n",
      "61:500 - loss: 0.03333424807649717, acc: 0.984\n",
      "61:550 - loss: 0.03715604026360496, acc: 0.9818181818181818\n",
      "epoch: 61 - train loss: 0.03804879037918788, train acc: 0.980565371024735\n",
      "epoch: 61 - test loss: 0.9117788145273397, test acc: 0.5608465608465608\n",
      "62:50 - loss: 0.031214147172311187, acc: 0.98\n",
      "62:100 - loss: 0.03195833592042266, acc: 0.98\n",
      "62:150 - loss: 0.10512491671615355, acc: 0.96\n",
      "62:200 - loss: 0.10258657189550625, acc: 0.965\n",
      "62:250 - loss: 0.10196449365390378, acc: 0.964\n",
      "62:300 - loss: 0.102042014516555, acc: 0.9633333333333334\n",
      "62:350 - loss: 0.09525952734435836, acc: 0.9657142857142857\n",
      "62:400 - loss: 0.0854935652470176, acc: 0.97\n",
      "62:450 - loss: 0.0794311482478962, acc: 0.9711111111111111\n",
      "62:500 - loss: 0.07509748499003967, acc: 0.972\n",
      "62:550 - loss: 0.08954262986763017, acc: 0.9672727272727273\n",
      "epoch: 62 - train loss: 0.08808648527574764, train acc: 0.9681978798586572\n",
      "epoch: 62 - test loss: 0.9545991040277138, test acc: 0.5238095238095238\n",
      "63:50 - loss: 0.036981330741149246, acc: 0.96\n",
      "63:100 - loss: 0.024208334226096313, acc: 0.98\n",
      "63:150 - loss: 0.03895553354839138, acc: 0.9733333333333334\n",
      "63:200 - loss: 0.038148348897617576, acc: 0.975\n",
      "63:250 - loss: 0.03507574838681902, acc: 0.976\n",
      "63:300 - loss: 0.0342231358173376, acc: 0.9766666666666667\n",
      "63:350 - loss: 0.032523789986249434, acc: 0.98\n",
      "63:400 - loss: 0.031881320557766074, acc: 0.98\n",
      "63:450 - loss: 0.038287546976399606, acc: 0.98\n",
      "63:500 - loss: 0.03971548881636065, acc: 0.98\n",
      "63:550 - loss: 0.037258469164517534, acc: 0.9818181818181818\n",
      "epoch: 63 - train loss: 0.036275327590730266, train acc: 0.9823321554770318\n",
      "epoch: 63 - test loss: 1.0032201203847455, test acc: 0.5555555555555556\n",
      "64:50 - loss: 0.005010337341103752, acc: 1.0\n",
      "64:100 - loss: 0.005517862063572048, acc: 1.0\n",
      "64:150 - loss: 0.012506782919363963, acc: 1.0\n",
      "64:200 - loss: 0.012667664996320278, acc: 1.0\n",
      "64:250 - loss: 0.0183132257915842, acc: 0.996\n",
      "64:300 - loss: 0.015948813140869513, acc: 0.9966666666666667\n",
      "64:350 - loss: 0.021801831728357677, acc: 0.9942857142857143\n",
      "64:400 - loss: 0.037649665601403794, acc: 0.99\n",
      "64:450 - loss: 0.03849987267847773, acc: 0.9888888888888889\n",
      "64:500 - loss: 0.04843102583157035, acc: 0.984\n",
      "64:550 - loss: 0.0647677515269911, acc: 0.98\n",
      "epoch: 64 - train loss: 0.06433389937802196, train acc: 0.980565371024735\n",
      "epoch: 64 - test loss: 1.1090673460712417, test acc: 0.5661375661375662\n",
      "65:50 - loss: 0.09407636093570187, acc: 0.96\n",
      "65:100 - loss: 0.052837355153024665, acc: 0.98\n",
      "65:150 - loss: 0.03566184368679057, acc: 0.9866666666666667\n",
      "65:200 - loss: 0.04049066550374997, acc: 0.985\n",
      "65:250 - loss: 0.05168191770091804, acc: 0.972\n",
      "65:300 - loss: 0.04362879502103565, acc: 0.9766666666666667\n",
      "65:350 - loss: 0.08306763850900191, acc: 0.9685714285714285\n",
      "65:400 - loss: 0.07960155227115044, acc: 0.97\n",
      "65:450 - loss: 0.0747058464199223, acc: 0.9711111111111111\n",
      "65:500 - loss: 0.06887753403798655, acc: 0.974\n",
      "65:550 - loss: 0.06514496894522584, acc: 0.9745454545454545\n",
      "epoch: 65 - train loss: 0.06335914193206973, train acc: 0.9752650176678446\n",
      "epoch: 65 - test loss: 0.9258102446386459, test acc: 0.5502645502645502\n",
      "66:50 - loss: 0.006392621642496063, acc: 1.0\n",
      "66:100 - loss: 0.0413047087100709, acc: 0.97\n",
      "66:150 - loss: 0.058993575669366756, acc: 0.96\n",
      "66:200 - loss: 0.07359248199426485, acc: 0.955\n",
      "66:250 - loss: 0.08635256547295525, acc: 0.948\n",
      "66:300 - loss: 0.09926224433014948, acc: 0.9433333333333334\n",
      "66:350 - loss: 0.12343405893887535, acc: 0.9457142857142857\n",
      "66:400 - loss: 0.12594078108888238, acc: 0.945\n",
      "66:450 - loss: 0.11829669155187321, acc: 0.9488888888888889\n",
      "66:500 - loss: 0.11340212546878685, acc: 0.95\n",
      "66:550 - loss: 0.10687644926331401, acc: 0.9527272727272728\n",
      "epoch: 66 - train loss: 0.10396404150988504, train acc: 0.9540636042402827\n",
      "epoch: 66 - test loss: 0.8874508728378949, test acc: 0.5608465608465608\n",
      "67:50 - loss: 0.08808905392351882, acc: 0.94\n",
      "67:100 - loss: 0.06175445471024395, acc: 0.96\n",
      "67:150 - loss: 0.05382991834666323, acc: 0.9666666666666667\n",
      "67:200 - loss: 0.057366262159434724, acc: 0.96\n",
      "67:250 - loss: 0.049500752689606804, acc: 0.968\n",
      "67:300 - loss: 0.042244102842143874, acc: 0.9733333333333334\n",
      "67:350 - loss: 0.04923355960327326, acc: 0.9742857142857143\n",
      "67:400 - loss: 0.04768404262013828, acc: 0.975\n",
      "67:450 - loss: 0.04270686567930212, acc: 0.9777777777777777\n",
      "67:500 - loss: 0.03918232554095476, acc: 0.98\n",
      "67:550 - loss: 0.039474328961807324, acc: 0.9781818181818182\n",
      "epoch: 67 - train loss: 0.03864648863906068, train acc: 0.9787985865724381\n",
      "epoch: 67 - test loss: 0.8374315498964056, test acc: 0.5873015873015873\n",
      "68:50 - loss: 0.024553679453003113, acc: 1.0\n",
      "68:100 - loss: 0.013733893534267308, acc: 1.0\n",
      "68:150 - loss: 0.009685324860880357, acc: 1.0\n",
      "68:200 - loss: 0.0077687517629171175, acc: 1.0\n",
      "68:250 - loss: 0.006664028818977121, acc: 1.0\n",
      "68:300 - loss: 0.012615995418382134, acc: 0.9933333333333333\n",
      "68:350 - loss: 0.015769050063400104, acc: 0.9885714285714285\n",
      "68:400 - loss: 0.0178340222692132, acc: 0.9875\n",
      "68:450 - loss: 0.01777024953807506, acc: 0.9888888888888889\n",
      "68:500 - loss: 0.018884800927785998, acc: 0.988\n",
      "68:550 - loss: 0.01879972462290676, acc: 0.9872727272727273\n",
      "epoch: 68 - train loss: 0.018287244877914246, train acc: 0.9876325088339223\n",
      "epoch: 68 - test loss: 1.210569432896673, test acc: 0.582010582010582\n",
      "69:50 - loss: 0.01138683067382501, acc: 1.0\n",
      "69:100 - loss: 0.006240433034630684, acc: 1.0\n",
      "69:150 - loss: 0.006974035429629408, acc: 1.0\n",
      "69:200 - loss: 0.01718927915378404, acc: 0.99\n",
      "69:250 - loss: 0.015366672910893197, acc: 0.992\n",
      "69:300 - loss: 0.012947630570630869, acc: 0.9933333333333333\n",
      "69:350 - loss: 0.011162596625573987, acc: 0.9942857142857143\n",
      "69:400 - loss: 0.013899576978258039, acc: 0.9925\n",
      "69:450 - loss: 0.013932422323513452, acc: 0.9933333333333333\n",
      "69:500 - loss: 0.013330112253865814, acc: 0.994\n",
      "69:550 - loss: 0.017211022379405017, acc: 0.9927272727272727\n",
      "epoch: 69 - train loss: 0.016745364056051603, train acc: 0.9929328621908127\n",
      "epoch: 69 - test loss: 0.9378996277486448, test acc: 0.5661375661375662\n",
      "70:50 - loss: 0.039162371785839534, acc: 0.98\n",
      "70:100 - loss: 0.01987278928690035, acc: 0.99\n",
      "70:150 - loss: 0.013513934565073684, acc: 0.9933333333333333\n",
      "70:200 - loss: 0.02658109138186068, acc: 0.98\n",
      "70:250 - loss: 0.021386160215144244, acc: 0.984\n",
      "70:300 - loss: 0.030435120236095563, acc: 0.9733333333333334\n",
      "70:350 - loss: 0.026170491277490566, acc: 0.9771428571428571\n",
      "70:400 - loss: 0.026152158335046864, acc: 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70:450 - loss: 0.02328813193431343, acc: 0.9822222222222222\n",
      "70:500 - loss: 0.020991745214650605, acc: 0.984\n",
      "70:550 - loss: 0.01912051076109265, acc: 0.9854545454545455\n",
      "epoch: 70 - train loss: 0.01859584514422063, train acc: 0.9858657243816255\n",
      "epoch: 70 - test loss: 0.907350183754939, test acc: 0.5396825396825397\n",
      "71:50 - loss: 0.009427242450918454, acc: 1.0\n",
      "71:100 - loss: 0.027439489180171292, acc: 0.99\n",
      "71:150 - loss: 0.020516613097789453, acc: 0.9933333333333333\n",
      "71:200 - loss: 0.023013220908125267, acc: 0.99\n",
      "71:250 - loss: 0.020719495833281764, acc: 0.992\n",
      "71:300 - loss: 0.0218303599422496, acc: 0.99\n",
      "71:350 - loss: 0.02225414154386261, acc: 0.9885714285714285\n",
      "71:400 - loss: 0.026717850479795427, acc: 0.9825\n",
      "71:450 - loss: 0.02377643646387639, acc: 0.9844444444444445\n",
      "71:500 - loss: 0.021424009275871594, acc: 0.986\n",
      "71:550 - loss: 0.0195035509219851, acc: 0.9872727272727273\n",
      "epoch: 71 - train loss: 0.018960818855617796, train acc: 0.9876325088339223\n",
      "epoch: 71 - test loss: 0.9577062965408104, test acc: 0.5555555555555556\n",
      "72:50 - loss: 0.0002647094488427487, acc: 1.0\n",
      "72:100 - loss: 0.000264971737992122, acc: 1.0\n",
      "72:150 - loss: 0.017793612398577328, acc: 0.9866666666666667\n",
      "72:200 - loss: 0.025981813782285156, acc: 0.98\n",
      "72:250 - loss: 0.02564727717618289, acc: 0.98\n",
      "72:300 - loss: 0.0214164441395576, acc: 0.9833333333333333\n",
      "72:350 - loss: 0.020539594644577486, acc: 0.9828571428571429\n",
      "72:400 - loss: 0.018013248671576377, acc: 0.985\n",
      "72:450 - loss: 0.017016359672048387, acc: 0.9866666666666667\n",
      "72:500 - loss: 0.026051334291110523, acc: 0.986\n",
      "72:550 - loss: 0.02532473158249755, acc: 0.9854545454545455\n",
      "epoch: 72 - train loss: 0.024642472141635598, train acc: 0.9858657243816255\n",
      "epoch: 72 - test loss: 1.1286618393152617, test acc: 0.5925925925925926\n",
      "73:50 - loss: 0.007177067955295348, acc: 1.0\n",
      "73:100 - loss: 0.006925020870340647, acc: 1.0\n",
      "73:150 - loss: 0.005307133634304949, acc: 1.0\n",
      "73:200 - loss: 0.0041937582894122334, acc: 1.0\n",
      "73:250 - loss: 0.003589537167711277, acc: 1.0\n",
      "73:300 - loss: 0.016576917646774436, acc: 0.9966666666666667\n",
      "73:350 - loss: 0.014290596788801912, acc: 0.9971428571428571\n",
      "73:400 - loss: 0.012567319643031809, acc: 0.9975\n",
      "73:450 - loss: 0.012694018913185405, acc: 0.9977777777777778\n",
      "73:500 - loss: 0.017759482025168607, acc: 0.996\n",
      "73:550 - loss: 0.017025193156188882, acc: 0.9963636363636363\n",
      "epoch: 73 - train loss: 0.016555566871172364, train acc: 0.9964664310954063\n",
      "epoch: 73 - test loss: 0.9254649203190539, test acc: 0.6084656084656085\n",
      "74:50 - loss: 0.03054284477146769, acc: 0.98\n",
      "74:100 - loss: 0.01550545801378875, acc: 0.99\n",
      "74:150 - loss: 0.014977603661875257, acc: 0.9933333333333333\n",
      "74:200 - loss: 0.020438947329777305, acc: 0.99\n",
      "74:250 - loss: 0.01641232958750981, acc: 0.992\n",
      "74:300 - loss: 0.021022416907865597, acc: 0.99\n",
      "74:350 - loss: 0.019594132673453328, acc: 0.9914285714285714\n",
      "74:400 - loss: 0.01727004181368322, acc: 0.9925\n",
      "74:450 - loss: 0.015412917653996055, acc: 0.9933333333333333\n",
      "74:500 - loss: 0.020326426289138257, acc: 0.988\n",
      "74:550 - loss: 0.01883622402966333, acc: 0.9890909090909091\n",
      "epoch: 74 - train loss: 0.018308018736985114, train acc: 0.9893992932862191\n",
      "epoch: 74 - test loss: 1.149614640389618, test acc: 0.5714285714285714\n",
      "75:50 - loss: 0.0004957430650529622, acc: 1.0\n",
      "75:100 - loss: 0.0014674495452536493, acc: 1.0\n",
      "75:150 - loss: 0.014397648973473683, acc: 0.9933333333333333\n",
      "75:200 - loss: 0.021601547693889316, acc: 0.99\n",
      "75:250 - loss: 0.021134475973636, acc: 0.988\n",
      "75:300 - loss: 0.022449108467014398, acc: 0.9866666666666667\n",
      "75:350 - loss: 0.021811052205332755, acc: 0.9857142857142858\n",
      "75:400 - loss: 0.01911147149435722, acc: 0.9875\n",
      "75:450 - loss: 0.02374829586707887, acc: 0.9844444444444445\n",
      "75:500 - loss: 0.021388269315068584, acc: 0.986\n",
      "75:550 - loss: 0.019463561773729068, acc: 0.9872727272727273\n",
      "epoch: 75 - train loss: 0.018916859878052354, train acc: 0.9876325088339223\n",
      "epoch: 75 - test loss: 1.1519714519933375, test acc: 0.5873015873015873\n",
      "76:50 - loss: 0.005149071737745005, acc: 1.0\n",
      "76:100 - loss: 0.002649489167414835, acc: 1.0\n",
      "76:150 - loss: 0.0018226461431288458, acc: 1.0\n",
      "76:200 - loss: 0.0013896406556961874, acc: 1.0\n",
      "76:250 - loss: 0.011022689851612564, acc: 0.996\n",
      "76:300 - loss: 0.014548389837671509, acc: 0.9933333333333333\n",
      "76:350 - loss: 0.012503265769893702, acc: 0.9942857142857143\n",
      "76:400 - loss: 0.015187459457705525, acc: 0.9925\n",
      "76:450 - loss: 0.013510988213796142, acc: 0.9933333333333333\n",
      "76:500 - loss: 0.01244924780379415, acc: 0.994\n",
      "76:550 - loss: 0.018142052348169922, acc: 0.990909090909091\n",
      "epoch: 76 - train loss: 0.01763407186445256, train acc: 0.991166077738516\n",
      "epoch: 76 - test loss: 1.154553790913887, test acc: 0.5608465608465608\n",
      "77:50 - loss: 0.030679502829865944, acc: 0.98\n",
      "77:100 - loss: 0.04658580437101948, acc: 0.97\n",
      "77:150 - loss: 0.04392807284873939, acc: 0.9666666666666667\n",
      "77:200 - loss: 0.032985589704970016, acc: 0.975\n",
      "77:250 - loss: 0.030878414380419675, acc: 0.98\n",
      "77:300 - loss: 0.025781852069701447, acc: 0.9833333333333333\n",
      "77:350 - loss: 0.022788083143564272, acc: 0.9857142857142858\n",
      "77:400 - loss: 0.02305553596643075, acc: 0.985\n",
      "77:450 - loss: 0.02263887997867427, acc: 0.9844444444444445\n",
      "77:500 - loss: 0.020383007210944517, acc: 0.986\n",
      "77:550 - loss: 0.019511839187016346, acc: 0.9872727272727273\n",
      "epoch: 77 - train loss: 0.01896198507836831, train acc: 0.9876325088339223\n",
      "epoch: 77 - test loss: 1.2295464418214657, test acc: 0.5555555555555556\n",
      "78:50 - loss: 8.241411564733723e-05, acc: 1.0\n",
      "78:100 - loss: 0.0023603864581613187, acc: 1.0\n",
      "78:150 - loss: 0.0032037650738446386, acc: 1.0\n",
      "78:200 - loss: 0.016034321273587587, acc: 0.995\n",
      "78:250 - loss: 0.01868464906478738, acc: 0.992\n",
      "78:300 - loss: 0.018627478898732973, acc: 0.99\n",
      "78:350 - loss: 0.015977197855767625, acc: 0.9914285714285714\n",
      "78:400 - loss: 0.019252333383212552, acc: 0.9875\n",
      "78:450 - loss: 0.017262836350559394, acc: 0.9888888888888889\n",
      "78:500 - loss: 0.019041071902922712, acc: 0.988\n",
      "78:550 - loss: 0.017315852056890707, acc: 0.9890909090909091\n",
      "epoch: 78 - train loss: 0.016829275348681182, train acc: 0.9893992932862191\n",
      "epoch: 78 - test loss: 1.3589465764797337, test acc: 0.582010582010582\n",
      "79:50 - loss: 0.007778258612588017, acc: 1.0\n",
      "79:100 - loss: 0.003911277072300828, acc: 1.0\n",
      "79:150 - loss: 0.02788315785987928, acc: 0.9933333333333333\n",
      "79:200 - loss: 0.021242706903601635, acc: 0.995\n",
      "79:250 - loss: 0.020455426668330473, acc: 0.992\n",
      "79:300 - loss: 0.01707214279216767, acc: 0.9933333333333333\n",
      "79:350 - loss: 0.015400081053667278, acc: 0.9942857142857143\n",
      "79:400 - loss: 0.013786808719162666, acc: 0.995\n",
      "79:450 - loss: 0.016899328200596968, acc: 0.9933333333333333\n",
      "79:500 - loss: 0.01755627913441299, acc: 0.992\n",
      "79:550 - loss: 0.01819489326799022, acc: 0.990909090909091\n",
      "epoch: 79 - train loss: 0.018805034386264508, train acc: 0.991166077738516\n",
      "epoch: 79 - test loss: 0.876005172913918, test acc: 0.5714285714285714\n",
      "80:50 - loss: 0.033078838707677444, acc: 0.98\n",
      "80:100 - loss: 0.022117536301718355, acc: 0.99\n",
      "80:150 - loss: 0.014793813318678269, acc: 0.9933333333333333\n",
      "80:200 - loss: 0.01885896383220044, acc: 0.99\n",
      "80:250 - loss: 0.01512627878551943, acc: 0.992\n",
      "80:300 - loss: 0.013968432850116862, acc: 0.9933333333333333\n",
      "80:350 - loss: 0.018005686096500436, acc: 0.9914285714285714\n",
      "80:400 - loss: 0.016958383232550455, acc: 0.9925\n",
      "80:450 - loss: 0.017983605432882202, acc: 0.9911111111111112\n",
      "80:500 - loss: 0.016199050243684718, acc: 0.992\n",
      "80:550 - loss: 0.01795751525950815, acc: 0.990909090909091\n",
      "epoch: 80 - train loss: 0.017451685359910762, train acc: 0.991166077738516\n",
      "epoch: 80 - test loss: 1.2979617964229522, test acc: 0.5396825396825397\n",
      "81:50 - loss: 0.02046028797950187, acc: 0.98\n",
      "81:100 - loss: 0.019234002248887924, acc: 0.98\n",
      "81:150 - loss: 0.012854487903797751, acc: 0.9866666666666667\n",
      "81:200 - loss: 0.009659156329371452, acc: 0.99\n",
      "81:250 - loss: 0.009709132759413224, acc: 0.992\n",
      "81:300 - loss: 0.009115377474720025, acc: 0.9933333333333333\n",
      "81:350 - loss: 0.008592431496114305, acc: 0.9942857142857143\n",
      "81:400 - loss: 0.016324582191452337, acc: 0.99\n",
      "81:450 - loss: 0.01456618341147538, acc: 0.9911111111111112\n",
      "81:500 - loss: 0.013118708628144416, acc: 0.992\n",
      "81:550 - loss: 0.015801105348906032, acc: 0.9890909090909091\n",
      "epoch: 81 - train loss: 0.01691345504350393, train acc: 0.9876325088339223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 81 - test loss: 1.2666413620598493, test acc: 0.5767195767195767\n",
      "82:50 - loss: 0.00014077801344586992, acc: 1.0\n",
      "82:100 - loss: 0.013005024428769187, acc: 0.99\n",
      "82:150 - loss: 0.014325047857760126, acc: 0.9866666666666667\n",
      "82:200 - loss: 0.02069232075465455, acc: 0.985\n",
      "82:250 - loss: 0.025808560860684348, acc: 0.98\n",
      "82:300 - loss: 0.02151985341907148, acc: 0.9833333333333333\n",
      "82:350 - loss: 0.01845961291768809, acc: 0.9857142857142858\n",
      "82:400 - loss: 0.018482396804129168, acc: 0.985\n",
      "82:450 - loss: 0.016434618430048658, acc: 0.9866666666666667\n",
      "82:500 - loss: 0.01955031591996774, acc: 0.986\n",
      "82:550 - loss: 0.017776663040092656, acc: 0.9872727272727273\n",
      "epoch: 82 - train loss: 0.017276012502176866, train acc: 0.9876325088339223\n",
      "epoch: 82 - test loss: 1.215654469297015, test acc: 0.5873015873015873\n",
      "83:50 - loss: 3.793878695027998e-05, acc: 1.0\n",
      "83:100 - loss: 0.007611893285349131, acc: 0.99\n",
      "83:150 - loss: 0.014981821730468185, acc: 0.9866666666666667\n",
      "83:200 - loss: 0.015094498319724394, acc: 0.985\n",
      "83:250 - loss: 0.016460424002721186, acc: 0.984\n",
      "83:300 - loss: 0.015409998847035424, acc: 0.9866666666666667\n",
      "83:350 - loss: 0.01690667961809994, acc: 0.9857142857142858\n",
      "83:400 - loss: 0.017548914710856168, acc: 0.9875\n",
      "83:450 - loss: 0.020001112546351014, acc: 0.9866666666666667\n",
      "83:500 - loss: 0.018004052452251573, acc: 0.988\n",
      "83:550 - loss: 0.018118061047512325, acc: 0.9872727272727273\n",
      "epoch: 83 - train loss: 0.017606842505452074, train acc: 0.9876325088339223\n",
      "epoch: 83 - test loss: 1.3251699337037064, test acc: 0.5767195767195767\n",
      "84:50 - loss: 0.01747696191108514, acc: 0.98\n",
      "84:100 - loss: 0.00876614695586331, acc: 0.99\n",
      "84:150 - loss: 0.015944869004317935, acc: 0.9866666666666667\n",
      "84:200 - loss: 0.011968101707221662, acc: 0.99\n",
      "84:250 - loss: 0.00958208175578259, acc: 0.992\n",
      "84:300 - loss: 0.007989298524230883, acc: 0.9933333333333333\n",
      "84:350 - loss: 0.011082677095795778, acc: 0.9942857142857143\n",
      "84:400 - loss: 0.00969988791070424, acc: 0.995\n",
      "84:450 - loss: 0.013114120856776724, acc: 0.9933333333333333\n",
      "84:500 - loss: 0.016104451443241404, acc: 0.99\n",
      "84:550 - loss: 0.017823285901560418, acc: 0.9890909090909091\n",
      "epoch: 84 - train loss: 0.01732120825895823, train acc: 0.9893992932862191\n",
      "epoch: 84 - test loss: 1.136776575442445, test acc: 0.5714285714285714\n",
      "85:50 - loss: 6.229452886730818e-05, acc: 1.0\n",
      "85:100 - loss: 0.004936406454522984, acc: 1.0\n",
      "85:150 - loss: 0.014624466663885798, acc: 0.9933333333333333\n",
      "85:200 - loss: 0.014763963960262439, acc: 0.995\n",
      "85:250 - loss: 0.02489099141073518, acc: 0.988\n",
      "85:300 - loss: 0.02253708294193539, acc: 0.99\n",
      "85:350 - loss: 0.020331537105016558, acc: 0.9914285714285714\n",
      "85:400 - loss: 0.01779165527883455, acc: 0.9925\n",
      "85:450 - loss: 0.016224645880834828, acc: 0.9933333333333333\n",
      "85:500 - loss: 0.014603555301589317, acc: 0.994\n",
      "85:550 - loss: 0.013276989080642562, acc: 0.9945454545454545\n",
      "epoch: 85 - train loss: 0.012902827233105575, train acc: 0.9946996466431095\n",
      "epoch: 85 - test loss: 1.1751222377746333, test acc: 0.6084656084656085\n",
      "86:50 - loss: 0.002882862664029518, acc: 1.0\n",
      "86:100 - loss: 0.023968015238745127, acc: 0.99\n",
      "86:150 - loss: 0.021708817057277445, acc: 0.9866666666666667\n",
      "86:200 - loss: 0.016898877343778932, acc: 0.99\n",
      "86:250 - loss: 0.12455427629007253, acc: 0.956\n",
      "86:300 - loss: 0.1959359342295689, acc: 0.92\n",
      "86:350 - loss: 0.2166909049186368, acc: 0.9085714285714286\n",
      "86:400 - loss: 0.23229511846554068, acc: 0.905\n",
      "86:450 - loss: 0.2614242071553719, acc: 0.8933333333333333\n",
      "86:500 - loss: 0.2494708307988195, acc: 0.898\n",
      "86:550 - loss: 0.2519583037297392, acc: 0.9018181818181819\n",
      "epoch: 86 - train loss: 0.24848322710858575, train acc: 0.9028268551236749\n",
      "epoch: 86 - test loss: 0.9021321190898429, test acc: 0.5238095238095238\n",
      "87:50 - loss: 0.1455418133498446, acc: 0.94\n",
      "87:100 - loss: 0.10900139945217878, acc: 0.96\n",
      "87:150 - loss: 0.09754153790044319, acc: 0.96\n",
      "87:200 - loss: 0.09757897906778985, acc: 0.955\n",
      "87:250 - loss: 0.09212555510487011, acc: 0.956\n",
      "87:300 - loss: 0.08012694779105567, acc: 0.96\n",
      "87:350 - loss: 0.07433321491429967, acc: 0.9628571428571429\n",
      "87:400 - loss: 0.06970473222457642, acc: 0.965\n",
      "87:450 - loss: 0.0774881462344347, acc: 0.9622222222222222\n",
      "87:500 - loss: 0.07036217690015806, acc: 0.966\n",
      "87:550 - loss: 0.06442490018080813, acc: 0.9690909090909091\n",
      "epoch: 87 - train loss: 0.06343978482675164, train acc: 0.9699646643109541\n",
      "epoch: 87 - test loss: 1.0616773326543674, test acc: 0.5291005291005291\n",
      "88:50 - loss: 0.02841454143501566, acc: 0.98\n",
      "88:100 - loss: 0.04876978801895295, acc: 0.97\n",
      "88:150 - loss: 0.05053965091014132, acc: 0.9666666666666667\n",
      "88:200 - loss: 0.0681592136164076, acc: 0.97\n",
      "88:250 - loss: 0.0688551546190283, acc: 0.968\n",
      "88:300 - loss: 0.061418403875172854, acc: 0.9733333333333334\n",
      "88:350 - loss: 0.05796573562637965, acc: 0.9742857142857143\n",
      "88:400 - loss: 0.06220027187363441, acc: 0.9725\n",
      "88:450 - loss: 0.0629901805322849, acc: 0.9733333333333334\n",
      "88:500 - loss: 0.06397431349273176, acc: 0.972\n",
      "88:550 - loss: 0.059315071223601086, acc: 0.9745454545454545\n",
      "epoch: 88 - train loss: 0.05776812679863292, train acc: 0.9752650176678446\n",
      "epoch: 88 - test loss: 1.0707657935693506, test acc: 0.5661375661375662\n",
      "89:50 - loss: 0.02081403200020636, acc: 0.98\n",
      "89:100 - loss: 0.03126398720136383, acc: 0.97\n",
      "89:150 - loss: 0.021845426472600796, acc: 0.98\n",
      "89:200 - loss: 0.016913483259205216, acc: 0.985\n",
      "89:250 - loss: 0.014008726287079248, acc: 0.988\n",
      "89:300 - loss: 0.012577000078882192, acc: 0.99\n",
      "89:350 - loss: 0.012457350946526152, acc: 0.9914285714285714\n",
      "89:400 - loss: 0.014626806276965014, acc: 0.99\n",
      "89:450 - loss: 0.013093361733822367, acc: 0.9911111111111112\n",
      "89:500 - loss: 0.012197025752792112, acc: 0.992\n",
      "89:550 - loss: 0.015197478830457757, acc: 0.990909090909091\n",
      "epoch: 89 - train loss: 0.018428695231899293, train acc: 0.9876325088339223\n",
      "epoch: 89 - test loss: 0.8707518210669125, test acc: 0.5873015873015873\n",
      "90:50 - loss: 0.0005255981200684021, acc: 1.0\n",
      "90:100 - loss: 0.000553450598317501, acc: 1.0\n",
      "90:150 - loss: 0.007497383856967544, acc: 0.9933333333333333\n",
      "90:200 - loss: 0.01816215517654332, acc: 0.99\n",
      "90:250 - loss: 0.018729325553135976, acc: 0.988\n",
      "90:300 - loss: 0.02147815872364607, acc: 0.9833333333333333\n",
      "90:350 - loss: 0.02022694345364641, acc: 0.9857142857142858\n",
      "90:400 - loss: 0.017801182548854627, acc: 0.9875\n",
      "90:450 - loss: 0.020043446186285528, acc: 0.9844444444444445\n",
      "90:500 - loss: 0.01809548627168323, acc: 0.986\n",
      "90:550 - loss: 0.01833030164621645, acc: 0.9854545454545455\n",
      "epoch: 90 - train loss: 0.017835826725108576, train acc: 0.9858657243816255\n",
      "epoch: 90 - test loss: 1.1975687966172672, test acc: 0.5714285714285714\n",
      "91:50 - loss: 0.0003010071441378305, acc: 1.0\n",
      "91:100 - loss: 0.006916097180255824, acc: 1.0\n",
      "91:150 - loss: 0.025384069544895933, acc: 0.98\n",
      "91:200 - loss: 0.023563785033178356, acc: 0.98\n",
      "91:250 - loss: 0.023044762914891655, acc: 0.98\n",
      "91:300 - loss: 0.019268090913402397, acc: 0.9833333333333333\n",
      "91:350 - loss: 0.02141093031392167, acc: 0.9828571428571429\n",
      "91:400 - loss: 0.01877015886891847, acc: 0.985\n",
      "91:450 - loss: 0.016727755952973187, acc: 0.9866666666666667\n",
      "91:500 - loss: 0.018044422919078866, acc: 0.984\n",
      "91:550 - loss: 0.01779316069436585, acc: 0.9836363636363636\n",
      "epoch: 91 - train loss: 0.01730248224240152, train acc: 0.9840989399293286\n",
      "epoch: 91 - test loss: 1.1473339614707088, test acc: 0.5502645502645502\n",
      "92:50 - loss: 0.009155077795478514, acc: 1.0\n",
      "92:100 - loss: 0.009228238438436263, acc: 1.0\n",
      "92:150 - loss: 0.01785955676955144, acc: 0.9933333333333333\n",
      "92:200 - loss: 0.01614591966791407, acc: 0.995\n",
      "92:250 - loss: 0.012987859234163845, acc: 0.996\n",
      "92:300 - loss: 0.015051927977838693, acc: 0.9933333333333333\n",
      "92:350 - loss: 0.014907060204157253, acc: 0.9942857142857143\n",
      "92:400 - loss: 0.015505761344647807, acc: 0.9925\n",
      "92:450 - loss: 0.013813060382677196, acc: 0.9933333333333333\n",
      "92:500 - loss: 0.017388422760614065, acc: 0.99\n",
      "92:550 - loss: 0.01582654070334132, acc: 0.990909090909091\n",
      "epoch: 92 - train loss: 0.016724923589480314, train acc: 0.9893992932862191\n",
      "epoch: 92 - test loss: 1.1137420848518789, test acc: 0.5608465608465608\n",
      "93:50 - loss: 0.00013325134145322583, acc: 1.0\n",
      "93:100 - loss: 0.010186305684301351, acc: 1.0\n",
      "93:150 - loss: 0.008492627741480453, acc: 1.0\n",
      "93:200 - loss: 0.006417460135323145, acc: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93:250 - loss: 0.01904089626632267, acc: 0.992\n",
      "93:300 - loss: 0.015892647479993232, acc: 0.9933333333333333\n",
      "93:350 - loss: 0.01768654962692598, acc: 0.9914285714285714\n",
      "93:400 - loss: 0.01550486358883768, acc: 0.9925\n",
      "93:450 - loss: 0.017073550929673463, acc: 0.9911111111111112\n",
      "93:500 - loss: 0.01635662667461336, acc: 0.992\n",
      "93:550 - loss: 0.014884291858033026, acc: 0.9927272727272727\n",
      "epoch: 93 - train loss: 0.014486105380615878, train acc: 0.9929328621908127\n",
      "epoch: 93 - test loss: 1.6233988993629715, test acc: 0.5238095238095238\n",
      "94:50 - loss: 0.025370584171785525, acc: 0.98\n",
      "94:100 - loss: 0.028951916977885093, acc: 0.97\n",
      "94:150 - loss: 0.019343679145983936, acc: 0.98\n",
      "94:200 - loss: 0.01898474259954368, acc: 0.98\n",
      "94:250 - loss: 0.019251248176540572, acc: 0.98\n",
      "94:300 - loss: 0.018379621262326548, acc: 0.9833333333333333\n",
      "94:350 - loss: 0.02098354224349264, acc: 0.9828571428571429\n",
      "94:400 - loss: 0.019604258488703813, acc: 0.985\n",
      "94:450 - loss: 0.017436153291453288, acc: 0.9866666666666667\n",
      "94:500 - loss: 0.015704196140150627, acc: 0.988\n",
      "94:550 - loss: 0.017816321235609462, acc: 0.9854545454545455\n",
      "epoch: 94 - train loss: 0.017317170998413095, train acc: 0.9858657243816255\n",
      "epoch: 94 - test loss: 1.1819561832696137, test acc: 0.5555555555555556\n",
      "95:50 - loss: 0.021793089313849715, acc: 0.98\n",
      "95:100 - loss: 0.01648209450403612, acc: 0.99\n",
      "95:150 - loss: 0.011032154449815585, acc: 0.9933333333333333\n",
      "95:200 - loss: 0.009978674769145919, acc: 0.995\n",
      "95:250 - loss: 0.013657164953565552, acc: 0.992\n",
      "95:300 - loss: 0.017748468379294724, acc: 0.9866666666666667\n",
      "95:350 - loss: 0.017612606002289873, acc: 0.9857142857142858\n",
      "95:400 - loss: 0.017540312646299717, acc: 0.985\n",
      "95:450 - loss: 0.016851182994846618, acc: 0.9866666666666667\n",
      "95:500 - loss: 0.01517409558080044, acc: 0.988\n",
      "95:550 - loss: 0.015990512769736447, acc: 0.9872727272727273\n",
      "epoch: 95 - train loss: 0.017059509429307635, train acc: 0.9858657243816255\n",
      "epoch: 95 - test loss: 1.122207618654162, test acc: 0.5767195767195767\n",
      "96:50 - loss: 0.022163109702782062, acc: 0.98\n",
      "96:100 - loss: 0.02625746779714999, acc: 0.98\n",
      "96:150 - loss: 0.026937145109985086, acc: 0.98\n",
      "96:200 - loss: 0.020221506711107687, acc: 0.985\n",
      "96:250 - loss: 0.018801611756577418, acc: 0.988\n",
      "96:300 - loss: 0.020772599186281007, acc: 0.9833333333333333\n",
      "96:350 - loss: 0.01781252004644441, acc: 0.9857142857142858\n",
      "96:400 - loss: 0.015592288743531538, acc: 0.9875\n",
      "96:450 - loss: 0.01688500811634957, acc: 0.9866666666666667\n",
      "96:500 - loss: 0.020509719102664928, acc: 0.982\n",
      "96:550 - loss: 0.01865389355801984, acc: 0.9836363636363636\n",
      "epoch: 96 - train loss: 0.018128581618717467, train acc: 0.9840989399293286\n",
      "epoch: 96 - test loss: 1.1433818044147044, test acc: 0.5343915343915344\n",
      "97:50 - loss: 5.735899454583914e-05, acc: 1.0\n",
      "97:100 - loss: 0.010153914499776104, acc: 0.99\n",
      "97:150 - loss: 0.010168889353844406, acc: 0.9933333333333333\n",
      "97:200 - loss: 0.01682266756015952, acc: 0.985\n",
      "97:250 - loss: 0.013466069134312135, acc: 0.988\n",
      "97:300 - loss: 0.011232753424003492, acc: 0.99\n",
      "97:350 - loss: 0.009634056694492864, acc: 0.9914285714285714\n",
      "97:400 - loss: 0.01413223498755463, acc: 0.9875\n",
      "97:450 - loss: 0.014745706729492465, acc: 0.9866666666666667\n",
      "97:500 - loss: 0.014799546876986367, acc: 0.986\n",
      "97:550 - loss: 0.018809285598990477, acc: 0.9854545454545455\n",
      "epoch: 97 - train loss: 0.02000251914491163, train acc: 0.9840989399293286\n",
      "epoch: 97 - test loss: 0.9413795058402643, test acc: 0.5767195767195767\n",
      "98:50 - loss: 0.03375079393290081, acc: 0.96\n",
      "98:100 - loss: 0.03270378886765886, acc: 0.97\n",
      "98:150 - loss: 0.021829599524442578, acc: 0.98\n",
      "98:200 - loss: 0.016386347847630035, acc: 0.985\n",
      "98:250 - loss: 0.013128969986511729, acc: 0.988\n",
      "98:300 - loss: 0.012318360907489874, acc: 0.99\n",
      "98:350 - loss: 0.011370745561688411, acc: 0.9914285714285714\n",
      "98:400 - loss: 0.019762420340701706, acc: 0.985\n",
      "98:450 - loss: 0.019417110199481946, acc: 0.9844444444444445\n",
      "98:500 - loss: 0.01748023699767774, acc: 0.986\n",
      "98:550 - loss: 0.015904113759700064, acc: 0.9872727272727273\n",
      "epoch: 98 - train loss: 0.017260758990684365, train acc: 0.9858657243816255\n",
      "epoch: 98 - test loss: 0.9058002635685949, test acc: 0.5873015873015873\n",
      "99:50 - loss: 0.011956234283500265, acc: 1.0\n",
      "99:100 - loss: 0.01903926903527539, acc: 0.99\n",
      "99:150 - loss: 0.017613011264375065, acc: 0.9866666666666667\n",
      "99:200 - loss: 0.017847081771120085, acc: 0.985\n",
      "99:250 - loss: 0.017543535332134572, acc: 0.984\n",
      "99:300 - loss: 0.017019293630154055, acc: 0.9833333333333333\n",
      "99:350 - loss: 0.018973693049725016, acc: 0.98\n",
      "99:400 - loss: 0.016609497132246916, acc: 0.9825\n",
      "99:450 - loss: 0.016055229623294642, acc: 0.9844444444444445\n",
      "99:500 - loss: 0.016420649532525776, acc: 0.984\n",
      "99:550 - loss: 0.016134558496061942, acc: 0.9854545454545455\n",
      "epoch: 99 - train loss: 0.01567923518106431, train acc: 0.9858657243816255\n",
      "epoch: 99 - test loss: 1.4596594896488355, test acc: 0.5978835978835979\n",
      "100:50 - loss: 3.728694576068629e-05, acc: 1.0\n",
      "100:100 - loss: 5.308436594659318e-05, acc: 1.0\n",
      "100:150 - loss: 0.008575101884417643, acc: 0.9933333333333333\n",
      "100:200 - loss: 0.00644179655975847, acc: 0.995\n",
      "100:250 - loss: 0.011283268728857819, acc: 0.992\n",
      "100:300 - loss: 0.015710312589033663, acc: 0.99\n",
      "100:350 - loss: 0.013470908728747111, acc: 0.9914285714285714\n",
      "100:400 - loss: 0.011791425973295577, acc: 0.9925\n",
      "100:450 - loss: 0.013926787091588998, acc: 0.9911111111111112\n",
      "100:500 - loss: 0.016292019268655308, acc: 0.988\n",
      "100:550 - loss: 0.01555528645435354, acc: 0.9890909090909091\n",
      "epoch: 100 - train loss: 0.015115983133687044, train acc: 0.9893992932862191\n",
      "epoch: 100 - test loss: 1.5218497620173965, test acc: 0.544973544973545\n"
     ]
    }
   ],
   "source": [
    "'''RESNET training code adapted from https://www.kaggle.com/gxkok21/resnet50-with-pytorch'''\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print('Using GPU.')\n",
    "    \n",
    "wandb.init()    \n",
    "wandb.watch(model)\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "EPOCHS = 50\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        samples = 0\n",
    "        loss_sum = 0\n",
    "        correct_sum = 0\n",
    "        for j, batch in enumerate(dataloaders[phase]):\n",
    "            X,labels = batch\n",
    "            #print(labels)\n",
    "            #print(len(X),len(X[0]),len(X[1]))\n",
    "            #print(labels[None,...].double())\n",
    "            #X = torch.Tensor(X)\n",
    "            #print(X.shape)\n",
    "            #labels = 1-labels\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                labels = labels.cuda()\n",
    "                #model = model.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                y = model(X[None,...].double())\n",
    "                #y = model(X.flatten().double())\n",
    "                #print(y,labels)\n",
    "                loss = criterion(\n",
    "                    y,\n",
    "                    #torch.clip(y,0,1), \n",
    "                    labels[None,...].double()\n",
    "                    #labels.double()\n",
    "                )\n",
    "                #print(loss.item())\n",
    "                #print(labels[None,...].double())\n",
    "                \n",
    "                #writer.add_scalar(\"Loss/train\", loss, i)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                loss_sum += loss.item() * X.shape[0] # We need to multiple by batch size as loss is the mean loss of the samples in the batch\n",
    "                samples += X.shape[0]\n",
    "                num_corrects = torch.sum((y >= 0.5).float() == labels[0].float())\n",
    "                correct_sum += num_corrects\n",
    "                \n",
    "                # Print batch statistics every 50 batches\n",
    "                if j % 50 == 49 and phase == \"train\":\n",
    "                    #wandb.log({'Train Loss': float(loss_sum) / float(samples), 'Train Accuracy': float(correct_sum) / float(samples)})\n",
    "\n",
    "                    print(\"{}:{} - loss: {}, acc: {}\".format(\n",
    "                        i + 1, \n",
    "                        j + 1, \n",
    "                        float(loss_sum) / float(samples), \n",
    "                        float(correct_sum) / float(samples)\n",
    "                    ))\n",
    "                    \n",
    "                \n",
    "        # Print epoch statistics\n",
    "        epoch_acc = float(correct_sum) / float(samples)\n",
    "        epoch_loss = float(loss_sum) / float(samples)\n",
    "        print(\"epoch: {} - {} loss: {}, {} acc: {}\".format(i + 1, phase, epoch_loss, phase, epoch_acc))\n",
    "        \n",
    "        # Deep copy the model\n",
    "        if phase == \"test\" and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, \"resnet18_coswara2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cd7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaf48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub # faster than librosa\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "import time\n",
    "\n",
    "from coughvid.pytorch.coughvid_dataset import CoughvidDataset\n",
    "from coughvid.pytorch.coughvid_dataset import CoughvidDataset\n",
    "from coughvid.pytorch.coswara_dataset import CoswaraDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.models import resnet50, resnet18\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd6fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask file C:/COUGHVID_public_dataset/public_dataset/ does not exist. Calculating masks on the fly.\n",
      "8997 records ready to load across 2 groups.\n",
      "699 samples in the minority class.\n",
      "Mask file C:/COUGHVID_public_dataset/public_dataset/ does not exist. Calculating masks on the fly.\n",
      "1398 records ready to load across 2 groups.\n"
     ]
    }
   ],
   "source": [
    "dir = 'C:/COUGHVID_public_dataset/public_dataset/'\n",
    "\n",
    "full_dataset = CoughvidDataset(dir, 'metadata_compiled.csv', get_features=True)\n",
    "\n",
    "dataframe = full_dataset.dataframe\n",
    "\n",
    "minority_class_count = len(dataframe[dataframe['status']==1])\n",
    "\n",
    "print(f'{minority_class_count} samples in the minority class.')\n",
    "\n",
    "sample_dataset = CoughvidDataset(dir, 'metadata_compiled.csv', get_features=True, samples_per_class=minority_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef46aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758 records ready to load across 2 groups.\n",
      "379 samples in the minority class.\n",
      "758 records ready to load across 2 groups.\n"
     ]
    }
   ],
   "source": [
    "dir = './data/coswara/'\n",
    "\n",
    "full_dataset = CoswaraDataset(dir, 'filtered_data.csv', get_features=True)\n",
    "\n",
    "dataframe = full_dataset.dataframe\n",
    "\n",
    "minority_class_count = len(dataframe[dataframe['covid_status']==1])\n",
    "\n",
    "print(f'{minority_class_count} samples in the minority class.')\n",
    "\n",
    "sample_dataset = CoswaraDataset(dir, 'filtered_data.csv', get_features=True, samples_per_class=minority_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf60937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test samples\n",
    "train_indices, test_indices = train_test_split(np.arange(0,len(sample_dataset)-1), test_size=0.25)\n",
    "\n",
    "batch_size  = 1\n",
    "num_workers = 2\n",
    "\n",
    "train_loader  = DataLoader(sample_dataset, \n",
    "                           num_workers=num_workers,\n",
    "                           sampler=SubsetRandomSampler(train_indices)\n",
    "                          )\n",
    "\n",
    "test_loader   = DataLoader(sample_dataset, \n",
    "                           num_workers=num_workers,\n",
    "                           sampler=SubsetRandomSampler(test_indices)\n",
    "                          )\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a011ecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=True)\n",
       "    (1): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (2): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model and change output shape for binary prediction\n",
    "model = resnet18()\n",
    "\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Dropout(0.10,inplace=True),\n",
    "    torch.nn.Linear(\n",
    "        in_features=512, #2048 for resnet50\n",
    "        out_features=1\n",
    "    ),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=True)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa02e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=4050, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "EPOCHS = int(n_iters / (len(sample_dataset) / batch_size))\n",
    "\n",
    "lr_rate = 0.00001    \n",
    "    \n",
    "model = LogisticRegression(4050,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "criterion = torch.nn.BCELoss()\n",
    "model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16812b10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:50 - loss: 0.8269391575306768, acc: 0.54\n",
      "1:100 - loss: 0.8007805970205196, acc: 0.54\n",
      "1:150 - loss: 0.7589026590127247, acc: 0.5466666666666666\n",
      "1:200 - loss: 0.765497807714462, acc: 0.545\n",
      "1:250 - loss: 0.7692785551749199, acc: 0.532\n",
      "1:300 - loss: 0.7680838852590592, acc: 0.5133333333333333\n",
      "1:350 - loss: 0.7801158084574296, acc: 0.5085714285714286\n",
      "1:400 - loss: 0.7741961137682348, acc: 0.515\n",
      "1:450 - loss: 0.7683172856743179, acc: 0.52\n",
      "1:500 - loss: 0.771655032604613, acc: 0.51\n",
      "1:550 - loss: 0.7721034887728451, acc: 0.5054545454545455\n",
      "1:600 - loss: 0.7725281898275577, acc: 0.5016666666666667\n",
      "1:650 - loss: 0.7702950134569199, acc: 0.5030769230769231\n",
      "1:700 - loss: 0.7716291003629021, acc: 0.5\n",
      "1:750 - loss: 0.7666966620427148, acc: 0.5093333333333333\n",
      "1:800 - loss: 0.7689655251296785, acc: 0.5025\n",
      "1:850 - loss: 0.7676761053436842, acc: 0.508235294117647\n",
      "1:900 - loss: 0.7703142958751942, acc: 0.5066666666666667\n",
      "1:950 - loss: 0.7731526118027576, acc: 0.49894736842105264\n",
      "1:1000 - loss: 0.7708044566567365, acc: 0.5\n",
      "epoch: 1 - train loss: 0.7720557455123207, train acc: 0.4976122254059217\n",
      "epoch: 1 - test loss: 0.6931920885251117, test acc: 0.5114285714285715\n",
      "2:50 - loss: 0.7344370105463086, acc: 0.44\n",
      "2:100 - loss: 0.7285980999536893, acc: 0.45\n",
      "2:150 - loss: 0.7208444909752689, acc: 0.47333333333333333\n",
      "2:200 - loss: 0.7435027583274549, acc: 0.47\n",
      "2:250 - loss: 0.7476538397371298, acc: 0.46\n",
      "2:300 - loss: 0.7488674498405797, acc: 0.45\n",
      "2:350 - loss: 0.7458508638805161, acc: 0.4542857142857143\n",
      "2:400 - loss: 0.747905804805375, acc: 0.45\n",
      "2:450 - loss: 0.7437539219036365, acc: 0.4622222222222222\n",
      "2:500 - loss: 0.7436404013077793, acc: 0.468\n",
      "2:550 - loss: 0.7428637257574437, acc: 0.4709090909090909\n",
      "2:600 - loss: 0.740226299778522, acc: 0.47\n",
      "2:650 - loss: 0.7368607731917685, acc: 0.47692307692307695\n",
      "2:700 - loss: 0.7376926854894553, acc: 0.48\n",
      "2:750 - loss: 0.7420437443059624, acc: 0.47333333333333333\n",
      "2:800 - loss: 0.7421273601276533, acc: 0.46875\n",
      "2:850 - loss: 0.7413666192123288, acc: 0.46705882352941175\n",
      "2:900 - loss: 0.7428087108833401, acc: 0.4677777777777778\n",
      "2:950 - loss: 0.7440540475311516, acc: 0.4694736842105263\n",
      "2:1000 - loss: 0.7445848258990317, acc: 0.467\n",
      "epoch: 2 - train loss: 0.7449327367809067, train acc: 0.46322827125119387\n",
      "epoch: 2 - test loss: 0.7137563004972308, test acc: 0.5114285714285715\n",
      "3:50 - loss: 0.721565899603798, acc: 0.48\n",
      "3:100 - loss: 0.7302615814104536, acc: 0.5\n",
      "3:150 - loss: 0.731818911516855, acc: 0.5\n",
      "3:200 - loss: 0.7303574312814537, acc: 0.515\n",
      "3:250 - loss: 0.7259031038440665, acc: 0.524\n",
      "3:300 - loss: 0.7253763391868966, acc: 0.5233333333333333\n",
      "3:350 - loss: 0.7264883033238163, acc: 0.5257142857142857\n",
      "3:400 - loss: 0.7153353463897579, acc: 0.5475\n",
      "3:450 - loss: 0.7136315322344807, acc: 0.5422222222222223\n",
      "3:500 - loss: 0.7155342795162262, acc: 0.528\n",
      "3:550 - loss: 0.7207543427637227, acc: 0.5218181818181818\n",
      "3:600 - loss: 0.724982314340777, acc: 0.515\n",
      "3:650 - loss: 0.7241471749830078, acc: 0.5092307692307693\n",
      "3:700 - loss: 0.7235783586727252, acc: 0.5114285714285715\n",
      "3:750 - loss: 0.7237883932715241, acc: 0.5106666666666667\n",
      "3:800 - loss: 0.7239906945528131, acc: 0.51125\n",
      "3:850 - loss: 0.7226900426404467, acc: 0.5129411764705882\n",
      "3:900 - loss: 0.7229502537021772, acc: 0.5077777777777778\n",
      "3:950 - loss: 0.7248438264176422, acc: 0.5042105263157894\n",
      "3:1000 - loss: 0.7271869811531215, acc: 0.505\n",
      "epoch: 3 - train loss: 0.7281670400282974, train acc: 0.5023877745940784\n",
      "epoch: 3 - test loss: 0.6936203417436445, test acc: 0.48857142857142855\n",
      "4:50 - loss: 0.7447217935594276, acc: 0.48\n",
      "4:100 - loss: 0.7190270263333768, acc: 0.53\n",
      "4:150 - loss: 0.7073094501624732, acc: 0.56\n",
      "4:200 - loss: 0.7237451714715571, acc: 0.545\n",
      "4:250 - loss: 0.7214867920889441, acc: 0.548\n",
      "4:300 - loss: 0.7195979428826096, acc: 0.5466666666666666\n",
      "4:350 - loss: 0.7242666711727771, acc: 0.5171428571428571\n",
      "4:400 - loss: 0.7208779898473721, acc: 0.5275\n",
      "4:450 - loss: 0.7212625145077344, acc: 0.5288888888888889\n",
      "4:500 - loss: 0.7134282178311617, acc: 0.544\n",
      "4:550 - loss: 0.7106156555506895, acc: 0.5545454545454546\n",
      "4:600 - loss: 0.7126046724186758, acc: 0.55\n",
      "4:650 - loss: 0.7128342417318575, acc: 0.5446153846153846\n",
      "4:700 - loss: 0.7135287060268544, acc: 0.54\n",
      "4:750 - loss: 0.7149567606303415, acc: 0.528\n",
      "4:800 - loss: 0.7127640803308719, acc: 0.53125\n",
      "4:850 - loss: 0.7146228926413413, acc: 0.5247058823529411\n",
      "4:900 - loss: 0.7138624470544866, acc: 0.5266666666666666\n",
      "4:950 - loss: 0.7152711155772662, acc: 0.5221052631578947\n",
      "4:1000 - loss: 0.7149147097505636, acc: 0.525\n",
      "epoch: 4 - train loss: 0.716091246941397, train acc: 0.5214899713467048\n",
      "epoch: 4 - test loss: 0.6932149567479634, test acc: 0.5114285714285715\n",
      "5:50 - loss: 0.6892776620033824, acc: 0.54\n",
      "5:100 - loss: 0.7133879260028412, acc: 0.48\n",
      "5:150 - loss: 0.7358669297870721, acc: 0.42\n",
      "5:200 - loss: 0.7270231857101082, acc: 0.41\n",
      "5:250 - loss: 0.726071711243683, acc: 0.424\n",
      "5:300 - loss: 0.725615462804525, acc: 0.43333333333333335\n",
      "5:350 - loss: 0.7268930408281108, acc: 0.44571428571428573\n",
      "5:400 - loss: 0.7264953974161804, acc: 0.455\n",
      "5:450 - loss: 0.7239325843776849, acc: 0.45555555555555555\n",
      "5:500 - loss: 0.7257099335801968, acc: 0.448\n",
      "5:550 - loss: 0.7273256994079244, acc: 0.4509090909090909\n",
      "5:600 - loss: 0.7294241749746395, acc: 0.4483333333333333\n",
      "5:650 - loss: 0.7279606835660809, acc: 0.45384615384615384\n",
      "5:700 - loss: 0.7280481751576354, acc: 0.4542857142857143\n",
      "5:750 - loss: 0.7275413173679673, acc: 0.45466666666666666\n",
      "5:800 - loss: 0.7267041671085072, acc: 0.45875\n",
      "5:850 - loss: 0.7247303245563028, acc: 0.4647058823529412\n",
      "5:900 - loss: 0.7275552638835258, acc: 0.4666666666666667\n",
      "5:950 - loss: 0.7267405322273489, acc: 0.46\n",
      "5:1000 - loss: 0.7261650932796759, acc: 0.457\n",
      "epoch: 5 - train loss: 0.7254415547734357, train acc: 0.4584527220630373\n",
      "epoch: 5 - test loss: 0.6932370164679548, test acc: 0.5114285714285715\n",
      "6:50 - loss: 0.7149546459382965, acc: 0.44\n",
      "6:100 - loss: 0.7211947296687216, acc: 0.45\n",
      "6:150 - loss: 0.7310074020493036, acc: 0.44666666666666666\n",
      "6:200 - loss: 0.7168871272651001, acc: 0.475\n",
      "6:250 - loss: 0.7173314504890561, acc: 0.48\n",
      "6:300 - loss: 0.725901518945363, acc: 0.4666666666666667\n",
      "6:350 - loss: 0.7230799111566775, acc: 0.48\n",
      "6:400 - loss: 0.7195922585543909, acc: 0.4925\n",
      "6:450 - loss: 0.7251643677167466, acc: 0.4777777777777778\n",
      "6:500 - loss: 0.7250458650438237, acc: 0.466\n",
      "6:550 - loss: 0.7211249090990424, acc: 0.4709090909090909\n",
      "6:600 - loss: 0.721216516381443, acc: 0.47333333333333333\n",
      "6:650 - loss: 0.7182800968238776, acc: 0.48\n",
      "6:700 - loss: 0.7153085589124016, acc: 0.49142857142857144\n",
      "6:750 - loss: 0.7131494477045767, acc: 0.49866666666666665\n",
      "6:800 - loss: 0.7112434963337584, acc: 0.505\n",
      "6:850 - loss: 0.7119538085342062, acc: 0.5047058823529412\n",
      "6:900 - loss: 0.7104982055301295, acc: 0.5055555555555555\n",
      "6:950 - loss: 0.7104931144655268, acc: 0.5042105263157894\n",
      "6:1000 - loss: 0.7113083715713212, acc: 0.493\n",
      "epoch: 6 - train loss: 0.7109098084732274, train acc: 0.49474689589302767\n",
      "epoch: 6 - test loss: 0.6931271091328898, test acc: 0.5114285714285715\n",
      "7:50 - loss: 0.6834065234194305, acc: 0.56\n",
      "7:100 - loss: 0.673422859638665, acc: 0.6\n",
      "7:150 - loss: 0.6911592317955532, acc: 0.5666666666666667\n",
      "7:200 - loss: 0.6941607457879954, acc: 0.56\n",
      "7:250 - loss: 0.7016371586522081, acc: 0.548\n",
      "7:300 - loss: 0.7015218397367885, acc: 0.5366666666666666\n",
      "7:350 - loss: 0.7016748293719213, acc: 0.5314285714285715\n",
      "7:400 - loss: 0.7005550123718965, acc: 0.5225\n",
      "7:450 - loss: 0.6990850722421701, acc: 0.5266666666666666\n",
      "7:500 - loss: 0.6995560686686688, acc: 0.534\n",
      "7:550 - loss: 0.6989562808354253, acc: 0.5345454545454545\n",
      "7:600 - loss: 0.6979741778995895, acc: 0.5333333333333333\n",
      "7:650 - loss: 0.7010695666722877, acc: 0.5323076923076923\n",
      "7:700 - loss: 0.7072805076642439, acc: 0.52\n",
      "7:750 - loss: 0.7083940305026247, acc: 0.5106666666666667\n",
      "7:800 - loss: 0.70857100341922, acc: 0.51375\n",
      "7:850 - loss: 0.7073971847542221, acc: 0.52\n",
      "7:900 - loss: 0.7050964330049374, acc: 0.5255555555555556\n",
      "7:950 - loss: 0.7064247082723787, acc: 0.5263157894736842\n",
      "7:1000 - loss: 0.706007216078612, acc: 0.527\n",
      "epoch: 7 - train loss: 0.7057278982723835, train acc: 0.5253104106972302\n",
      "epoch: 7 - test loss: 0.6962516028963244, test acc: 0.5114285714285715\n",
      "8:50 - loss: 0.6998168948054904, acc: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8:100 - loss: 0.7047864834085682, acc: 0.52\n",
      "8:150 - loss: 0.6954771950890262, acc: 0.54\n",
      "8:200 - loss: 0.6963243593377613, acc: 0.54\n",
      "8:250 - loss: 0.7035103052434206, acc: 0.524\n",
      "8:300 - loss: 0.7038057289919544, acc: 0.52\n",
      "8:350 - loss: 0.7034293392769965, acc: 0.5171428571428571\n",
      "8:400 - loss: 0.703688295222219, acc: 0.52\n",
      "8:450 - loss: 0.7037035359533019, acc: 0.5133333333333333\n",
      "8:500 - loss: 0.701281804378787, acc: 0.528\n",
      "8:550 - loss: 0.7025403846480507, acc: 0.5272727272727272\n",
      "8:600 - loss: 0.7048186170310602, acc: 0.5133333333333333\n",
      "8:650 - loss: 0.7034872867974259, acc: 0.5215384615384615\n",
      "8:700 - loss: 0.7020020574590516, acc: 0.5271428571428571\n",
      "8:750 - loss: 0.702836937060866, acc: 0.524\n",
      "8:800 - loss: 0.7033285439115268, acc: 0.525\n",
      "8:850 - loss: 0.7043462355351569, acc: 0.5211764705882352\n",
      "8:900 - loss: 0.7047383639401397, acc: 0.5177777777777778\n",
      "8:950 - loss: 0.7039643744927112, acc: 0.5221052631578947\n",
      "8:1000 - loss: 0.7042889678925687, acc: 0.522\n",
      "epoch: 8 - train loss: 0.7042871398710796, train acc: 0.5186246418338109\n",
      "epoch: 8 - test loss: 0.6935884106131993, test acc: 0.48857142857142855\n",
      "9:50 - loss: 0.7009993845593273, acc: 0.5\n",
      "9:100 - loss: 0.7042697953116851, acc: 0.47\n",
      "9:150 - loss: 0.7033339957578378, acc: 0.46\n",
      "9:200 - loss: 0.7016786841732303, acc: 0.47\n",
      "9:250 - loss: 0.7022862553342456, acc: 0.468\n",
      "9:300 - loss: 0.7075746210222409, acc: 0.45666666666666667\n",
      "9:350 - loss: 0.7082629687589042, acc: 0.44857142857142857\n",
      "9:400 - loss: 0.7078809359035012, acc: 0.4575\n",
      "9:450 - loss: 0.7057772426587526, acc: 0.4622222222222222\n",
      "9:500 - loss: 0.7072980072691485, acc: 0.458\n",
      "9:550 - loss: 0.7066330965944069, acc: 0.46\n",
      "9:600 - loss: 0.706912290175547, acc: 0.455\n",
      "9:650 - loss: 0.7069273403064764, acc: 0.45384615384615384\n",
      "9:700 - loss: 0.7065445912759044, acc: 0.45714285714285713\n",
      "9:750 - loss: 0.7050775926122793, acc: 0.468\n",
      "9:800 - loss: 0.7037154218467583, acc: 0.47375\n",
      "9:850 - loss: 0.7039511015767891, acc: 0.4717647058823529\n",
      "9:900 - loss: 0.7039899798341432, acc: 0.47\n",
      "9:950 - loss: 0.7024706920503935, acc: 0.4747368421052632\n",
      "9:1000 - loss: 0.7027247333187939, acc: 0.476\n",
      "epoch: 9 - train loss: 0.7032568435144192, train acc: 0.47277936962750716\n",
      "epoch: 9 - test loss: 0.6942665529130199, test acc: 0.48857142857142855\n",
      "10:50 - loss: 0.6978342939929416, acc: 0.56\n",
      "10:100 - loss: 0.7033915314641294, acc: 0.47\n",
      "10:150 - loss: 0.7052439131467372, acc: 0.44666666666666666\n",
      "10:200 - loss: 0.7075121653929837, acc: 0.45\n",
      "10:250 - loss: 0.6994478716972689, acc: 0.488\n",
      "10:300 - loss: 0.7041009543791735, acc: 0.48333333333333334\n",
      "10:350 - loss: 0.7039083810110136, acc: 0.4857142857142857\n",
      "10:400 - loss: 0.7024557543281152, acc: 0.4925\n",
      "10:450 - loss: 0.7017785265549558, acc: 0.5022222222222222\n",
      "10:500 - loss: 0.7020663933313132, acc: 0.502\n",
      "10:550 - loss: 0.6988648508223353, acc: 0.5127272727272727\n",
      "10:600 - loss: 0.6998612808875772, acc: 0.5133333333333333\n",
      "10:650 - loss: 0.7000556973355623, acc: 0.5138461538461538\n",
      "10:700 - loss: 0.6998347902987665, acc: 0.5128571428571429\n",
      "10:750 - loss: 0.6997248269368057, acc: 0.5133333333333333\n",
      "10:800 - loss: 0.7000042447184306, acc: 0.5175\n",
      "10:850 - loss: 0.7009215165493673, acc: 0.5129411764705882\n",
      "10:900 - loss: 0.7016198589693352, acc: 0.5111111111111111\n",
      "10:950 - loss: 0.7019567312320428, acc: 0.5073684210526316\n",
      "10:1000 - loss: 0.7019901452372261, acc: 0.502\n",
      "epoch: 10 - train loss: 0.7020867874542477, train acc: 0.4976122254059217\n",
      "epoch: 10 - test loss: 0.695877481218413, test acc: 0.48857142857142855\n",
      "11:50 - loss: 0.6965429192079907, acc: 0.54\n",
      "11:100 - loss: 0.6907122876207508, acc: 0.56\n",
      "11:150 - loss: 0.6960398351290098, acc: 0.5333333333333333\n",
      "11:200 - loss: 0.6918718777594661, acc: 0.545\n",
      "11:250 - loss: 0.6950148199964371, acc: 0.532\n",
      "11:300 - loss: 0.6963476850761594, acc: 0.53\n",
      "11:350 - loss: 0.6945612258181548, acc: 0.5342857142857143\n",
      "11:400 - loss: 0.6981511842245266, acc: 0.5225\n",
      "11:450 - loss: 0.6978924828573656, acc: 0.5177777777777778\n",
      "11:500 - loss: 0.698553939394074, acc: 0.518\n",
      "11:550 - loss: 0.6993196025639097, acc: 0.5072727272727273\n",
      "11:600 - loss: 0.6990594551812751, acc: 0.5116666666666667\n",
      "11:650 - loss: 0.7001519353942537, acc: 0.5061538461538462\n",
      "11:700 - loss: 0.699385394377261, acc: 0.5114285714285715\n",
      "11:750 - loss: 0.7000974641137846, acc: 0.508\n",
      "11:800 - loss: 0.6995914123460791, acc: 0.51\n",
      "11:850 - loss: 0.6996590143445645, acc: 0.5117647058823529\n",
      "11:900 - loss: 0.700041183786399, acc: 0.5055555555555555\n",
      "11:950 - loss: 0.7001732248734583, acc: 0.5010526315789474\n",
      "11:1000 - loss: 0.6999098231280982, acc: 0.499\n",
      "epoch: 11 - train loss: 0.6990205973619621, train acc: 0.504297994269341\n",
      "epoch: 11 - test loss: 0.7417376813799946, test acc: 0.48857142857142855\n",
      "12:50 - loss: 0.7479247000976362, acc: 0.48\n",
      "12:100 - loss: 0.7292941637191409, acc: 0.51\n",
      "12:150 - loss: 0.7226033626724175, acc: 0.5266666666666666\n",
      "12:200 - loss: 0.7139491061210357, acc: 0.545\n",
      "12:250 - loss: 0.713698415161177, acc: 0.54\n",
      "12:300 - loss: 0.7144538687626546, acc: 0.5133333333333333\n",
      "12:350 - loss: 0.7138190719334113, acc: 0.49714285714285716\n",
      "12:400 - loss: 0.7123599229212074, acc: 0.49\n",
      "12:450 - loss: 0.7100211947049472, acc: 0.4911111111111111\n",
      "12:500 - loss: 0.7112287489560707, acc: 0.49\n",
      "12:550 - loss: 0.7108097221546079, acc: 0.4909090909090909\n",
      "12:600 - loss: 0.7095975023202626, acc: 0.49666666666666665\n",
      "12:650 - loss: 0.7069426895321407, acc: 0.5061538461538462\n",
      "12:700 - loss: 0.7075307466606322, acc: 0.5028571428571429\n",
      "12:750 - loss: 0.7071248344201896, acc: 0.5\n",
      "12:800 - loss: 0.7063563389107352, acc: 0.50375\n",
      "12:850 - loss: 0.7051269700036836, acc: 0.5058823529411764\n",
      "12:900 - loss: 0.7056537915327418, acc: 0.5066666666666667\n",
      "12:950 - loss: 0.705041739937259, acc: 0.5094736842105263\n",
      "12:1000 - loss: 0.705164404903658, acc: 0.51\n",
      "epoch: 12 - train loss: 0.7040070203712572, train acc: 0.5138490926456543\n",
      "epoch: 12 - test loss: 0.6970979752776321, test acc: 0.5114285714285715\n",
      "13:50 - loss: 0.6950894226543423, acc: 0.54\n",
      "13:100 - loss: 0.7006774569770141, acc: 0.45\n",
      "13:150 - loss: 0.6956738610978307, acc: 0.5066666666666667\n",
      "13:200 - loss: 0.6936880032032338, acc: 0.52\n",
      "13:250 - loss: 0.6957412725882965, acc: 0.516\n",
      "13:300 - loss: 0.6947904622202663, acc: 0.5133333333333333\n",
      "13:350 - loss: 0.6971669568453769, acc: 0.5028571428571429\n",
      "13:400 - loss: 0.6978255365472475, acc: 0.5025\n",
      "13:450 - loss: 0.6992626050751131, acc: 0.4888888888888889\n",
      "13:500 - loss: 0.698690491484216, acc: 0.482\n",
      "13:550 - loss: 0.6983495571300298, acc: 0.48545454545454547\n",
      "13:600 - loss: 0.6970390760154743, acc: 0.49333333333333335\n",
      "13:650 - loss: 0.6987994900985683, acc: 0.48615384615384616\n",
      "13:700 - loss: 0.6992112411563064, acc: 0.48714285714285716\n",
      "13:750 - loss: 0.7000881550290546, acc: 0.47733333333333333\n",
      "13:800 - loss: 0.7001230118769496, acc: 0.475\n",
      "13:850 - loss: 0.6999267556418257, acc: 0.48\n",
      "13:900 - loss: 0.6986934282265516, acc: 0.48777777777777775\n",
      "13:950 - loss: 0.6990786097628119, acc: 0.4905263157894737\n",
      "13:1000 - loss: 0.6990716744423013, acc: 0.489\n",
      "epoch: 13 - train loss: 0.6989157448181427, train acc: 0.48615090735434574\n",
      "epoch: 13 - test loss: 0.6908906574627124, test acc: 0.5114285714285715\n",
      "14:50 - loss: 0.6785541897384142, acc: 0.62\n",
      "14:100 - loss: 0.6919338946751502, acc: 0.56\n",
      "14:150 - loss: 0.6937749041920076, acc: 0.5466666666666666\n",
      "14:200 - loss: 0.6932852137885754, acc: 0.545\n",
      "14:250 - loss: 0.699374236077147, acc: 0.528\n",
      "14:300 - loss: 0.6997614380061965, acc: 0.5366666666666666\n",
      "14:350 - loss: 0.6991901199189198, acc: 0.5342857142857143\n",
      "14:400 - loss: 0.7021591822281897, acc: 0.525\n",
      "14:450 - loss: 0.7007817068826445, acc: 0.5244444444444445\n",
      "14:500 - loss: 0.7022427353284018, acc: 0.522\n",
      "14:550 - loss: 0.7019583268895301, acc: 0.5109090909090909\n",
      "14:600 - loss: 0.7013167099216572, acc: 0.5133333333333333\n",
      "14:650 - loss: 0.7010494922613082, acc: 0.5092307692307693\n",
      "14:700 - loss: 0.6994737858748769, acc: 0.5171428571428571\n",
      "14:750 - loss: 0.6996776553992592, acc: 0.5133333333333333\n",
      "14:800 - loss: 0.6984665355669989, acc: 0.51875\n",
      "14:850 - loss: 0.6980429549552177, acc: 0.5188235294117647\n",
      "14:900 - loss: 0.6976393088147105, acc: 0.5211111111111111\n",
      "14:950 - loss: 0.6977106610496439, acc: 0.52\n",
      "14:1000 - loss: 0.6973725236676847, acc: 0.52\n",
      "epoch: 14 - train loss: 0.6977335964315182, train acc: 0.5186246418338109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 - test loss: 0.6897806521008446, test acc: 0.52\n",
      "15:50 - loss: 0.7020997133142372, acc: 0.46\n",
      "15:100 - loss: 0.7066923864325787, acc: 0.49\n",
      "15:150 - loss: 0.7307390412879211, acc: 0.49333333333333335\n",
      "15:200 - loss: 0.7226274207275816, acc: 0.51\n",
      "15:250 - loss: 0.7244939763502465, acc: 0.512\n",
      "15:300 - loss: 0.7207207547511274, acc: 0.5066666666666667\n",
      "15:350 - loss: 0.7201535386994248, acc: 0.49714285714285716\n",
      "15:400 - loss: 0.7183589399974576, acc: 0.4925\n",
      "15:450 - loss: 0.7140894242066238, acc: 0.5066666666666667\n",
      "15:500 - loss: 0.7107478277907272, acc: 0.514\n",
      "15:550 - loss: 0.7098367478868374, acc: 0.5163636363636364\n",
      "15:600 - loss: 0.7077328036967196, acc: 0.52\n",
      "15:650 - loss: 0.707721327412376, acc: 0.5184615384615384\n",
      "15:700 - loss: 0.706536168826129, acc: 0.52\n",
      "15:750 - loss: 0.7054127828535458, acc: 0.5226666666666666\n",
      "15:800 - loss: 0.7052663332899651, acc: 0.52\n",
      "15:850 - loss: 0.7048378465344046, acc: 0.5164705882352941\n",
      "15:900 - loss: 0.7046270867241079, acc: 0.5166666666666667\n",
      "15:950 - loss: 0.7044473292387964, acc: 0.5136842105263157\n",
      "15:1000 - loss: 0.7045515396341028, acc: 0.514\n",
      "epoch: 15 - train loss: 0.7040663361934064, train acc: 0.5119388729703916\n",
      "epoch: 15 - test loss: 0.693646204413495, test acc: 0.5057142857142857\n",
      "16:50 - loss: 0.6978079055632185, acc: 0.58\n",
      "16:100 - loss: 0.6924129310670019, acc: 0.59\n",
      "16:150 - loss: 0.6899100945551997, acc: 0.5933333333333334\n",
      "16:200 - loss: 0.6948251505511504, acc: 0.56\n",
      "16:250 - loss: 0.6957807634114228, acc: 0.56\n",
      "16:300 - loss: 0.6958747978301105, acc: 0.55\n",
      "16:350 - loss: 0.6945147880820518, acc: 0.5514285714285714\n",
      "16:400 - loss: 0.6951558578615398, acc: 0.545\n",
      "16:450 - loss: 0.6956102938753717, acc: 0.5266666666666666\n",
      "16:500 - loss: 0.6950581462041471, acc: 0.53\n",
      "16:550 - loss: 0.6948738067644179, acc: 0.5236363636363637\n",
      "16:600 - loss: 0.6936611288274963, acc: 0.5316666666666666\n",
      "16:650 - loss: 0.6937552811622466, acc: 0.5323076923076923\n",
      "16:700 - loss: 0.6932968730466568, acc: 0.53\n",
      "16:750 - loss: 0.6934711418694086, acc: 0.532\n",
      "16:800 - loss: 0.6936011065449379, acc: 0.53\n",
      "16:850 - loss: 0.6931853454174771, acc: 0.5294117647058824\n",
      "16:900 - loss: 0.6941833440709045, acc: 0.5322222222222223\n",
      "16:950 - loss: 0.6947560007375244, acc: 0.5284210526315789\n",
      "16:1000 - loss: 0.6946370692516962, acc: 0.527\n",
      "epoch: 16 - train loss: 0.6947085813900631, train acc: 0.5262655205348615\n",
      "epoch: 16 - test loss: 0.688615109776342, test acc: 0.5228571428571429\n",
      "17:50 - loss: 0.6952589142183254, acc: 0.54\n",
      "17:100 - loss: 0.7023954253318881, acc: 0.49\n",
      "17:150 - loss: 0.7005320228838997, acc: 0.5\n",
      "17:200 - loss: 0.6994690314451043, acc: 0.505\n",
      "17:250 - loss: 0.6945173166362253, acc: 0.528\n",
      "17:300 - loss: 0.6984928249485401, acc: 0.5066666666666667\n",
      "17:350 - loss: 0.6968736896470727, acc: 0.52\n",
      "17:400 - loss: 0.6970075201137942, acc: 0.515\n",
      "17:450 - loss: 0.6946762953740814, acc: 0.5288888888888889\n",
      "17:500 - loss: 0.6939555164295221, acc: 0.524\n",
      "17:550 - loss: 0.6953019696504068, acc: 0.5181818181818182\n",
      "17:600 - loss: 0.6955198679627072, acc: 0.5183333333333333\n",
      "17:650 - loss: 0.6968555225334034, acc: 0.5092307692307693\n",
      "17:700 - loss: 0.697578870877119, acc: 0.51\n",
      "17:750 - loss: 0.6971582086054204, acc: 0.512\n",
      "17:800 - loss: 0.6970936938943979, acc: 0.51375\n",
      "17:850 - loss: 0.6967082592606376, acc: 0.5117647058823529\n",
      "17:900 - loss: 0.6962598311919788, acc: 0.5166666666666667\n",
      "17:950 - loss: 0.6962160030417133, acc: 0.5157894736842106\n",
      "17:1000 - loss: 0.6963420272283094, acc: 0.512\n",
      "epoch: 17 - train loss: 0.6948089533684874, train acc: 0.5195797516714422\n",
      "epoch: 17 - test loss: 0.6950299266199617, test acc: 0.5257142857142857\n",
      "18:50 - loss: 0.6629828569429849, acc: 0.66\n",
      "18:100 - loss: 0.6761080143929779, acc: 0.6\n",
      "18:150 - loss: 0.6666952519455321, acc: 0.6066666666666667\n",
      "18:200 - loss: 0.6724373485451474, acc: 0.63\n",
      "18:250 - loss: 0.6744247661821655, acc: 0.612\n",
      "18:300 - loss: 0.6866024977345511, acc: 0.5666666666666667\n",
      "18:350 - loss: 0.6891270716135429, acc: 0.5485714285714286\n",
      "18:400 - loss: 0.6922338556868168, acc: 0.5275\n",
      "18:450 - loss: 0.6939131804858039, acc: 0.5133333333333333\n",
      "18:500 - loss: 0.6962355715545505, acc: 0.504\n",
      "18:550 - loss: 0.6964358683426303, acc: 0.5036363636363637\n",
      "18:600 - loss: 0.695224064757161, acc: 0.5116666666666667\n",
      "18:650 - loss: 0.6945718880202675, acc: 0.5153846153846153\n",
      "18:700 - loss: 0.6947108904741801, acc: 0.5114285714285715\n",
      "18:750 - loss: 0.6928538084305281, acc: 0.516\n",
      "18:800 - loss: 0.6950502046757289, acc: 0.51\n",
      "18:850 - loss: 0.6957592341658686, acc: 0.508235294117647\n",
      "18:900 - loss: 0.695283210697948, acc: 0.5088888888888888\n",
      "18:950 - loss: 0.6941895946400772, acc: 0.5147368421052632\n",
      "18:1000 - loss: 0.6947944012810611, acc: 0.516\n",
      "epoch: 18 - train loss: 0.6950072425085281, train acc: 0.5167144221585482\n",
      "epoch: 18 - test loss: 0.6859134594867614, test acc: 0.5571428571428572\n",
      "19:50 - loss: 0.679997762093054, acc: 0.62\n",
      "19:100 - loss: 0.6955687381121828, acc: 0.56\n",
      "19:150 - loss: 0.6923804013433509, acc: 0.5866666666666667\n",
      "19:200 - loss: 0.6646428122411954, acc: 0.625\n",
      "19:250 - loss: 0.6778551562098007, acc: 0.612\n",
      "19:300 - loss: 0.6798017771596326, acc: 0.6\n",
      "19:350 - loss: 0.6840158371724134, acc: 0.58\n",
      "19:400 - loss: 0.686420989791718, acc: 0.5625\n",
      "19:450 - loss: 0.6851795845645854, acc: 0.5644444444444444\n",
      "19:500 - loss: 0.6834859310532926, acc: 0.566\n",
      "19:550 - loss: 0.6833615018172968, acc: 0.5727272727272728\n",
      "19:600 - loss: 0.6817422690412723, acc: 0.5733333333333334\n",
      "19:650 - loss: 0.6824364506794771, acc: 0.5707692307692308\n",
      "19:700 - loss: 0.6849053966589911, acc: 0.5671428571428572\n",
      "19:750 - loss: 0.6863552064244006, acc: 0.5613333333333334\n",
      "19:800 - loss: 0.6867043005121232, acc: 0.5575\n",
      "19:850 - loss: 0.6862270801412699, acc: 0.5588235294117647\n",
      "19:900 - loss: 0.6854430005365747, acc: 0.5588888888888889\n",
      "19:950 - loss: 0.6858134194054574, acc: 0.5589473684210526\n",
      "19:1000 - loss: 0.6869762495332599, acc: 0.562\n",
      "epoch: 19 - train loss: 0.6849554399963506, train acc: 0.5702005730659025\n",
      "epoch: 19 - test loss: 0.6817482760166915, test acc: 0.5342857142857143\n",
      "20:50 - loss: 0.6729112045704475, acc: 0.62\n",
      "20:100 - loss: 0.6974803587476728, acc: 0.56\n",
      "20:150 - loss: 0.7000932454412921, acc: 0.5333333333333333\n",
      "20:200 - loss: 0.6867936383135551, acc: 0.555\n",
      "20:250 - loss: 0.6867520618520284, acc: 0.56\n",
      "20:300 - loss: 0.6852610985468418, acc: 0.5733333333333334\n",
      "20:350 - loss: 0.6789429940140219, acc: 0.5771428571428572\n",
      "20:400 - loss: 0.6822803972082667, acc: 0.5725\n",
      "20:450 - loss: 0.6830704671114133, acc: 0.5711111111111111\n",
      "20:500 - loss: 0.681010874717699, acc: 0.576\n",
      "20:550 - loss: 0.6861158769987492, acc: 0.5672727272727273\n",
      "20:600 - loss: 0.6867737822842622, acc: 0.5633333333333334\n",
      "20:650 - loss: 0.6857047283036473, acc: 0.5646153846153846\n",
      "20:700 - loss: 0.6841421287337621, acc: 0.57\n",
      "20:750 - loss: 0.6821159080846688, acc: 0.568\n",
      "20:800 - loss: 0.6815805643064455, acc: 0.57125\n",
      "20:850 - loss: 0.6789949071396331, acc: 0.5764705882352941\n",
      "20:900 - loss: 0.6808516344166952, acc: 0.5744444444444444\n",
      "20:950 - loss: 0.6778578905807316, acc: 0.58\n",
      "20:1000 - loss: 0.6765133687712439, acc: 0.582\n",
      "epoch: 20 - train loss: 0.6756852129632712, train acc: 0.5854823304680038\n",
      "epoch: 20 - test loss: 0.6901910316639163, test acc: 0.54\n",
      "21:50 - loss: 0.6128226853469737, acc: 0.66\n",
      "21:100 - loss: 0.6550756331008527, acc: 0.6\n",
      "21:150 - loss: 0.6490067231636132, acc: 0.6066666666666667\n",
      "21:200 - loss: 0.6434513334279132, acc: 0.615\n",
      "21:250 - loss: 0.6519435217469932, acc: 0.6\n",
      "21:300 - loss: 0.6622289963901908, acc: 0.5933333333333334\n",
      "21:350 - loss: 0.6632990251210246, acc: 0.5857142857142857\n",
      "21:400 - loss: 0.6635963278733308, acc: 0.59\n",
      "21:450 - loss: 0.6598619897357638, acc: 0.6088888888888889\n",
      "21:500 - loss: 0.6604311422564868, acc: 0.61\n",
      "21:550 - loss: 0.6651963107741333, acc: 0.5981818181818181\n",
      "21:600 - loss: 0.6658256304646719, acc: 0.6016666666666667\n",
      "21:650 - loss: 0.6638925490663427, acc: 0.6061538461538462\n",
      "21:700 - loss: 0.664706007987919, acc: 0.6028571428571429\n",
      "21:750 - loss: 0.6644616353756412, acc: 0.6\n",
      "21:800 - loss: 0.6633698393132964, acc: 0.60375\n",
      "21:850 - loss: 0.661824702358649, acc: 0.6047058823529412\n",
      "21:900 - loss: 0.6600636668247462, acc: 0.6055555555555555\n",
      "21:950 - loss: 0.6592680687168306, acc: 0.6063157894736843\n",
      "21:1000 - loss: 0.6574975357943489, acc: 0.611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21 - train loss: 0.6601983161412964, train acc: 0.6045845272206304\n",
      "epoch: 21 - test loss: 0.6873000051020562, test acc: 0.5257142857142857\n",
      "22:50 - loss: 0.6141985283614253, acc: 0.62\n",
      "22:100 - loss: 0.6555332538471872, acc: 0.58\n",
      "22:150 - loss: 0.6740660484697962, acc: 0.5933333333333334\n",
      "22:200 - loss: 0.6478381080033941, acc: 0.635\n",
      "22:250 - loss: 0.643901933157816, acc: 0.636\n",
      "22:300 - loss: 0.6448946758965356, acc: 0.64\n",
      "22:350 - loss: 0.6455425450202955, acc: 0.6428571428571429\n",
      "22:400 - loss: 0.647034483599192, acc: 0.6425\n",
      "22:450 - loss: 0.6390241957362777, acc: 0.6555555555555556\n",
      "22:500 - loss: 0.6430751700473487, acc: 0.648\n",
      "22:550 - loss: 0.6404810121684715, acc: 0.6527272727272727\n",
      "22:600 - loss: 0.6433461274366284, acc: 0.6483333333333333\n",
      "22:650 - loss: 0.6408120435394521, acc: 0.6523076923076923\n",
      "22:700 - loss: 0.6426466985797389, acc: 0.6471428571428571\n",
      "22:750 - loss: 0.6428420127965123, acc: 0.652\n",
      "22:800 - loss: 0.6459138925963644, acc: 0.6425\n",
      "22:850 - loss: 0.6437902700536382, acc: 0.6458823529411765\n",
      "22:900 - loss: 0.6445282215614222, acc: 0.6455555555555555\n",
      "22:950 - loss: 0.6457855552738524, acc: 0.6442105263157895\n",
      "22:1000 - loss: 0.6442865826912547, acc: 0.647\n",
      "epoch: 22 - train loss: 0.6429912713220888, train acc: 0.6466093600764088\n",
      "epoch: 22 - test loss: 0.6987383641773319, test acc: 0.5742857142857143\n",
      "23:50 - loss: 0.5843593100385995, acc: 0.72\n",
      "23:100 - loss: 0.5960885372305487, acc: 0.7\n",
      "23:150 - loss: 0.5965626115625472, acc: 0.7066666666666667\n",
      "23:200 - loss: 0.5889425138501003, acc: 0.71\n",
      "23:250 - loss: 0.590042425980536, acc: 0.696\n",
      "23:300 - loss: 0.6019099787507436, acc: 0.68\n",
      "23:350 - loss: 0.5985750092173133, acc: 0.6885714285714286\n",
      "23:400 - loss: 0.5956591568753608, acc: 0.6875\n",
      "23:450 - loss: 0.5980706431112086, acc: 0.68\n",
      "23:500 - loss: 0.5992282458186622, acc: 0.682\n",
      "23:550 - loss: 0.6054844787136529, acc: 0.6727272727272727\n",
      "23:600 - loss: 0.6083770002473626, acc: 0.6633333333333333\n",
      "23:650 - loss: 0.6115794748040088, acc: 0.6553846153846153\n",
      "23:700 - loss: 0.6143157032161807, acc: 0.6557142857142857\n",
      "23:750 - loss: 0.6177701319403691, acc: 0.6533333333333333\n",
      "23:800 - loss: 0.6199264948564878, acc: 0.65\n",
      "23:850 - loss: 0.6170437557641753, acc: 0.6529411764705882\n",
      "23:900 - loss: 0.6157588568210398, acc: 0.6555555555555556\n",
      "23:950 - loss: 0.6168953385562842, acc: 0.6578947368421053\n",
      "23:1000 - loss: 0.6192733322332663, acc: 0.658\n",
      "epoch: 23 - train loss: 0.6199085881617508, train acc: 0.6590257879656161\n",
      "epoch: 23 - test loss: 0.7113368951440614, test acc: 0.5085714285714286\n",
      "24:50 - loss: 0.5565763818451278, acc: 0.7\n",
      "24:100 - loss: 0.5825948484130461, acc: 0.66\n",
      "24:150 - loss: 0.5901171971755694, acc: 0.6666666666666666\n",
      "24:200 - loss: 0.6148223185904298, acc: 0.645\n",
      "24:250 - loss: 0.5965151459701457, acc: 0.668\n",
      "24:300 - loss: 0.588066123652149, acc: 0.6766666666666666\n",
      "24:350 - loss: 0.6042653546910193, acc: 0.6571428571428571\n",
      "24:400 - loss: 0.6031448970648852, acc: 0.6625\n",
      "24:450 - loss: 0.5994625551267014, acc: 0.6711111111111111\n",
      "24:500 - loss: 0.6010121782025283, acc: 0.674\n",
      "24:550 - loss: 0.6002046444708862, acc: 0.6763636363636364\n",
      "24:600 - loss: 0.5992643632229917, acc: 0.68\n",
      "24:650 - loss: 0.5955382170098039, acc: 0.6815384615384615\n",
      "24:700 - loss: 0.5925186574983884, acc: 0.6842857142857143\n",
      "24:750 - loss: 0.5921143245688504, acc: 0.6853333333333333\n",
      "24:800 - loss: 0.5936619935249007, acc: 0.68125\n",
      "24:850 - loss: 0.5982895722884045, acc: 0.6764705882352942\n",
      "24:900 - loss: 0.5956522113761321, acc: 0.6766666666666666\n",
      "24:950 - loss: 0.5997060549247464, acc: 0.6705263157894736\n",
      "24:1000 - loss: 0.6021994283773513, acc: 0.671\n",
      "epoch: 24 - train loss: 0.6036635446329879, train acc: 0.6695319961795606\n",
      "epoch: 24 - test loss: 0.703672160063445, test acc: 0.5085714285714286\n",
      "25:50 - loss: 0.5036301815079587, acc: 0.74\n",
      "25:100 - loss: 0.5026666197602849, acc: 0.75\n",
      "25:150 - loss: 0.5299096509338606, acc: 0.76\n",
      "25:200 - loss: 0.5450342290630688, acc: 0.755\n",
      "25:250 - loss: 0.5586941660659848, acc: 0.732\n",
      "25:300 - loss: 0.5697176685234129, acc: 0.7233333333333334\n",
      "25:350 - loss: 0.5712901845940485, acc: 0.7285714285714285\n",
      "25:400 - loss: 0.5535845982849453, acc: 0.7425\n",
      "25:450 - loss: 0.5589726318471404, acc: 0.74\n",
      "25:500 - loss: 0.5510604988035364, acc: 0.746\n",
      "25:550 - loss: 0.5509963952390862, acc: 0.7472727272727273\n",
      "25:600 - loss: 0.5632321707765294, acc: 0.74\n",
      "25:650 - loss: 0.5615089749488308, acc: 0.7415384615384616\n",
      "25:700 - loss: 0.567851385106593, acc: 0.7342857142857143\n",
      "25:750 - loss: 0.5719907040009934, acc: 0.7293333333333333\n",
      "25:800 - loss: 0.5718123742212121, acc: 0.7275\n",
      "25:850 - loss: 0.5774400187252965, acc: 0.72\n",
      "25:900 - loss: 0.5836549459240189, acc: 0.7155555555555555\n",
      "25:950 - loss: 0.5809202555067856, acc: 0.7189473684210527\n",
      "25:1000 - loss: 0.5796619525023732, acc: 0.72\n",
      "epoch: 25 - train loss: 0.579518451477469, train acc: 0.7182425978987583\n",
      "epoch: 25 - test loss: 0.7297684809061792, test acc: 0.5085714285714286\n",
      "26:50 - loss: 0.49216089889016845, acc: 0.8\n",
      "26:100 - loss: 0.4952822203493318, acc: 0.76\n",
      "26:150 - loss: 0.5123085114521363, acc: 0.7466666666666667\n",
      "26:200 - loss: 0.47900996742304874, acc: 0.78\n",
      "26:250 - loss: 0.4798435522250035, acc: 0.784\n",
      "26:300 - loss: 0.4840254890492861, acc: 0.7933333333333333\n",
      "26:350 - loss: 0.5014735381308059, acc: 0.7828571428571428\n",
      "26:400 - loss: 0.5086892211094463, acc: 0.775\n",
      "26:450 - loss: 0.5255366967431289, acc: 0.7555555555555555\n",
      "26:500 - loss: 0.5301331341375536, acc: 0.742\n",
      "26:550 - loss: 0.5320271965930184, acc: 0.7418181818181818\n",
      "26:600 - loss: 0.5301599586107364, acc: 0.7416666666666667\n",
      "26:650 - loss: 0.5272920387281491, acc: 0.7476923076923077\n",
      "26:700 - loss: 0.5246517410963983, acc: 0.7471428571428571\n",
      "26:750 - loss: 0.5263285192525201, acc: 0.748\n",
      "26:800 - loss: 0.5281852055726772, acc: 0.74625\n",
      "26:850 - loss: 0.5373508107461591, acc: 0.7411764705882353\n",
      "26:900 - loss: 0.5359585416036691, acc: 0.7422222222222222\n",
      "26:950 - loss: 0.5378624451890726, acc: 0.7421052631578947\n",
      "26:1000 - loss: 0.5339949443201699, acc: 0.746\n",
      "epoch: 26 - train loss: 0.5353225193666927, train acc: 0.7440305635148042\n",
      "epoch: 26 - test loss: 0.7412949938535979, test acc: 0.4828571428571429\n",
      "27:50 - loss: 0.5485536724628038, acc: 0.74\n",
      "27:100 - loss: 0.51210901634079, acc: 0.75\n",
      "27:150 - loss: 0.46506708480155806, acc: 0.7933333333333333\n",
      "27:200 - loss: 0.43371574809356944, acc: 0.81\n",
      "27:250 - loss: 0.4174404387516125, acc: 0.82\n",
      "27:300 - loss: 0.43161345353908537, acc: 0.81\n",
      "27:350 - loss: 0.43032235596794505, acc: 0.82\n",
      "27:400 - loss: 0.4578086017405982, acc: 0.8075\n",
      "27:450 - loss: 0.47028224147779596, acc: 0.7977777777777778\n",
      "27:500 - loss: 0.47543348766589033, acc: 0.792\n",
      "27:550 - loss: 0.4837037259201389, acc: 0.7854545454545454\n",
      "27:600 - loss: 0.48717895568007974, acc: 0.785\n",
      "27:650 - loss: 0.48740060966964666, acc: 0.7861538461538462\n",
      "27:700 - loss: 0.48765564628268804, acc: 0.7857142857142857\n",
      "27:750 - loss: 0.5014758918600479, acc: 0.772\n",
      "27:800 - loss: 0.5010167046316213, acc: 0.77\n",
      "27:850 - loss: 0.49823367890643516, acc: 0.7705882352941177\n",
      "27:900 - loss: 0.5018017260645309, acc: 0.7688888888888888\n",
      "27:950 - loss: 0.50338082692679, acc: 0.7673684210526316\n",
      "27:1000 - loss: 0.5106776930917186, acc: 0.763\n",
      "epoch: 27 - train loss: 0.5107037247013444, train acc: 0.7631327602674307\n",
      "epoch: 27 - test loss: 0.744980828050634, test acc: 0.5\n",
      "28:50 - loss: 0.49144754777795796, acc: 0.74\n",
      "28:100 - loss: 0.44241860196018096, acc: 0.81\n",
      "28:150 - loss: 0.4071038857301602, acc: 0.8466666666666667\n",
      "28:200 - loss: 0.41766483562980483, acc: 0.825\n",
      "28:250 - loss: 0.43541284386610923, acc: 0.824\n",
      "28:300 - loss: 0.4473741245225619, acc: 0.8166666666666667\n",
      "28:350 - loss: 0.4746975346051518, acc: 0.7914285714285715\n",
      "28:400 - loss: 0.46649400342624714, acc: 0.7975\n",
      "28:450 - loss: 0.46681840982428985, acc: 0.7933333333333333\n",
      "28:500 - loss: 0.4842852385719738, acc: 0.786\n",
      "28:550 - loss: 0.47778451692145846, acc: 0.7890909090909091\n",
      "28:600 - loss: 0.4805707391356533, acc: 0.7883333333333333\n",
      "28:650 - loss: 0.48636122760871403, acc: 0.78\n",
      "28:700 - loss: 0.4809615063500713, acc: 0.7857142857142857\n",
      "28:750 - loss: 0.47654336371631406, acc: 0.788\n",
      "28:800 - loss: 0.4756842821036853, acc: 0.7925\n",
      "28:850 - loss: 0.46627162786307763, acc: 0.7988235294117647\n",
      "28:900 - loss: 0.4691136242727254, acc: 0.7977777777777778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28:950 - loss: 0.46961989822038447, acc: 0.7957894736842105\n",
      "28:1000 - loss: 0.4769744145188776, acc: 0.793\n",
      "epoch: 28 - train loss: 0.4780825081204299, train acc: 0.789875835721108\n",
      "epoch: 28 - test loss: 0.8066051146815828, test acc: 0.5171428571428571\n",
      "29:50 - loss: 0.39539131086636947, acc: 0.8\n",
      "29:100 - loss: 0.36746038321083296, acc: 0.85\n",
      "29:150 - loss: 0.3969618907077236, acc: 0.8333333333333334\n",
      "29:200 - loss: 0.3795605498970643, acc: 0.845\n",
      "29:250 - loss: 0.3861571685426831, acc: 0.836\n",
      "29:300 - loss: 0.40385531119314116, acc: 0.8233333333333334\n",
      "29:350 - loss: 0.39810957444053113, acc: 0.8257142857142857\n",
      "29:400 - loss: 0.40288526108747613, acc: 0.8225\n",
      "29:450 - loss: 0.4009835115913209, acc: 0.8266666666666667\n",
      "29:500 - loss: 0.4110343409261006, acc: 0.82\n",
      "29:550 - loss: 0.410949280579464, acc: 0.82\n",
      "29:600 - loss: 0.40570014035052654, acc: 0.8183333333333334\n",
      "29:650 - loss: 0.4144292622140151, acc: 0.816923076923077\n",
      "29:700 - loss: 0.40952382849477664, acc: 0.82\n",
      "29:750 - loss: 0.41201810953054785, acc: 0.816\n",
      "29:800 - loss: 0.41559095604497, acc: 0.81375\n",
      "29:850 - loss: 0.4113623904994009, acc: 0.8164705882352942\n",
      "29:900 - loss: 0.4112367972134794, acc: 0.8155555555555556\n",
      "29:950 - loss: 0.4134039613039338, acc: 0.8168421052631579\n",
      "29:1000 - loss: 0.4138306078870182, acc: 0.816\n",
      "epoch: 29 - train loss: 0.41211434605189523, train acc: 0.8147086914995224\n",
      "epoch: 29 - test loss: 0.8529595000943776, test acc: 0.5314285714285715\n",
      "30:50 - loss: 0.20955348774399066, acc: 0.94\n",
      "30:100 - loss: 0.3014596436213548, acc: 0.9\n",
      "30:150 - loss: 0.3085246478502304, acc: 0.8733333333333333\n",
      "30:200 - loss: 0.297948562853838, acc: 0.875\n",
      "30:250 - loss: 0.292335763988562, acc: 0.876\n",
      "30:300 - loss: 0.286068553237151, acc: 0.89\n",
      "30:350 - loss: 0.3025768338663664, acc: 0.88\n",
      "30:400 - loss: 0.32587174894532, acc: 0.875\n",
      "30:450 - loss: 0.34303792637193015, acc: 0.8688888888888889\n",
      "30:500 - loss: 0.34635768375842385, acc: 0.868\n",
      "30:550 - loss: 0.33905349166378035, acc: 0.8709090909090909\n",
      "30:600 - loss: 0.34876560839504217, acc: 0.8666666666666667\n",
      "30:650 - loss: 0.3486739348420491, acc: 0.8661538461538462\n",
      "30:700 - loss: 0.34461892981033293, acc: 0.8671428571428571\n",
      "30:750 - loss: 0.35060317964060617, acc: 0.8613333333333333\n",
      "30:800 - loss: 0.36294305841791635, acc: 0.85625\n",
      "30:850 - loss: 0.3617430338460132, acc: 0.8588235294117647\n",
      "30:900 - loss: 0.3600194738402351, acc: 0.8577777777777778\n",
      "30:950 - loss: 0.3589157205576827, acc: 0.8568421052631578\n",
      "30:1000 - loss: 0.3609088128583644, acc: 0.856\n",
      "epoch: 30 - train loss: 0.3696306547022228, train acc: 0.8519579751671442\n",
      "epoch: 30 - test loss: 0.8764571704291727, test acc: 0.5057142857142857\n",
      "31:50 - loss: 0.1670068236975402, acc: 0.96\n",
      "31:100 - loss: 0.2012110652743138, acc: 0.95\n",
      "31:150 - loss: 0.23632055888661835, acc: 0.9266666666666666\n",
      "31:200 - loss: 0.29069219184717343, acc: 0.89\n",
      "31:250 - loss: 0.3037495176645249, acc: 0.884\n",
      "31:300 - loss: 0.29379883359459086, acc: 0.8866666666666667\n",
      "31:350 - loss: 0.295619871912027, acc: 0.8857142857142857\n",
      "31:400 - loss: 0.2852205912784008, acc: 0.89\n",
      "31:450 - loss: 0.28411885326309727, acc: 0.8888888888888888\n",
      "31:500 - loss: 0.2730283607842667, acc: 0.892\n",
      "31:550 - loss: 0.2759171442279939, acc: 0.8909090909090909\n",
      "31:600 - loss: 0.2777743859196078, acc: 0.8883333333333333\n",
      "31:650 - loss: 0.2958936822405548, acc: 0.8753846153846154\n",
      "31:700 - loss: 0.3081051967671095, acc: 0.8714285714285714\n",
      "31:750 - loss: 0.3052753903907382, acc: 0.8746666666666667\n",
      "31:800 - loss: 0.30099711271709956, acc: 0.875\n",
      "31:850 - loss: 0.3008083599246221, acc: 0.8752941176470588\n",
      "31:900 - loss: 0.3048149901443003, acc: 0.8733333333333333\n",
      "31:950 - loss: 0.30737301769505954, acc: 0.8736842105263158\n",
      "31:1000 - loss: 0.31075433877801933, acc: 0.875\n",
      "epoch: 31 - train loss: 0.3259909190602127, train acc: 0.8691499522445081\n",
      "epoch: 31 - test loss: 0.9312963126052949, test acc: 0.5228571428571429\n",
      "32:50 - loss: 0.2872818676142546, acc: 0.88\n",
      "32:100 - loss: 0.2968716768257489, acc: 0.89\n",
      "32:150 - loss: 0.2837049239697775, acc: 0.9\n",
      "32:200 - loss: 0.25471832962912805, acc: 0.905\n",
      "32:250 - loss: 0.2737836105502462, acc: 0.888\n",
      "32:300 - loss: 0.27673430323716103, acc: 0.8833333333333333\n",
      "32:350 - loss: 0.28798184331549337, acc: 0.8771428571428571\n",
      "32:400 - loss: 0.28450445797600726, acc: 0.8775\n",
      "32:450 - loss: 0.2816930684883376, acc: 0.8822222222222222\n",
      "32:500 - loss: 0.27610512436824536, acc: 0.886\n",
      "32:550 - loss: 0.26367244051574296, acc: 0.8909090909090909\n",
      "32:600 - loss: 0.2946397829456891, acc: 0.88\n",
      "32:650 - loss: 0.28313433169076546, acc: 0.8876923076923077\n",
      "32:700 - loss: 0.2803605829284963, acc: 0.8885714285714286\n",
      "32:750 - loss: 0.27761987840889185, acc: 0.8906666666666667\n",
      "32:800 - loss: 0.2779414735218166, acc: 0.88875\n",
      "32:850 - loss: 0.28423319750640624, acc: 0.8847058823529412\n",
      "32:900 - loss: 0.284136917266904, acc: 0.8844444444444445\n",
      "32:950 - loss: 0.28400762292105636, acc: 0.8831578947368421\n",
      "32:1000 - loss: 0.29077475035548317, acc: 0.88\n",
      "epoch: 32 - train loss: 0.29815905970746004, train acc: 0.874880611270296\n",
      "epoch: 32 - test loss: 0.9602624591251158, test acc: 0.5171428571428571\n",
      "33:50 - loss: 0.34710633789514744, acc: 0.86\n",
      "33:100 - loss: 0.29949221282959054, acc: 0.89\n",
      "33:150 - loss: 0.2695967316693981, acc: 0.8866666666666667\n",
      "33:200 - loss: 0.286026957231779, acc: 0.89\n",
      "33:250 - loss: 0.25812527707443705, acc: 0.9\n",
      "33:300 - loss: 0.26596054209370457, acc: 0.8933333333333333\n",
      "33:350 - loss: 0.26543760137079536, acc: 0.8942857142857142\n",
      "33:400 - loss: 0.28368989023939584, acc: 0.8875\n",
      "33:450 - loss: 0.283168777239667, acc: 0.8888888888888888\n",
      "33:500 - loss: 0.28078818049091436, acc: 0.89\n",
      "33:550 - loss: 0.27310143424331057, acc: 0.8945454545454545\n",
      "33:600 - loss: 0.2608161663902429, acc: 0.9033333333333333\n",
      "33:650 - loss: 0.2520532530815234, acc: 0.9061538461538462\n",
      "33:700 - loss: 0.2584181952753999, acc: 0.9042857142857142\n",
      "33:750 - loss: 0.26313907174227524, acc: 0.904\n",
      "33:800 - loss: 0.2614719595247576, acc: 0.90375\n",
      "33:850 - loss: 0.25476342694923093, acc: 0.9047058823529411\n",
      "33:900 - loss: 0.26033770871783, acc: 0.9011111111111111\n",
      "33:950 - loss: 0.2555807383207341, acc: 0.9031578947368422\n",
      "33:1000 - loss: 0.24985601272411848, acc: 0.904\n",
      "epoch: 33 - train loss: 0.25301604294608504, train acc: 0.9016236867239733\n",
      "epoch: 33 - test loss: 1.1162091939982315, test acc: 0.4942857142857143\n",
      "34:50 - loss: 0.17489546581046253, acc: 0.94\n",
      "34:100 - loss: 0.1230774754596438, acc: 0.96\n",
      "34:150 - loss: 0.113263445991745, acc: 0.9666666666666667\n",
      "34:200 - loss: 0.12217108731888704, acc: 0.955\n",
      "34:250 - loss: 0.13322126911159915, acc: 0.952\n",
      "34:300 - loss: 0.12878697719439783, acc: 0.95\n",
      "34:350 - loss: 0.1476871889885502, acc: 0.9428571428571428\n",
      "34:400 - loss: 0.1557981470045666, acc: 0.945\n",
      "34:450 - loss: 0.1596093463781981, acc: 0.9355555555555556\n",
      "34:500 - loss: 0.17843281992569446, acc: 0.928\n",
      "34:550 - loss: 0.18705832937710426, acc: 0.9254545454545454\n",
      "34:600 - loss: 0.18373594518883557, acc: 0.9266666666666666\n",
      "34:650 - loss: 0.1872626431770179, acc: 0.9261538461538461\n",
      "34:700 - loss: 0.19374346157038336, acc: 0.9228571428571428\n",
      "34:750 - loss: 0.19062231104582894, acc: 0.9253333333333333\n",
      "34:800 - loss: 0.18960559435587457, acc: 0.92625\n",
      "34:850 - loss: 0.18662690355476932, acc: 0.9270588235294117\n",
      "34:900 - loss: 0.18496856838634343, acc: 0.93\n",
      "34:950 - loss: 0.18474505349498396, acc: 0.9315789473684211\n",
      "34:1000 - loss: 0.18301428064934808, acc: 0.933\n",
      "epoch: 34 - train loss: 0.18521088095969082, train acc: 0.9321872015281757\n",
      "epoch: 34 - test loss: 1.3187582585264759, test acc: 0.47714285714285715\n",
      "35:50 - loss: 0.40298803274748285, acc: 0.88\n",
      "35:100 - loss: 0.3075547687736405, acc: 0.91\n",
      "35:150 - loss: 0.25018810504782124, acc: 0.9266666666666666\n",
      "35:200 - loss: 0.23436996755882375, acc: 0.93\n",
      "35:250 - loss: 0.20454175375978637, acc: 0.94\n",
      "35:300 - loss: 0.20694398058364186, acc: 0.9366666666666666\n",
      "35:350 - loss: 0.20517063656700282, acc: 0.9314285714285714\n",
      "35:400 - loss: 0.2113237636976862, acc: 0.93\n",
      "35:450 - loss: 0.20961316829668444, acc: 0.9311111111111111\n",
      "35:500 - loss: 0.20244524274627343, acc: 0.934\n",
      "35:550 - loss: 0.1922941179524826, acc: 0.9381818181818182\n",
      "35:600 - loss: 0.19697504882946057, acc: 0.9366666666666666\n",
      "35:650 - loss: 0.20770862362397569, acc: 0.9307692307692308\n",
      "35:700 - loss: 0.21268335711575118, acc: 0.9242857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35:750 - loss: 0.21217942127034492, acc: 0.9226666666666666\n",
      "35:800 - loss: 0.22347450544696693, acc: 0.9175\n",
      "35:850 - loss: 0.2305957808091033, acc: 0.9129411764705883\n",
      "35:900 - loss: 0.22260549113159303, acc: 0.9177777777777778\n",
      "35:950 - loss: 0.21526753005826343, acc: 0.9210526315789473\n",
      "35:1000 - loss: 0.2116075283267791, acc: 0.922\n",
      "epoch: 35 - train loss: 0.21135832365623203, train acc: 0.9216809933142311\n",
      "epoch: 35 - test loss: 1.128479567283923, test acc: 0.49142857142857144\n",
      "36:50 - loss: 0.12143467003711697, acc: 0.94\n",
      "36:100 - loss: 0.11401965406738991, acc: 0.95\n",
      "36:150 - loss: 0.09182168377077828, acc: 0.96\n",
      "36:200 - loss: 0.09046469074192523, acc: 0.965\n",
      "36:250 - loss: 0.09591598527048337, acc: 0.964\n",
      "36:300 - loss: 0.088528197805955, acc: 0.9666666666666667\n",
      "36:350 - loss: 0.08145342900845653, acc: 0.9685714285714285\n",
      "36:400 - loss: 0.08474327074211478, acc: 0.97\n",
      "36:450 - loss: 0.08777868512530121, acc: 0.9688888888888889\n",
      "36:500 - loss: 0.10582821614003007, acc: 0.96\n",
      "36:550 - loss: 0.11163604529435449, acc: 0.96\n",
      "36:600 - loss: 0.11498276475882352, acc: 0.9583333333333334\n",
      "36:650 - loss: 0.11891588342957839, acc: 0.9553846153846154\n",
      "36:700 - loss: 0.12000969430029311, acc: 0.9528571428571428\n",
      "36:750 - loss: 0.12945903540165118, acc: 0.9506666666666667\n",
      "36:800 - loss: 0.13467161013014767, acc: 0.94875\n",
      "36:850 - loss: 0.13957220630959313, acc: 0.9447058823529412\n",
      "36:900 - loss: 0.15224955433732038, acc: 0.9377777777777778\n",
      "36:950 - loss: 0.16092918164491044, acc: 0.9347368421052632\n",
      "36:1000 - loss: 0.16316895106629029, acc: 0.935\n",
      "epoch: 36 - train loss: 0.16146339837186535, train acc: 0.9350525310410697\n",
      "epoch: 36 - test loss: 1.2497192234061398, test acc: 0.4657142857142857\n",
      "37:50 - loss: 0.14087058857785223, acc: 0.94\n",
      "37:100 - loss: 0.12887747135206717, acc: 0.95\n",
      "37:150 - loss: 0.12250664111509522, acc: 0.9533333333333334\n",
      "37:200 - loss: 0.1531548339906303, acc: 0.945\n",
      "37:250 - loss: 0.1799806182025112, acc: 0.936\n",
      "37:300 - loss: 0.1644230007327258, acc: 0.94\n",
      "37:350 - loss: 0.14940599143893715, acc: 0.9457142857142857\n",
      "37:400 - loss: 0.14217696680937583, acc: 0.9475\n",
      "37:450 - loss: 0.17361566078412713, acc: 0.9333333333333333\n",
      "37:500 - loss: 0.17478214600849507, acc: 0.932\n",
      "37:550 - loss: 0.16762355755115324, acc: 0.9363636363636364\n",
      "37:600 - loss: 0.15723706224235454, acc: 0.9416666666666667\n",
      "37:650 - loss: 0.15905684754566696, acc: 0.9415384615384615\n",
      "37:700 - loss: 0.15481774306639812, acc: 0.9414285714285714\n",
      "37:750 - loss: 0.16176496491557102, acc: 0.94\n",
      "37:800 - loss: 0.1574932092612924, acc: 0.94125\n",
      "37:850 - loss: 0.15102518773115678, acc: 0.9447058823529412\n",
      "37:900 - loss: 0.1484541725719728, acc: 0.9466666666666667\n",
      "37:950 - loss: 0.14745621717944446, acc: 0.9473684210526315\n",
      "37:1000 - loss: 0.14693026922775265, acc: 0.949\n",
      "epoch: 37 - train loss: 0.14431485697847152, train acc: 0.9503342884431709\n",
      "epoch: 37 - test loss: 1.3291794081427828, test acc: 0.48857142857142855\n",
      "38:50 - loss: 0.03857942660848366, acc: 1.0\n",
      "38:100 - loss: 0.08937372725379872, acc: 0.98\n",
      "38:150 - loss: 0.11719976800897505, acc: 0.9666666666666667\n",
      "38:200 - loss: 0.12009057835913986, acc: 0.96\n",
      "38:250 - loss: 0.11326070416172504, acc: 0.96\n",
      "38:300 - loss: 0.10388993747733315, acc: 0.9633333333333334\n",
      "38:350 - loss: 0.1097611587004469, acc: 0.96\n",
      "38:400 - loss: 0.10362953848085976, acc: 0.965\n",
      "38:450 - loss: 0.10068338200309683, acc: 0.9644444444444444\n",
      "38:500 - loss: 0.09805179739369248, acc: 0.964\n",
      "38:550 - loss: 0.0931095355252563, acc: 0.9672727272727273\n",
      "38:600 - loss: 0.09048707263809465, acc: 0.9683333333333334\n",
      "38:650 - loss: 0.08912675880266051, acc: 0.9676923076923077\n",
      "38:700 - loss: 0.09020032533829438, acc: 0.9685714285714285\n",
      "38:750 - loss: 0.09369405795566345, acc: 0.9666666666666667\n",
      "38:800 - loss: 0.11119528046045664, acc: 0.95875\n",
      "38:850 - loss: 0.11727325801248746, acc: 0.9552941176470588\n",
      "38:900 - loss: 0.12780926846243704, acc: 0.9522222222222222\n",
      "38:950 - loss: 0.12536315040119675, acc: 0.9536842105263158\n",
      "38:1000 - loss: 0.1415777129434972, acc: 0.948\n",
      "epoch: 38 - train loss: 0.14946564277830862, train acc: 0.944603629417383\n",
      "epoch: 38 - test loss: 1.2942385251392508, test acc: 0.49714285714285716\n",
      "39:50 - loss: 0.17672248285258152, acc: 0.96\n",
      "39:100 - loss: 0.11889805436215303, acc: 0.97\n",
      "39:150 - loss: 0.10157413493322817, acc: 0.9733333333333334\n",
      "39:200 - loss: 0.11151438811961983, acc: 0.96\n",
      "39:250 - loss: 0.111385207751696, acc: 0.956\n",
      "39:300 - loss: 0.11273944536544205, acc: 0.95\n",
      "39:350 - loss: 0.11389925150962961, acc: 0.9542857142857143\n",
      "39:400 - loss: 0.1104660820720931, acc: 0.955\n",
      "39:450 - loss: 0.10464906666658594, acc: 0.96\n",
      "39:500 - loss: 0.09992069723302444, acc: 0.962\n",
      "39:550 - loss: 0.09936710483197295, acc: 0.9618181818181818\n",
      "39:600 - loss: 0.10620175775525753, acc: 0.96\n",
      "39:650 - loss: 0.10720825143425733, acc: 0.9584615384615385\n",
      "39:700 - loss: 0.11374260241146614, acc: 0.9585714285714285\n",
      "39:750 - loss: 0.11034590234257494, acc: 0.96\n",
      "39:800 - loss: 0.1098303501493272, acc: 0.96125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16228/3387253059.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m                 \u001b[0mloss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# We need to multiple by batch size as loss is the mean loss of the samples in the batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m                 \u001b[0mnum_corrects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''RESNET training code adapted from https://www.kaggle.com/gxkok21/resnet50-with-pytorch'''\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    model = model.cuda()\n",
    "#    print('Using GPU.')\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        samples = 0\n",
    "        loss_sum = 0\n",
    "        correct_sum = 0\n",
    "        for j, batch in enumerate(dataloaders[phase]):\n",
    "            X,labels = batch\n",
    "            #print(labels)\n",
    "            #print(len(X),len(X[0]),len(X[1]))\n",
    "            #print(labels[None,...].double())\n",
    "            #X = torch.Tensor(X)\n",
    "            #print(X.shape)\n",
    "            #labels = 1-labels\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                labels = labels.cuda()\n",
    "                model = model.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                y = model(X[None,...].double())\n",
    "                #y = model(X.flatten().double())\n",
    "                #print(y,labels)\n",
    "                loss = criterion(\n",
    "                    y,\n",
    "                    #torch.clip(y,0,1), \n",
    "                    labels[None,...].double()\n",
    "                    #labels.double()\n",
    "                )\n",
    "                #print(loss.item())\n",
    "                #print(labels[None,...].double())\n",
    "                \n",
    "                #writer.add_scalar(\"Loss/train\", loss, i)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                loss_sum += loss.item() * X.shape[0] # We need to multiple by batch size as loss is the mean loss of the samples in the batch\n",
    "                samples += X.shape[0]\n",
    "                num_corrects = torch.sum((y >= 0.5).float() == labels[0].float())\n",
    "                correct_sum += num_corrects\n",
    "                \n",
    "                # Print batch statistics every 50 batches\n",
    "                if j % 50 == 49 and phase == \"train\":\n",
    "                    print(\"{}:{} - loss: {}, acc: {}\".format(\n",
    "                        i + 1, \n",
    "                        j + 1, \n",
    "                        float(loss_sum) / float(samples), \n",
    "                        float(correct_sum) / float(samples)\n",
    "                    ))\n",
    "                \n",
    "        # Print epoch statistics\n",
    "        epoch_acc = float(correct_sum) / float(samples)\n",
    "        epoch_loss = float(loss_sum) / float(samples)\n",
    "        print(\"epoch: {} - {} loss: {}, {} acc: {}\".format(i + 1, phase, epoch_loss, phase, epoch_acc))\n",
    "        \n",
    "        # Deep copy the model\n",
    "        if phase == \"test\" and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, \"resnet18_coswara2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cd7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

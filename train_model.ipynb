{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebaf48b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydub # faster than librosa\n",
    "import numpy      as np\n",
    "import pandas     as pd\n",
    "import time\n",
    "\n",
    "from coughvid.pytorch.coughvid_dataset import CoughvidDataset\n",
    "from coughvid.pytorch.coughvid_dataset import CoughvidDataset\n",
    "from coughvid.pytorch.coswara_dataset import CoswaraDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchvision.models import resnet50, resnet18\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import copy\n",
    "\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "#writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdd6fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask file C:/COUGHVID_public_dataset/public_dataset/ does not exist. Calculating masks on the fly.\n",
      "8997 records ready to load across 2 groups.\n",
      "699 samples in the minority class.\n"
     ]
    }
   ],
   "source": [
    "dir = 'C:/COUGHVID_public_dataset/public_dataset/'\n",
    "\n",
    "full_dataset = CoughvidDataset(dir, 'metadata_compiled.csv', get_features=True)\n",
    "\n",
    "dataframe = full_dataset.dataframe\n",
    "\n",
    "minority_class_count = len(dataframe[dataframe['status']==2])\n",
    "\n",
    "print(f'{minority_class_count} samples in the minority class.')\n",
    "\n",
    "sample_dataset = CoughvidDataset(dir, 'metadata_compiled.csv', get_features=True, samples_per_class=minority_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ef46aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758 records ready to load across 2 groups.\n",
      "379 samples in the minority class.\n",
      "758 records ready to load across 2 groups.\n"
     ]
    }
   ],
   "source": [
    "dir = './data/coswara/'\n",
    "\n",
    "full_dataset = CoswaraDataset(dir, 'filtered_data.csv', get_features=True)\n",
    "\n",
    "dataframe = full_dataset.dataframe\n",
    "\n",
    "minority_class_count = len(dataframe[dataframe['covid_status']==1])\n",
    "\n",
    "print(f'{minority_class_count} samples in the minority class.')\n",
    "\n",
    "sample_dataset = CoswaraDataset(dir, 'filtered_data.csv', get_features=True, samples_per_class=minority_class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf60937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test samples\n",
    "train_indices, test_indices = train_test_split(np.arange(0,len(sample_dataset)-1), test_size=0.25)\n",
    "\n",
    "batch_size  = 1\n",
    "num_workers = 2\n",
    "\n",
    "train_loader  = DataLoader(sample_dataset, \n",
    "                           num_workers=num_workers,\n",
    "                           sampler=SubsetRandomSampler(train_indices)\n",
    "                          )\n",
    "\n",
    "test_loader   = DataLoader(sample_dataset, \n",
    "                           num_workers=num_workers,\n",
    "                           sampler=SubsetRandomSampler(test_indices)\n",
    "                          )\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"test\": test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a011ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and change output shape for binary prediction\n",
    "model = resnet50()\n",
    "\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(\n",
    "        in_features=2048, #2048 for resnet50\n",
    "        out_features=1\n",
    "    ),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,bias=True)\n",
    "\n",
    "model.double()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa02e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (linear): Linear(in_features=4050, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs\n",
    "    \n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "EPOCHS = int(n_iters / (len(sample_dataset) / batch_size))\n",
    "\n",
    "lr_rate = 0.00001    \n",
    "    \n",
    "model = LogisticRegression(4050,1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr_rate)\n",
    "criterion = torch.nn.BCELoss()\n",
    "model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16812b10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:50 - loss: 1.3773438241659657, acc: 0.52\n",
      "1:100 - loss: 1.210422534143371, acc: 0.5\n",
      "1:150 - loss: 1.103712671869177, acc: 0.5133333333333333\n",
      "1:200 - loss: 1.1194825138055535, acc: 0.495\n",
      "1:250 - loss: 1.0582234638692185, acc: 0.492\n",
      "1:300 - loss: 1.036168053782516, acc: 0.48333333333333334\n",
      "1:350 - loss: 1.0162485492627167, acc: 0.4828571428571429\n",
      "1:400 - loss: 0.9997444971665211, acc: 0.4775\n",
      "1:450 - loss: 1.0008785378209286, acc: 0.4688888888888889\n",
      "1:500 - loss: 0.9949616565729389, acc: 0.47\n",
      "1:550 - loss: 0.9756683694603254, acc: 0.4709090909090909\n",
      "epoch: 1 - train loss: 0.9722796260657434, train acc: 0.4691358024691358\n",
      "epoch: 1 - test loss: 0.705419147142651, test acc: 0.5789473684210527\n",
      "2:50 - loss: 0.7777113691118274, acc: 0.54\n",
      "2:100 - loss: 0.8208505019383524, acc: 0.54\n",
      "2:150 - loss: 0.8166100903143694, acc: 0.58\n",
      "2:200 - loss: 0.8130358767633598, acc: 0.575\n",
      "2:250 - loss: 0.8015454872175428, acc: 0.568\n",
      "2:300 - loss: 0.7883982937300786, acc: 0.56\n",
      "2:350 - loss: 0.7891802490850887, acc: 0.5485714285714286\n",
      "2:400 - loss: 0.7919743315268782, acc: 0.54\n",
      "2:450 - loss: 0.7922038356450466, acc: 0.5355555555555556\n",
      "2:500 - loss: 0.7822102412796371, acc: 0.548\n",
      "2:550 - loss: 0.7813539819366898, acc: 0.5454545454545454\n",
      "epoch: 2 - train loss: 0.7821211333243907, train acc: 0.5379188712522046\n",
      "epoch: 2 - test loss: 0.6843015104301894, test acc: 0.531578947368421\n",
      "3:50 - loss: 0.7057789822585369, acc: 0.58\n",
      "3:100 - loss: 0.7626789528686255, acc: 0.57\n",
      "3:150 - loss: 0.7987042396405848, acc: 0.54\n",
      "3:200 - loss: 0.7739594912184508, acc: 0.545\n",
      "3:250 - loss: 0.7514034221779672, acc: 0.572\n",
      "3:300 - loss: 0.7310839183396483, acc: 0.58\n",
      "3:350 - loss: 0.7389859610391678, acc: 0.5628571428571428\n",
      "3:400 - loss: 0.7411224560229698, acc: 0.5575\n",
      "3:450 - loss: 0.7554372883338104, acc: 0.5533333333333333\n",
      "3:500 - loss: 0.763702406395536, acc: 0.54\n",
      "3:550 - loss: 0.7628449903681362, acc: 0.5345454545454545\n",
      "epoch: 3 - train loss: 0.763975869737244, train acc: 0.5343915343915344\n",
      "epoch: 3 - test loss: 0.6909372485779398, test acc: 0.5052631578947369\n",
      "4:50 - loss: 0.6887032669140658, acc: 0.68\n",
      "4:100 - loss: 0.7258195005595439, acc: 0.62\n",
      "4:150 - loss: 0.7488033881434906, acc: 0.58\n",
      "4:200 - loss: 0.7609304733606203, acc: 0.57\n",
      "4:250 - loss: 0.7370589973730614, acc: 0.58\n",
      "4:300 - loss: 0.7472682197995769, acc: 0.5666666666666667\n",
      "4:350 - loss: 0.7518146606082122, acc: 0.5514285714285714\n",
      "4:400 - loss: 0.7427862824956604, acc: 0.565\n",
      "4:450 - loss: 0.7362381931326312, acc: 0.5733333333333334\n",
      "4:500 - loss: 0.7343603886205695, acc: 0.576\n",
      "4:550 - loss: 0.7229603155714124, acc: 0.5854545454545454\n",
      "epoch: 4 - train loss: 0.7234928278982673, train acc: 0.582010582010582\n",
      "epoch: 4 - test loss: 0.6823699737258443, test acc: 0.6052631578947368\n",
      "5:50 - loss: 0.6589925398250602, acc: 0.68\n",
      "5:100 - loss: 0.7132140337809186, acc: 0.63\n",
      "5:150 - loss: 0.7358204466970606, acc: 0.5733333333333334\n",
      "5:200 - loss: 0.7489165672554922, acc: 0.575\n",
      "5:250 - loss: 0.7567875100330753, acc: 0.568\n",
      "5:300 - loss: 0.7526766273184219, acc: 0.5666666666666667\n",
      "5:350 - loss: 0.7449366890418836, acc: 0.5628571428571428\n",
      "5:400 - loss: 0.7464668372251033, acc: 0.5675\n",
      "5:450 - loss: 0.7293759753531658, acc: 0.5844444444444444\n",
      "5:500 - loss: 0.7371759095176466, acc: 0.57\n",
      "5:550 - loss: 0.7322364159398753, acc: 0.5763636363636364\n",
      "epoch: 5 - train loss: 0.7338407570044401, train acc: 0.5714285714285714\n",
      "epoch: 5 - test loss: 0.6960547980043783, test acc: 0.5631578947368421\n",
      "6:50 - loss: 0.6530457624943786, acc: 0.68\n",
      "6:100 - loss: 0.6944287248480693, acc: 0.62\n",
      "6:150 - loss: 0.6803694500246755, acc: 0.6\n",
      "6:200 - loss: 0.6906542036428087, acc: 0.595\n",
      "6:250 - loss: 0.709182495050827, acc: 0.58\n",
      "6:300 - loss: 0.6996377054841176, acc: 0.5833333333333334\n",
      "6:350 - loss: 0.6971205210427139, acc: 0.5857142857142857\n",
      "6:400 - loss: 0.6905469450467164, acc: 0.5925\n",
      "6:450 - loss: 0.6953305801215096, acc: 0.58\n",
      "6:500 - loss: 0.697965722983097, acc: 0.574\n",
      "6:550 - loss: 0.6978608149502501, acc: 0.5690909090909091\n",
      "epoch: 6 - train loss: 0.6967580665770344, train acc: 0.5679012345679012\n",
      "epoch: 6 - test loss: 0.7282579165412103, test acc: 0.4631578947368421\n",
      "7:50 - loss: 0.6701610242255939, acc: 0.56\n",
      "7:100 - loss: 0.6784735179710129, acc: 0.58\n",
      "7:150 - loss: 0.6720763068711257, acc: 0.5866666666666667\n",
      "7:200 - loss: 0.6734826614873402, acc: 0.58\n",
      "7:250 - loss: 0.6543078074045595, acc: 0.584\n",
      "7:300 - loss: 0.6679306507413246, acc: 0.5766666666666667\n",
      "7:350 - loss: 0.6721751440555366, acc: 0.5714285714285714\n",
      "7:400 - loss: 0.6775761142685127, acc: 0.56\n",
      "7:450 - loss: 0.6800780119941596, acc: 0.5644444444444444\n",
      "7:500 - loss: 0.6838904440216282, acc: 0.572\n",
      "7:550 - loss: 0.6873009714066534, acc: 0.5690909090909091\n",
      "epoch: 7 - train loss: 0.6926356996305695, train acc: 0.5608465608465608\n",
      "epoch: 7 - test loss: 0.7330711148487777, test acc: 0.5578947368421052\n",
      "8:50 - loss: 0.6929420121639289, acc: 0.56\n",
      "8:100 - loss: 0.7152867836426118, acc: 0.54\n",
      "8:150 - loss: 0.7086698963943409, acc: 0.5333333333333333\n",
      "8:200 - loss: 0.7113624243728829, acc: 0.54\n",
      "8:250 - loss: 0.7026386829863103, acc: 0.568\n",
      "8:300 - loss: 0.7031839265610571, acc: 0.5633333333333334\n",
      "8:350 - loss: 0.6863590230638994, acc: 0.5914285714285714\n",
      "8:400 - loss: 0.6808484492144679, acc: 0.6\n",
      "8:450 - loss: 0.6869283037457722, acc: 0.5844444444444444\n",
      "8:500 - loss: 0.6870247263370789, acc: 0.588\n",
      "8:550 - loss: 0.6906306969773491, acc: 0.5709090909090909\n",
      "epoch: 8 - train loss: 0.688937885692692, train acc: 0.5749559082892416\n",
      "epoch: 8 - test loss: 1.0691771819536153, test acc: 0.4631578947368421\n",
      "9:50 - loss: 0.6629020052100004, acc: 0.52\n",
      "9:100 - loss: 0.6227398170714146, acc: 0.64\n",
      "9:150 - loss: 0.6352499527887958, acc: 0.64\n",
      "9:200 - loss: 0.6942927826278665, acc: 0.605\n",
      "9:250 - loss: 0.707576755271429, acc: 0.588\n",
      "9:300 - loss: 0.7259587496547308, acc: 0.5666666666666667\n",
      "9:350 - loss: 0.7203962896569134, acc: 0.5657142857142857\n",
      "9:400 - loss: 0.7136235447682915, acc: 0.575\n",
      "9:450 - loss: 0.7102772054093395, acc: 0.58\n",
      "9:500 - loss: 0.729827236737479, acc: 0.572\n",
      "9:550 - loss: 0.7286872202142635, acc: 0.5672727272727273\n",
      "epoch: 9 - train loss: 0.7255057492669246, train acc: 0.5714285714285714\n",
      "epoch: 9 - test loss: 1.114957666374813, test acc: 0.4631578947368421\n",
      "10:50 - loss: 0.7635674283131128, acc: 0.5\n",
      "10:100 - loss: 0.7365406523900812, acc: 0.52\n",
      "10:150 - loss: 0.720303114061812, acc: 0.54\n",
      "10:200 - loss: 0.7206511867644836, acc: 0.53\n",
      "10:250 - loss: 0.7182206165017049, acc: 0.528\n",
      "10:300 - loss: 0.7180980970710665, acc: 0.5133333333333333\n",
      "10:350 - loss: 0.7209884234423184, acc: 0.5114285714285715\n",
      "10:400 - loss: 0.71720118470247, acc: 0.5225\n",
      "10:450 - loss: 0.7179068357598183, acc: 0.5088888888888888\n",
      "10:500 - loss: 0.7162879928951718, acc: 0.506\n",
      "10:550 - loss: 0.7151222711762047, acc: 0.5109090909090909\n",
      "epoch: 10 - train loss: 0.7148851013519792, train acc: 0.5097001763668431\n",
      "epoch: 10 - test loss: 0.6677748149465961, test acc: 0.5421052631578948\n",
      "11:50 - loss: 0.6885643421315204, acc: 0.5\n",
      "11:100 - loss: 0.7035654809620793, acc: 0.52\n",
      "11:150 - loss: 0.7054740247958112, acc: 0.5\n",
      "11:200 - loss: 0.7037798099187913, acc: 0.505\n",
      "11:250 - loss: 0.7045903498999319, acc: 0.496\n",
      "11:300 - loss: 0.7057994860035689, acc: 0.49333333333333335\n",
      "11:350 - loss: 0.7037661746523166, acc: 0.49714285714285716\n",
      "11:400 - loss: 0.6998786921932673, acc: 0.52\n",
      "11:450 - loss: 0.6953818050620786, acc: 0.5377777777777778\n",
      "11:500 - loss: 0.6965622997177291, acc: 0.546\n",
      "11:550 - loss: 0.6977889447488302, acc: 0.5436363636363636\n",
      "epoch: 11 - train loss: 0.6996002363751602, train acc: 0.5414462081128748\n",
      "epoch: 11 - test loss: 0.7958307867216283, test acc: 0.4631578947368421\n",
      "12:50 - loss: 0.7236798258283244, acc: 0.52\n",
      "12:100 - loss: 0.7462199064821856, acc: 0.47\n",
      "12:150 - loss: 0.7281737440371623, acc: 0.5133333333333333\n",
      "12:200 - loss: 0.7230789310962632, acc: 0.515\n",
      "12:250 - loss: 0.6977528489464402, acc: 0.524\n",
      "12:300 - loss: 0.7047155748806907, acc: 0.5266666666666666\n",
      "12:350 - loss: 0.7034783045078677, acc: 0.5342857142857143\n",
      "12:400 - loss: 0.7032878637651198, acc: 0.5225\n",
      "12:450 - loss: 0.7056761185803254, acc: 0.5155555555555555\n",
      "12:500 - loss: 0.6978670489661586, acc: 0.528\n",
      "12:550 - loss: 0.69613378382284, acc: 0.5236363636363637\n",
      "epoch: 12 - train loss: 0.6969009238863578, train acc: 0.5255731922398589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 - test loss: 0.6497733136009725, test acc: 0.5789473684210527\n",
      "13:50 - loss: 0.6459336890685639, acc: 0.68\n",
      "13:100 - loss: 0.6881361544733993, acc: 0.59\n",
      "13:150 - loss: 0.7006788023286343, acc: 0.54\n",
      "13:200 - loss: 0.6986750918813897, acc: 0.505\n",
      "13:250 - loss: 0.6927291265910736, acc: 0.516\n",
      "13:300 - loss: 0.7030358818236778, acc: 0.52\n",
      "13:350 - loss: 0.7075149721593521, acc: 0.52\n",
      "13:400 - loss: 0.7166119093916383, acc: 0.5125\n",
      "13:450 - loss: 0.712567375207528, acc: 0.5155555555555555\n",
      "13:500 - loss: 0.713166963153416, acc: 0.522\n",
      "13:550 - loss: 0.7163538515616824, acc: 0.509090909090909\n",
      "epoch: 13 - train loss: 0.7163495609513182, train acc: 0.5079365079365079\n",
      "epoch: 13 - test loss: 0.670345557303403, test acc: 0.6210526315789474\n",
      "14:50 - loss: 0.6314504265135681, acc: 0.7\n",
      "14:100 - loss: 0.6651339576913995, acc: 0.61\n",
      "14:150 - loss: 0.6802941735349299, acc: 0.6066666666666667\n",
      "14:200 - loss: 0.6870106328468043, acc: 0.59\n",
      "14:250 - loss: 0.6927060865885185, acc: 0.56\n",
      "14:300 - loss: 0.6901153400933963, acc: 0.5733333333333334\n",
      "14:350 - loss: 0.6855283952365335, acc: 0.5885714285714285\n",
      "14:400 - loss: 0.6911751143474311, acc: 0.575\n",
      "14:450 - loss: 0.6937531329709167, acc: 0.5711111111111111\n",
      "14:500 - loss: 0.6949015291933452, acc: 0.558\n",
      "14:550 - loss: 0.6952204803434903, acc: 0.56\n",
      "epoch: 14 - train loss: 0.6956285233027276, train acc: 0.5590828924162258\n",
      "epoch: 14 - test loss: 0.6898265232675632, test acc: 0.5789473684210527\n",
      "15:50 - loss: 0.7011267617546916, acc: 0.44\n",
      "15:100 - loss: 0.6968198997737491, acc: 0.49\n",
      "15:150 - loss: 0.7048775397862751, acc: 0.5066666666666667\n",
      "15:200 - loss: 0.7043815395292239, acc: 0.515\n",
      "15:250 - loss: 0.710716884132664, acc: 0.52\n",
      "15:300 - loss: 0.7107404782196041, acc: 0.5166666666666667\n",
      "15:350 - loss: 0.7098935682206594, acc: 0.52\n",
      "15:400 - loss: 0.7083850985256304, acc: 0.52\n",
      "15:450 - loss: 0.708296209632198, acc: 0.5222222222222223\n",
      "15:500 - loss: 0.7065433973222146, acc: 0.524\n",
      "15:550 - loss: 0.7073449658716041, acc: 0.5272727272727272\n",
      "epoch: 15 - train loss: 0.707825052160534, train acc: 0.5255731922398589\n",
      "epoch: 15 - test loss: 0.6934989055980042, test acc: 0.5210526315789473\n",
      "16:50 - loss: 0.6892133757498652, acc: 0.56\n",
      "16:100 - loss: 0.6907681223221621, acc: 0.54\n",
      "16:150 - loss: 0.6924097533994088, acc: 0.5266666666666666\n",
      "16:200 - loss: 0.7012416860722247, acc: 0.5\n",
      "16:250 - loss: 0.7014516298785236, acc: 0.504\n",
      "16:300 - loss: 0.7011636348323751, acc: 0.5166666666666667\n",
      "16:350 - loss: 0.7002460378506118, acc: 0.5171428571428571\n",
      "16:400 - loss: 0.7006089172512724, acc: 0.5125\n",
      "16:450 - loss: 0.7016181286757153, acc: 0.5133333333333333\n",
      "16:500 - loss: 0.7035208717690327, acc: 0.508\n",
      "16:550 - loss: 0.70346489933802, acc: 0.5036363636363637\n",
      "epoch: 16 - train loss: 0.7032727522727635, train acc: 0.5044091710758377\n",
      "epoch: 16 - test loss: 0.6893447781683787, test acc: 0.531578947368421\n",
      "17:50 - loss: 0.7032534768320395, acc: 0.48\n",
      "17:100 - loss: 0.6987908140287827, acc: 0.5\n",
      "17:150 - loss: 0.6992176874250763, acc: 0.48\n",
      "17:200 - loss: 0.6996468369660882, acc: 0.48\n",
      "17:250 - loss: 0.7001993195713245, acc: 0.472\n",
      "17:300 - loss: 0.6996104945319684, acc: 0.48\n",
      "17:350 - loss: 0.6996461163193601, acc: 0.48857142857142855\n",
      "17:400 - loss: 0.6983883340644723, acc: 0.4975\n",
      "17:450 - loss: 0.7001828427594926, acc: 0.4911111111111111\n",
      "17:500 - loss: 0.6996473493196671, acc: 0.498\n",
      "17:550 - loss: 0.699904899492295, acc: 0.49454545454545457\n",
      "epoch: 17 - train loss: 0.700248951112626, train acc: 0.48853615520282184\n",
      "epoch: 17 - test loss: 0.6927157506329615, test acc: 0.4631578947368421\n",
      "18:50 - loss: 0.6987783267214676, acc: 0.46\n",
      "18:100 - loss: 0.6980597821089186, acc: 0.48\n",
      "18:150 - loss: 0.6989263937776607, acc: 0.48\n",
      "18:200 - loss: 0.6981059446606652, acc: 0.49\n",
      "18:250 - loss: 0.6980690269976266, acc: 0.496\n",
      "18:300 - loss: 0.6969228430706079, acc: 0.49666666666666665\n",
      "18:350 - loss: 0.6958791782052353, acc: 0.5057142857142857\n",
      "18:400 - loss: 0.6969068886593389, acc: 0.5025\n",
      "18:450 - loss: 0.6961364100980229, acc: 0.5111111111111111\n",
      "18:500 - loss: 0.6974071652109282, acc: 0.516\n",
      "18:550 - loss: 0.6974729964610946, acc: 0.5145454545454545\n",
      "epoch: 18 - train loss: 0.698068655971773, train acc: 0.5132275132275133\n",
      "epoch: 18 - test loss: 0.7008354840198131, test acc: 0.4631578947368421\n",
      "19:50 - loss: 0.7047007786594641, acc: 0.48\n",
      "19:100 - loss: 0.705126792243983, acc: 0.48\n",
      "19:150 - loss: 0.7044658012339935, acc: 0.49333333333333335\n",
      "19:200 - loss: 0.7020936847528608, acc: 0.505\n",
      "19:250 - loss: 0.7001591768171822, acc: 0.504\n",
      "19:300 - loss: 0.6976720300885937, acc: 0.52\n",
      "19:350 - loss: 0.6995822971287041, acc: 0.5057142857142857\n",
      "19:400 - loss: 0.6986085439314834, acc: 0.5\n",
      "19:450 - loss: 0.6974459994444614, acc: 0.5022222222222222\n",
      "19:500 - loss: 0.6970985734715055, acc: 0.508\n",
      "19:550 - loss: 0.6970647681104701, acc: 0.5072727272727273\n",
      "epoch: 19 - train loss: 0.6962689353401693, train acc: 0.5132275132275133\n",
      "epoch: 19 - test loss: 0.6937469492879429, test acc: 0.5\n",
      "20:50 - loss: 0.6861575998860957, acc: 0.58\n",
      "20:100 - loss: 0.7017589180915946, acc: 0.51\n",
      "20:150 - loss: 0.6993007093297242, acc: 0.5066666666666667\n",
      "20:200 - loss: 0.7065910964630211, acc: 0.51\n",
      "20:250 - loss: 0.708085092013614, acc: 0.504\n",
      "20:300 - loss: 0.7080723372948046, acc: 0.48333333333333334\n",
      "20:350 - loss: 0.7066746494670313, acc: 0.4857142857142857\n",
      "20:400 - loss: 0.7058770414565868, acc: 0.4875\n",
      "20:450 - loss: 0.7034605334594105, acc: 0.4955555555555556\n",
      "20:500 - loss: 0.7017974621644654, acc: 0.504\n",
      "20:550 - loss: 0.7024386308268876, acc: 0.49454545454545457\n",
      "epoch: 20 - train loss: 0.702271529579963, train acc: 0.49559082892416223\n",
      "epoch: 20 - test loss: 0.694456411773335, test acc: 0.5105263157894737\n",
      "21:50 - loss: 0.695643361639139, acc: 0.5\n",
      "21:100 - loss: 0.6991436431195079, acc: 0.5\n",
      "21:150 - loss: 0.6979785541190774, acc: 0.5\n",
      "21:200 - loss: 0.6947121801359859, acc: 0.525\n",
      "21:250 - loss: 0.695327081832288, acc: 0.524\n",
      "21:300 - loss: 0.7011759895761359, acc: 0.52\n",
      "21:350 - loss: 0.7002463087977965, acc: 0.52\n",
      "21:400 - loss: 0.7012053405185182, acc: 0.5175\n",
      "21:450 - loss: 0.696352463099503, acc: 0.5333333333333333\n",
      "21:500 - loss: 0.7005863443214922, acc: 0.526\n",
      "21:550 - loss: 0.699968061760477, acc: 0.5290909090909091\n",
      "epoch: 21 - train loss: 0.6999596702108247, train acc: 0.5255731922398589\n",
      "epoch: 21 - test loss: 0.6927127175754151, test acc: 0.531578947368421\n",
      "22:50 - loss: 0.6992385769327012, acc: 0.54\n",
      "22:100 - loss: 0.6951858593012316, acc: 0.45\n",
      "22:150 - loss: 0.69724116945241, acc: 0.4666666666666667\n",
      "22:200 - loss: 0.7084071018372242, acc: 0.475\n",
      "22:250 - loss: 0.7063877141114041, acc: 0.492\n",
      "22:300 - loss: 0.710559351356081, acc: 0.5033333333333333\n",
      "22:350 - loss: 0.7114322236280096, acc: 0.5\n",
      "22:400 - loss: 0.7152039637720214, acc: 0.505\n",
      "22:450 - loss: 0.7174421129273029, acc: 0.4911111111111111\n",
      "22:500 - loss: 0.7183159099630723, acc: 0.484\n",
      "22:550 - loss: 0.7160033623810587, acc: 0.4909090909090909\n",
      "epoch: 22 - train loss: 0.71531344438412, train acc: 0.49382716049382713\n",
      "epoch: 22 - test loss: 0.6896117566299201, test acc: 0.5526315789473685\n",
      "23:50 - loss: 0.6884807317662661, acc: 0.54\n",
      "23:100 - loss: 0.6897584157841203, acc: 0.52\n",
      "23:150 - loss: 0.6924989652640914, acc: 0.5266666666666666\n",
      "23:200 - loss: 0.6909701429889732, acc: 0.545\n",
      "23:250 - loss: 0.6933984217522645, acc: 0.552\n",
      "23:300 - loss: 0.6994610902326811, acc: 0.5333333333333333\n",
      "23:350 - loss: 0.7015605631337625, acc: 0.5085714285714286\n",
      "23:400 - loss: 0.7012387546760374, acc: 0.51\n",
      "23:450 - loss: 0.7008856498271299, acc: 0.5044444444444445\n",
      "23:500 - loss: 0.6997145473576132, acc: 0.508\n",
      "23:550 - loss: 0.6993829067010721, acc: 0.5072727272727273\n",
      "epoch: 23 - train loss: 0.6993143787099779, train acc: 0.5097001763668431\n",
      "epoch: 23 - test loss: 0.6814460278471186, test acc: 0.5473684210526316\n",
      "24:50 - loss: 0.6625479648407652, acc: 0.68\n",
      "24:100 - loss: 0.6704723514999003, acc: 0.64\n",
      "24:150 - loss: 0.6968483302608119, acc: 0.5866666666666667\n",
      "24:200 - loss: 0.7031735763313499, acc: 0.56\n",
      "24:250 - loss: 0.702813342977584, acc: 0.532\n",
      "24:300 - loss: 0.7032981362888799, acc: 0.5133333333333333\n",
      "24:350 - loss: 0.7036708524820693, acc: 0.5028571428571429\n",
      "24:400 - loss: 0.7036698507323377, acc: 0.505\n",
      "24:450 - loss: 0.7002459289856885, acc: 0.5177777777777778\n",
      "24:500 - loss: 0.7033399441253304, acc: 0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24:550 - loss: 0.7012299602022262, acc: 0.5145454545454545\n",
      "epoch: 24 - train loss: 0.6981589346159892, train acc: 0.5220458553791887\n",
      "epoch: 24 - test loss: 0.8882429895381964, test acc: 0.4631578947368421\n",
      "25:50 - loss: 0.7164761710052149, acc: 0.58\n",
      "25:100 - loss: 0.7175332925762434, acc: 0.57\n",
      "25:150 - loss: 0.7126058916661332, acc: 0.54\n",
      "25:200 - loss: 0.6996134398137192, acc: 0.575\n",
      "25:250 - loss: 0.7021722682850365, acc: 0.556\n",
      "25:300 - loss: 0.701345206787489, acc: 0.53\n",
      "25:350 - loss: 0.7008874712099208, acc: 0.5285714285714286\n",
      "25:400 - loss: 0.7011319000850168, acc: 0.51\n",
      "25:450 - loss: 0.6995986829868895, acc: 0.52\n",
      "25:500 - loss: 0.7006153934417052, acc: 0.524\n",
      "25:550 - loss: 0.7013042543503863, acc: 0.5236363636363637\n",
      "epoch: 25 - train loss: 0.7006199886643599, train acc: 0.5255731922398589\n",
      "epoch: 25 - test loss: 0.7006045699863187, test acc: 0.5210526315789473\n",
      "26:50 - loss: 0.6901898606985469, acc: 0.54\n",
      "26:100 - loss: 0.6963570495158491, acc: 0.48\n",
      "26:150 - loss: 0.6924813865871045, acc: 0.49333333333333335\n",
      "26:200 - loss: 0.6948489975066685, acc: 0.495\n",
      "26:250 - loss: 0.691403829611471, acc: 0.508\n",
      "26:300 - loss: 0.6933832884236243, acc: 0.5\n",
      "26:350 - loss: 0.6943754209734242, acc: 0.5\n",
      "26:400 - loss: 0.6951377168691742, acc: 0.4925\n",
      "26:450 - loss: 0.6910155974121216, acc: 0.5066666666666667\n",
      "26:500 - loss: 0.6954253547700299, acc: 0.5\n",
      "26:550 - loss: 0.6955808538029553, acc: 0.5054545454545455\n",
      "epoch: 26 - train loss: 0.6964468957050328, train acc: 0.5061728395061729\n",
      "epoch: 26 - test loss: 4.151377484874665, test acc: 0.47368421052631576\n",
      "27:50 - loss: 0.6919917866753607, acc: 0.52\n",
      "27:100 - loss: 0.7090445253031251, acc: 0.44\n",
      "27:150 - loss: 0.7046371736049308, acc: 0.46\n",
      "27:200 - loss: 0.6991415098973117, acc: 0.5\n",
      "27:250 - loss: 0.7013854507816917, acc: 0.488\n",
      "27:300 - loss: 0.7008978758400782, acc: 0.4766666666666667\n",
      "27:350 - loss: 0.7012107496838202, acc: 0.4857142857142857\n",
      "27:400 - loss: 0.7008227689704014, acc: 0.49\n",
      "27:450 - loss: 0.7019375254078153, acc: 0.4822222222222222\n",
      "27:500 - loss: 0.7023660908392574, acc: 0.474\n",
      "27:550 - loss: 0.7013504967106959, acc: 0.48\n",
      "epoch: 27 - train loss: 0.7013147978542715, train acc: 0.48148148148148145\n",
      "epoch: 27 - test loss: 0.695140123340418, test acc: 0.4631578947368421\n",
      "28:50 - loss: 0.7000471300189045, acc: 0.52\n",
      "28:100 - loss: 0.693839449470874, acc: 0.53\n",
      "28:150 - loss: 0.7005313714216835, acc: 0.5266666666666666\n",
      "28:200 - loss: 0.7009806226711117, acc: 0.505\n",
      "28:250 - loss: 0.6995499824504612, acc: 0.508\n",
      "28:300 - loss: 0.6987871600780502, acc: 0.49666666666666665\n",
      "28:350 - loss: 0.6983295712885828, acc: 0.49142857142857144\n",
      "28:400 - loss: 0.6988347685871122, acc: 0.4825\n",
      "28:450 - loss: 0.6988580206060385, acc: 0.48444444444444446\n",
      "28:500 - loss: 0.6988862995593071, acc: 0.48\n",
      "28:550 - loss: 0.6983326532228943, acc: 0.4763636363636364\n",
      "epoch: 28 - train loss: 0.697939562208051, train acc: 0.48500881834215165\n",
      "epoch: 28 - test loss: 0.6902571307644002, test acc: 0.5631578947368421\n",
      "29:50 - loss: 0.6904468291315595, acc: 0.48\n",
      "29:100 - loss: 0.683127205784757, acc: 0.49\n",
      "29:150 - loss: 0.6788749019899682, acc: 0.5133333333333333\n",
      "29:200 - loss: 0.6806221617855912, acc: 0.515\n",
      "29:250 - loss: 0.6837402965065071, acc: 0.52\n",
      "29:300 - loss: 0.6859566347585203, acc: 0.5266666666666666\n",
      "29:350 - loss: 0.6855450356000494, acc: 0.5314285714285715\n",
      "29:400 - loss: 0.6856169745679834, acc: 0.5375\n",
      "29:450 - loss: 0.6850737269538224, acc: 0.5444444444444444\n",
      "29:500 - loss: 0.6885886265067099, acc: 0.532\n",
      "29:550 - loss: 0.6905998188203463, acc: 0.5327272727272727\n",
      "epoch: 29 - train loss: 0.6904116404209374, train acc: 0.5361552028218695\n",
      "epoch: 29 - test loss: 0.9274156823384053, test acc: 0.45789473684210524\n",
      "30:50 - loss: 0.7257332362236092, acc: 0.48\n",
      "30:100 - loss: 0.717603560367975, acc: 0.47\n",
      "30:150 - loss: 0.7102256357402704, acc: 0.49333333333333335\n",
      "30:200 - loss: 0.7097676713542629, acc: 0.47\n",
      "30:250 - loss: 0.707222116788479, acc: 0.472\n",
      "30:300 - loss: 0.7064803652607724, acc: 0.4766666666666667\n",
      "30:350 - loss: 0.704914246292317, acc: 0.4685714285714286\n",
      "30:400 - loss: 0.7034185255180911, acc: 0.475\n",
      "30:450 - loss: 0.7023630094659861, acc: 0.48\n",
      "30:500 - loss: 0.7004784257746111, acc: 0.492\n",
      "30:550 - loss: 0.7007812056361856, acc: 0.4890909090909091\n",
      "epoch: 30 - train loss: 0.7005231174240454, train acc: 0.48853615520282184\n",
      "epoch: 30 - test loss: 0.6914612052098652, test acc: 0.531578947368421\n",
      "31:50 - loss: 0.6927593489051853, acc: 0.52\n",
      "31:100 - loss: 0.691780100707286, acc: 0.54\n",
      "31:150 - loss: 0.6931371463190469, acc: 0.5266666666666666\n",
      "31:200 - loss: 0.6939782980837115, acc: 0.515\n",
      "31:250 - loss: 0.69369622580422, acc: 0.512\n",
      "31:300 - loss: 0.6931545020555112, acc: 0.5333333333333333\n",
      "31:350 - loss: 0.6937852920900791, acc: 0.5257142857142857\n",
      "31:400 - loss: 0.692893419403234, acc: 0.5275\n",
      "31:450 - loss: 0.6928810737947306, acc: 0.5244444444444445\n",
      "31:500 - loss: 0.6926762455560491, acc: 0.524\n",
      "31:550 - loss: 0.6905814158790748, acc: 0.5290909090909091\n",
      "epoch: 31 - train loss: 0.6908494343693298, train acc: 0.527336860670194\n",
      "epoch: 31 - test loss: 0.8064949069941961, test acc: 0.4631578947368421\n",
      "32:50 - loss: 0.6958478653973296, acc: 0.46\n",
      "32:100 - loss: 0.6865231563443009, acc: 0.54\n",
      "32:150 - loss: 0.6860594958747299, acc: 0.54\n",
      "32:200 - loss: 0.6840460303547607, acc: 0.535\n",
      "32:250 - loss: 0.6804468963273069, acc: 0.54\n",
      "32:300 - loss: 0.6760589783388075, acc: 0.55\n",
      "32:350 - loss: 0.6782235059634039, acc: 0.5485714285714286\n",
      "32:400 - loss: 0.6829201241251548, acc: 0.54\n",
      "32:450 - loss: 0.6873188060913772, acc: 0.5266666666666666\n",
      "32:500 - loss: 0.687610082614142, acc: 0.528\n",
      "32:550 - loss: 0.6895909979077764, acc: 0.5236363636363637\n",
      "epoch: 32 - train loss: 0.6897796741186458, train acc: 0.527336860670194\n",
      "epoch: 32 - test loss: 0.8038216174439838, test acc: 0.5\n",
      "33:50 - loss: 0.6869268348386223, acc: 0.58\n",
      "33:100 - loss: 0.6977545261536753, acc: 0.54\n",
      "33:150 - loss: 0.6982858087122454, acc: 0.49333333333333335\n",
      "33:200 - loss: 0.6959243696408892, acc: 0.5\n",
      "33:250 - loss: 0.7022639224191819, acc: 0.496\n",
      "33:300 - loss: 0.7037083992466681, acc: 0.5\n",
      "33:350 - loss: 0.7038830677925045, acc: 0.5\n",
      "33:400 - loss: 0.7035811438475226, acc: 0.5075\n",
      "33:450 - loss: 0.7053732840089219, acc: 0.5\n",
      "33:500 - loss: 0.7037165129291659, acc: 0.504\n",
      "33:550 - loss: 0.7031514698618262, acc: 0.5127272727272727\n",
      "epoch: 33 - train loss: 0.7046732959255653, train acc: 0.5114638447971781\n",
      "epoch: 33 - test loss: 0.6904124290432898, test acc: 0.5473684210526316\n",
      "34:50 - loss: 0.6776658209585679, acc: 0.62\n",
      "34:100 - loss: 0.7074091296220592, acc: 0.55\n",
      "34:150 - loss: 0.7072091672462274, acc: 0.5333333333333333\n",
      "34:200 - loss: 0.7058993826386054, acc: 0.5\n",
      "34:250 - loss: 0.703964330513657, acc: 0.492\n",
      "34:300 - loss: 0.7030032906080549, acc: 0.5033333333333333\n",
      "34:350 - loss: 0.6991806037417923, acc: 0.52\n",
      "34:400 - loss: 0.6986505740953536, acc: 0.5225\n",
      "34:450 - loss: 0.6963813747741108, acc: 0.5266666666666666\n",
      "34:500 - loss: 0.6973441528986694, acc: 0.524\n",
      "34:550 - loss: 0.6957466578139527, acc: 0.5309090909090909\n",
      "epoch: 34 - train loss: 0.6956782584959278, train acc: 0.5326278659611993\n",
      "epoch: 34 - test loss: 0.6864622980055023, test acc: 0.5947368421052631\n",
      "35:50 - loss: 0.6985670683172206, acc: 0.52\n",
      "35:100 - loss: 0.694372049731826, acc: 0.56\n",
      "35:150 - loss: 0.6930597994038816, acc: 0.5533333333333333\n",
      "35:200 - loss: 0.6907966698541596, acc: 0.56\n",
      "35:250 - loss: 0.6987817451439706, acc: 0.564\n",
      "35:300 - loss: 0.702939038601857, acc: 0.5566666666666666\n",
      "35:350 - loss: 0.6927342317832664, acc: 0.5657142857142857\n",
      "35:400 - loss: 0.6992687566826864, acc: 0.5425\n",
      "35:450 - loss: 0.7011558729193308, acc: 0.5288888888888889\n",
      "35:500 - loss: 0.7015470039695493, acc: 0.528\n",
      "35:550 - loss: 0.7008239437674509, acc: 0.5236363636363637\n",
      "epoch: 35 - train loss: 0.6980836934968803, train acc: 0.527336860670194\n",
      "epoch: 35 - test loss: 0.7441376852794992, test acc: 0.4631578947368421\n",
      "36:50 - loss: 0.7110603392895051, acc: 0.52\n",
      "36:100 - loss: 0.7075296494523653, acc: 0.51\n",
      "36:150 - loss: 0.6925980267818213, acc: 0.5266666666666666\n",
      "36:200 - loss: 0.6821651224689925, acc: 0.555\n",
      "36:250 - loss: 0.6981875395750345, acc: 0.548\n",
      "36:300 - loss: 0.704981616528131, acc: 0.5266666666666666\n",
      "36:350 - loss: 0.7023159981634699, acc: 0.5285714285714286\n",
      "36:400 - loss: 0.7012078563397757, acc: 0.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36:450 - loss: 0.6967925814708688, acc: 0.54\n",
      "36:500 - loss: 0.6975158467137941, acc: 0.538\n",
      "36:550 - loss: 0.6974719005140779, acc: 0.5345454545454545\n",
      "epoch: 36 - train loss: 0.695372229761262, train acc: 0.5396825396825397\n",
      "epoch: 36 - test loss: 0.7152334613513028, test acc: 0.46842105263157896\n",
      "37:50 - loss: 0.7311657259266338, acc: 0.52\n",
      "37:100 - loss: 0.7158984860306568, acc: 0.55\n",
      "37:150 - loss: 0.7189938340528269, acc: 0.49333333333333335\n",
      "37:200 - loss: 0.7162933681715085, acc: 0.49\n",
      "37:250 - loss: 0.7144279920181017, acc: 0.48\n",
      "37:300 - loss: 0.7125314472694965, acc: 0.48333333333333334\n",
      "37:350 - loss: 0.7109087868279047, acc: 0.48857142857142855\n",
      "37:400 - loss: 0.7092613519211731, acc: 0.495\n",
      "37:450 - loss: 0.7057228869635774, acc: 0.5088888888888888\n",
      "37:500 - loss: 0.7021457016797473, acc: 0.518\n",
      "37:550 - loss: 0.6998010595644683, acc: 0.5236363636363637\n",
      "epoch: 37 - train loss: 0.6946481040810284, train acc: 0.5308641975308642\n",
      "epoch: 37 - test loss: 1.6558484749160847, test acc: 0.4631578947368421\n",
      "38:50 - loss: 0.7476295868532109, acc: 0.54\n",
      "38:100 - loss: 0.7182347539522114, acc: 0.56\n",
      "38:150 - loss: 0.7126716831883942, acc: 0.5266666666666666\n",
      "38:200 - loss: 0.707138143750421, acc: 0.535\n",
      "38:250 - loss: 0.7011766374558845, acc: 0.544\n",
      "38:300 - loss: 0.7064684705084757, acc: 0.5233333333333333\n",
      "38:350 - loss: 0.7060543534281178, acc: 0.5085714285714286\n",
      "38:400 - loss: 0.7047216284848283, acc: 0.5125\n",
      "38:450 - loss: 0.7024427348560418, acc: 0.5133333333333333\n",
      "38:500 - loss: 0.6998883682317061, acc: 0.518\n",
      "38:550 - loss: 0.6950508861714102, acc: 0.5272727272727272\n",
      "epoch: 38 - train loss: 0.6937356467695066, train acc: 0.5308641975308642\n",
      "epoch: 38 - test loss: 0.6868262170363465, test acc: 0.5368421052631579\n",
      "39:50 - loss: 0.695820797442054, acc: 0.52\n",
      "39:100 - loss: 0.7024860479127365, acc: 0.49\n",
      "39:150 - loss: 0.697342370461397, acc: 0.52\n",
      "39:200 - loss: 0.6882803602787524, acc: 0.555\n",
      "39:250 - loss: 0.6841879288632132, acc: 0.568\n",
      "39:300 - loss: 0.6842809987696657, acc: 0.56\n",
      "39:350 - loss: 0.6875011473547632, acc: 0.5542857142857143\n",
      "39:400 - loss: 0.6851757032438089, acc: 0.555\n",
      "39:450 - loss: 0.6814472704727165, acc: 0.5644444444444444\n",
      "39:500 - loss: 0.6832526914219726, acc: 0.558\n",
      "39:550 - loss: 0.6776628198988935, acc: 0.5618181818181818\n",
      "epoch: 39 - train loss: 0.6737032048421165, train acc: 0.5714285714285714\n",
      "epoch: 39 - test loss: 0.7067599998902616, test acc: 0.45789473684210524\n",
      "40:50 - loss: 0.5734212813751575, acc: 0.68\n",
      "40:100 - loss: 0.6108639460534464, acc: 0.65\n",
      "40:150 - loss: 0.6422969933297241, acc: 0.6133333333333333\n",
      "40:200 - loss: 0.6458169510643883, acc: 0.615\n",
      "40:250 - loss: 0.6515778353226778, acc: 0.608\n",
      "40:300 - loss: 0.6546617096436237, acc: 0.5933333333333334\n",
      "40:350 - loss: 0.6533235425558819, acc: 0.6057142857142858\n",
      "40:400 - loss: 0.6523389806126718, acc: 0.6075\n",
      "40:450 - loss: 0.6506070650152672, acc: 0.6133333333333333\n",
      "40:500 - loss: 0.6492031879677275, acc: 0.62\n",
      "40:550 - loss: 0.6528825634824559, acc: 0.6127272727272727\n",
      "epoch: 40 - train loss: 0.6519784304608562, train acc: 0.6137566137566137\n",
      "epoch: 40 - test loss: 0.6851434580586153, test acc: 0.5578947368421052\n",
      "41:50 - loss: 0.6495064204519977, acc: 0.68\n",
      "41:100 - loss: 0.6130184782128721, acc: 0.64\n",
      "41:150 - loss: 0.6582632801663899, acc: 0.6133333333333333\n",
      "41:200 - loss: 0.6678399938094877, acc: 0.595\n",
      "41:250 - loss: 0.682650784725994, acc: 0.564\n",
      "41:300 - loss: 0.6781072883564776, acc: 0.5833333333333334\n",
      "41:350 - loss: 0.681956411709471, acc: 0.5742857142857143\n",
      "41:400 - loss: 0.6856831961582084, acc: 0.565\n",
      "41:450 - loss: 0.6870666033507743, acc: 0.5666666666666667\n",
      "41:500 - loss: 0.6914245319040936, acc: 0.542\n",
      "41:550 - loss: 0.6928158405490273, acc: 0.54\n",
      "epoch: 41 - train loss: 0.6917433058426253, train acc: 0.544973544973545\n",
      "epoch: 41 - test loss: 0.7061854055282819, test acc: 0.4473684210526316\n",
      "42:50 - loss: 0.6322663372858041, acc: 0.58\n",
      "42:100 - loss: 0.6419036946126581, acc: 0.57\n",
      "42:150 - loss: 0.661284154929385, acc: 0.5533333333333333\n",
      "42:200 - loss: 0.6723648366575837, acc: 0.535\n",
      "42:250 - loss: 0.6665561111740987, acc: 0.548\n",
      "42:300 - loss: 0.6514667905579333, acc: 0.5733333333333334\n",
      "42:350 - loss: 0.6530484348408033, acc: 0.58\n",
      "42:400 - loss: 0.6562651956049024, acc: 0.5775\n",
      "42:450 - loss: 0.6533781059331113, acc: 0.5933333333333334\n",
      "42:500 - loss: 0.6554850190723621, acc: 0.598\n",
      "42:550 - loss: 0.6533091327781235, acc: 0.5981818181818181\n",
      "epoch: 42 - train loss: 0.6540785592798491, train acc: 0.599647266313933\n",
      "epoch: 42 - test loss: 0.6804790469621866, test acc: 0.5157894736842106\n",
      "43:50 - loss: 0.6586941423212067, acc: 0.6\n",
      "43:100 - loss: 0.6474725397456993, acc: 0.64\n",
      "43:150 - loss: 0.6264765170972935, acc: 0.66\n",
      "43:200 - loss: 0.6264231791406035, acc: 0.655\n",
      "43:250 - loss: 0.6297296541070612, acc: 0.652\n",
      "43:300 - loss: 0.6221038906026684, acc: 0.67\n",
      "43:350 - loss: 0.6087017958042882, acc: 0.6685714285714286\n",
      "43:400 - loss: 0.6239501422553908, acc: 0.655\n",
      "43:450 - loss: 0.6295282897523403, acc: 0.6422222222222222\n",
      "43:500 - loss: 0.6366262090895064, acc: 0.624\n",
      "43:550 - loss: 0.6370146657441184, acc: 0.6218181818181818\n",
      "epoch: 43 - train loss: 0.6406624394678265, train acc: 0.6155202821869489\n",
      "epoch: 43 - test loss: 0.6948947082741125, test acc: 0.45789473684210524\n",
      "44:50 - loss: 0.6050192318196157, acc: 0.62\n",
      "44:100 - loss: 0.6461626031962915, acc: 0.57\n",
      "44:150 - loss: 0.6198748462586484, acc: 0.6133333333333333\n",
      "44:200 - loss: 0.6453328586044106, acc: 0.585\n",
      "44:250 - loss: 0.6492006272781609, acc: 0.592\n",
      "44:300 - loss: 0.651712173608006, acc: 0.6033333333333334\n",
      "44:350 - loss: 0.6608000612909558, acc: 0.5885714285714285\n",
      "44:400 - loss: 0.6500470157493028, acc: 0.6125\n",
      "44:450 - loss: 0.6404664346465058, acc: 0.6288888888888889\n",
      "44:500 - loss: 0.6432054983394727, acc: 0.634\n",
      "44:550 - loss: 0.6397175594084296, acc: 0.64\n",
      "epoch: 44 - train loss: 0.6423329023247648, train acc: 0.6384479717813051\n",
      "epoch: 44 - test loss: 0.7000424623927807, test acc: 0.5631578947368421\n",
      "45:50 - loss: 0.6428532394949947, acc: 0.62\n",
      "45:100 - loss: 0.6383547539940408, acc: 0.64\n",
      "45:150 - loss: 0.61941877697797, acc: 0.66\n",
      "45:200 - loss: 0.622991545295315, acc: 0.65\n",
      "45:250 - loss: 0.621705860573217, acc: 0.66\n",
      "45:300 - loss: 0.6446096379084405, acc: 0.64\n",
      "45:350 - loss: 0.6435381639137399, acc: 0.6485714285714286\n",
      "45:400 - loss: 0.6408586034992215, acc: 0.65\n",
      "45:450 - loss: 0.6274744609310434, acc: 0.66\n",
      "45:500 - loss: 0.6248896848133969, acc: 0.662\n",
      "45:550 - loss: 0.6292884733055083, acc: 0.6581818181818182\n",
      "epoch: 45 - train loss: 0.628708934878552, train acc: 0.654320987654321\n",
      "epoch: 45 - test loss: 0.679506885382313, test acc: 0.6\n",
      "46:50 - loss: 0.4959631183151438, acc: 0.76\n",
      "46:100 - loss: 0.5381900345874857, acc: 0.74\n",
      "46:150 - loss: 0.5764210658479018, acc: 0.7066666666666667\n",
      "46:200 - loss: 0.5893108793530916, acc: 0.7\n",
      "46:250 - loss: 0.5803332355580139, acc: 0.716\n",
      "46:300 - loss: 0.591378771456451, acc: 0.71\n",
      "46:350 - loss: 0.5970374484211042, acc: 0.6914285714285714\n",
      "46:400 - loss: 0.6050348783927543, acc: 0.685\n",
      "46:450 - loss: 0.5996985831371217, acc: 0.6911111111111111\n",
      "46:500 - loss: 0.6001311468541367, acc: 0.688\n",
      "46:550 - loss: 0.5952230517142797, acc: 0.6890909090909091\n",
      "epoch: 46 - train loss: 0.5933291190750544, train acc: 0.689594356261023\n",
      "epoch: 46 - test loss: 0.7569107586909268, test acc: 0.5157894736842106\n",
      "47:50 - loss: 0.47212415972673016, acc: 0.8\n",
      "47:100 - loss: 0.5291780732869882, acc: 0.72\n",
      "47:150 - loss: 0.5212484714910841, acc: 0.74\n",
      "47:200 - loss: 0.5607569334790827, acc: 0.685\n",
      "47:250 - loss: 0.5858406240330717, acc: 0.664\n",
      "47:300 - loss: 0.5855629559055165, acc: 0.6666666666666666\n",
      "47:350 - loss: 0.5963947180346946, acc: 0.6628571428571428\n",
      "47:400 - loss: 0.6141703173341057, acc: 0.6475\n",
      "47:450 - loss: 0.6117360965504458, acc: 0.6466666666666666\n",
      "47:500 - loss: 0.6063118125137005, acc: 0.652\n",
      "47:550 - loss: 0.606235293430571, acc: 0.6509090909090909\n",
      "epoch: 47 - train loss: 0.6106881117106764, train acc: 0.6455026455026455\n",
      "epoch: 47 - test loss: 1.218699150730494, test acc: 0.4789473684210526\n",
      "48:50 - loss: 0.6639312775844654, acc: 0.68\n",
      "48:100 - loss: 0.6347131176511374, acc: 0.67\n",
      "48:150 - loss: 0.6026738556774109, acc: 0.6933333333333334\n",
      "48:200 - loss: 0.597317077137139, acc: 0.695\n",
      "48:250 - loss: 0.6189994636286335, acc: 0.688\n",
      "48:300 - loss: 0.6052449725551247, acc: 0.7033333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48:350 - loss: 0.6003172181738955, acc: 0.7028571428571428\n",
      "48:400 - loss: 0.5873342143304776, acc: 0.7125\n",
      "48:450 - loss: 0.5930284361425381, acc: 0.7022222222222222\n",
      "48:500 - loss: 0.5885370514420438, acc: 0.702\n",
      "48:550 - loss: 0.5781107280927805, acc: 0.7\n",
      "epoch: 48 - train loss: 0.5722905040869994, train acc: 0.7054673721340388\n",
      "epoch: 48 - test loss: 1.7094089076699264, test acc: 0.5105263157894737\n",
      "49:50 - loss: 0.5881008464833698, acc: 0.78\n",
      "49:100 - loss: 0.5558975905252046, acc: 0.77\n",
      "49:150 - loss: 0.557953758688539, acc: 0.7733333333333333\n",
      "49:200 - loss: 0.5683528411804802, acc: 0.76\n",
      "49:250 - loss: 0.5747035179457198, acc: 0.74\n",
      "49:300 - loss: 0.5862895970803156, acc: 0.7166666666666667\n",
      "49:350 - loss: 0.5862765252574, acc: 0.7114285714285714\n",
      "49:400 - loss: 0.5816830447003042, acc: 0.705\n",
      "49:450 - loss: 0.5754372337565681, acc: 0.7022222222222222\n",
      "49:500 - loss: 0.5653453963377285, acc: 0.712\n",
      "49:550 - loss: 0.5631372932779359, acc: 0.7090909090909091\n",
      "epoch: 49 - train loss: 0.5601856310849064, train acc: 0.708994708994709\n",
      "epoch: 49 - test loss: 0.8210937023152597, test acc: 0.5263157894736842\n",
      "50:50 - loss: 0.6485141787103917, acc: 0.72\n",
      "50:100 - loss: 0.6041666642807382, acc: 0.7\n",
      "50:150 - loss: 0.6001775291667321, acc: 0.7\n",
      "50:200 - loss: 0.5784096056733103, acc: 0.705\n",
      "50:250 - loss: 0.5849794738568803, acc: 0.7\n",
      "50:300 - loss: 0.5629978560102318, acc: 0.72\n",
      "50:350 - loss: 0.5595945911376706, acc: 0.7257142857142858\n",
      "50:400 - loss: 0.5436517681901794, acc: 0.7425\n",
      "50:450 - loss: 0.5465770619756428, acc: 0.7377777777777778\n",
      "50:500 - loss: 0.5450371174749808, acc: 0.734\n",
      "50:550 - loss: 0.5417274943182405, acc: 0.7345454545454545\n",
      "epoch: 50 - train loss: 0.5397328968002189, train acc: 0.7354497354497355\n",
      "epoch: 50 - test loss: 0.726397276570202, test acc: 0.6263157894736842\n",
      "51:50 - loss: 0.40971762486983243, acc: 0.84\n",
      "51:100 - loss: 0.47227029861742204, acc: 0.8\n",
      "51:150 - loss: 0.4991306640166844, acc: 0.78\n",
      "51:200 - loss: 0.496149283809568, acc: 0.79\n",
      "51:250 - loss: 0.49152318578524673, acc: 0.792\n",
      "51:300 - loss: 0.48421116065425074, acc: 0.79\n",
      "51:350 - loss: 0.485733863055821, acc: 0.7857142857142857\n",
      "51:400 - loss: 0.47787636224820107, acc: 0.79\n",
      "51:450 - loss: 0.47342783320819126, acc: 0.7911111111111111\n",
      "51:500 - loss: 0.48086800721347833, acc: 0.782\n",
      "51:550 - loss: 0.48240641614161983, acc: 0.7781818181818182\n",
      "epoch: 51 - train loss: 0.4818408858310528, train acc: 0.7760141093474426\n",
      "epoch: 51 - test loss: 0.7104848848322078, test acc: 0.5526315789473685\n",
      "52:50 - loss: 0.5855619048391275, acc: 0.74\n",
      "52:100 - loss: 0.4797233616880709, acc: 0.79\n",
      "52:150 - loss: 0.45171013194813564, acc: 0.8\n",
      "52:200 - loss: 0.46687277940967503, acc: 0.775\n",
      "52:250 - loss: 0.4876958646476345, acc: 0.768\n",
      "52:300 - loss: 0.502633052968622, acc: 0.7566666666666667\n",
      "52:350 - loss: 0.5001421390142408, acc: 0.7628571428571429\n",
      "52:400 - loss: 0.4981079598291438, acc: 0.765\n",
      "52:450 - loss: 0.5012801277590293, acc: 0.7622222222222222\n",
      "52:500 - loss: 0.4903295561734034, acc: 0.766\n",
      "52:550 - loss: 0.48942194962435864, acc: 0.7672727272727272\n",
      "epoch: 52 - train loss: 0.4860917757944341, train acc: 0.7707231040564374\n",
      "epoch: 52 - test loss: 0.7106467033686307, test acc: 0.5947368421052631\n",
      "53:50 - loss: 0.40102762839232076, acc: 0.8\n",
      "53:100 - loss: 0.47607151784374646, acc: 0.77\n",
      "53:150 - loss: 0.4771875172978925, acc: 0.7533333333333333\n",
      "53:200 - loss: 0.48128654646273505, acc: 0.745\n",
      "53:250 - loss: 0.4897604676326719, acc: 0.736\n",
      "53:300 - loss: 0.4768872448292111, acc: 0.7533333333333333\n",
      "53:350 - loss: 0.4893874307068054, acc: 0.74\n",
      "53:400 - loss: 0.479162119574119, acc: 0.755\n",
      "53:450 - loss: 0.47467153076904445, acc: 0.7644444444444445\n",
      "53:500 - loss: 0.456393332159772, acc: 0.776\n",
      "53:550 - loss: 0.44420144069958434, acc: 0.7854545454545454\n",
      "epoch: 53 - train loss: 0.4562945670078883, train acc: 0.781305114638448\n",
      "epoch: 53 - test loss: 1.1303585433134438, test acc: 0.5421052631578948\n",
      "54:50 - loss: 0.30058427770518464, acc: 0.92\n",
      "54:100 - loss: 0.40458761040709035, acc: 0.84\n",
      "54:150 - loss: 0.43218110354009914, acc: 0.82\n",
      "54:200 - loss: 0.4746980756783152, acc: 0.79\n",
      "54:250 - loss: 0.47310181140637453, acc: 0.792\n",
      "54:300 - loss: 0.4456953199788308, acc: 0.81\n",
      "54:350 - loss: 0.4288057194770431, acc: 0.8171428571428572\n",
      "54:400 - loss: 0.4493279172007464, acc: 0.81\n",
      "54:450 - loss: 0.45138488754273765, acc: 0.8\n",
      "54:500 - loss: 0.4366139770330212, acc: 0.808\n",
      "54:550 - loss: 0.4323896175963476, acc: 0.8109090909090909\n",
      "epoch: 54 - train loss: 0.4360745370141152, train acc: 0.8077601410934744\n",
      "epoch: 54 - test loss: 0.9376950736382527, test acc: 0.5631578947368421\n",
      "55:50 - loss: 0.33031289624755816, acc: 0.84\n",
      "55:100 - loss: 0.271660524881066, acc: 0.89\n",
      "55:150 - loss: 0.3175343098357454, acc: 0.88\n",
      "55:200 - loss: 0.34074005492589166, acc: 0.845\n",
      "55:250 - loss: 0.35592027145690336, acc: 0.84\n",
      "55:300 - loss: 0.34707781824442246, acc: 0.85\n",
      "55:350 - loss: 0.33820884152967023, acc: 0.8514285714285714\n",
      "55:400 - loss: 0.3595010502621932, acc: 0.845\n",
      "55:450 - loss: 0.3825202225896326, acc: 0.8288888888888889\n",
      "55:500 - loss: 0.3856341494273639, acc: 0.824\n",
      "55:550 - loss: 0.3973308536183622, acc: 0.8145454545454546\n",
      "epoch: 55 - train loss: 0.39557424557421517, train acc: 0.8165784832451499\n",
      "epoch: 55 - test loss: 1.1356377233539132, test acc: 0.47368421052631576\n",
      "56:50 - loss: 0.34843084457682977, acc: 0.92\n",
      "56:100 - loss: 0.30648729451166945, acc: 0.92\n",
      "56:150 - loss: 0.3376099460372987, acc: 0.9\n",
      "56:200 - loss: 0.3709864267669039, acc: 0.865\n",
      "56:250 - loss: 0.3643425866673165, acc: 0.864\n",
      "56:300 - loss: 0.36806424313007813, acc: 0.8633333333333333\n",
      "56:350 - loss: 0.38790576091431506, acc: 0.8457142857142858\n",
      "56:400 - loss: 0.38672931447211945, acc: 0.8425\n",
      "56:450 - loss: 0.3844761083774051, acc: 0.8377777777777777\n",
      "56:500 - loss: 0.38608346321120457, acc: 0.836\n",
      "56:550 - loss: 0.3814454352181799, acc: 0.8381818181818181\n",
      "epoch: 56 - train loss: 0.38518296198757085, train acc: 0.8342151675485009\n",
      "epoch: 56 - test loss: 0.9234062190237221, test acc: 0.5789473684210527\n",
      "57:50 - loss: 0.3789393132596065, acc: 0.84\n",
      "57:100 - loss: 0.3640856841078853, acc: 0.85\n",
      "57:150 - loss: 0.3544080343171048, acc: 0.86\n",
      "57:200 - loss: 0.34828741699775917, acc: 0.87\n",
      "57:250 - loss: 0.34598875569301113, acc: 0.86\n",
      "57:300 - loss: 0.3386200782304973, acc: 0.86\n",
      "57:350 - loss: 0.32694192297444674, acc: 0.8628571428571429\n",
      "57:400 - loss: 0.31689833748313523, acc: 0.865\n",
      "57:450 - loss: 0.32454505020499785, acc: 0.86\n",
      "57:500 - loss: 0.32619143965072783, acc: 0.862\n",
      "57:550 - loss: 0.3191891292576457, acc: 0.8654545454545455\n",
      "epoch: 57 - train loss: 0.31457339542827056, train acc: 0.8677248677248677\n",
      "epoch: 57 - test loss: 0.8335430113989232, test acc: 0.6421052631578947\n",
      "58:50 - loss: 0.15866484574027123, acc: 0.98\n",
      "58:100 - loss: 0.2848113885858877, acc: 0.88\n",
      "58:150 - loss: 0.3280419594484853, acc: 0.86\n",
      "58:200 - loss: 0.30506724151693465, acc: 0.87\n",
      "58:250 - loss: 0.31209438357216945, acc: 0.872\n",
      "58:300 - loss: 0.3049793666589133, acc: 0.8766666666666667\n",
      "58:350 - loss: 0.334806395923883, acc: 0.86\n",
      "58:400 - loss: 0.33900690264850897, acc: 0.85\n",
      "58:450 - loss: 0.3514379187791436, acc: 0.8422222222222222\n",
      "58:500 - loss: 0.34729303649857274, acc: 0.84\n",
      "58:550 - loss: 0.35311098514466926, acc: 0.8381818181818181\n",
      "epoch: 58 - train loss: 0.34731348236980963, train acc: 0.8430335097001763\n",
      "epoch: 58 - test loss: 0.9021345707768204, test acc: 0.5105263157894737\n",
      "59:50 - loss: 0.2552695108932665, acc: 0.94\n",
      "59:100 - loss: 0.23810914183199203, acc: 0.93\n",
      "59:150 - loss: 0.24122805844354295, acc: 0.9133333333333333\n",
      "59:200 - loss: 0.23332771886332415, acc: 0.915\n",
      "59:250 - loss: 0.24078701732344304, acc: 0.912\n",
      "59:300 - loss: 0.24266738060071796, acc: 0.91\n",
      "59:350 - loss: 0.23529272490632208, acc: 0.9114285714285715\n",
      "59:400 - loss: 0.24206144084185818, acc: 0.9125\n",
      "59:450 - loss: 0.2513103955343484, acc: 0.9022222222222223\n",
      "59:500 - loss: 0.2796405241630246, acc: 0.888\n",
      "59:550 - loss: 0.2837499108579077, acc: 0.8854545454545455\n",
      "epoch: 59 - train loss: 0.28910908123884443, train acc: 0.8835978835978836\n",
      "epoch: 59 - test loss: 0.8832279216746082, test acc: 0.5368421052631579\n",
      "60:50 - loss: 0.23598485593424157, acc: 0.92\n",
      "60:100 - loss: 0.35117017638467635, acc: 0.88\n",
      "60:150 - loss: 0.3491678233820345, acc: 0.8733333333333333\n",
      "60:200 - loss: 0.3331261490350461, acc: 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60:250 - loss: 0.32123315043222883, acc: 0.88\n",
      "60:300 - loss: 0.32762987995487397, acc: 0.8666666666666667\n",
      "60:350 - loss: 0.3261254626140917, acc: 0.8657142857142858\n",
      "60:400 - loss: 0.31964412405847276, acc: 0.8725\n",
      "60:450 - loss: 0.3104597997177522, acc: 0.8777777777777778\n",
      "60:500 - loss: 0.31532099597582325, acc: 0.88\n"
     ]
    }
   ],
   "source": [
    "'''RESNET training code adapted from https://www.kaggle.com/gxkok21/resnet50-with-pytorch'''\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    model = model.cuda()\n",
    "#    print('Using GPU.')\n",
    "\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        samples = 0\n",
    "        loss_sum = 0\n",
    "        correct_sum = 0\n",
    "        for j, batch in enumerate(dataloaders[phase]):\n",
    "            X,labels = batch\n",
    "            #print(labels)\n",
    "            #print(len(X),len(X[0]),len(X[1]))\n",
    "            #print(labels[None,...].double())\n",
    "            #X = torch.Tensor(X)\n",
    "            #print(X.shape)\n",
    "            #labels = 1-labels\n",
    "            if torch.cuda.is_available():\n",
    "                X = X.cuda()\n",
    "                labels = labels.cuda()\n",
    "                model = model.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                y = model(X[None,...].double())\n",
    "                #y = model(X.flatten().double())\n",
    "                #print(y,labels)\n",
    "                loss = criterion(\n",
    "                    y,\n",
    "                    #torch.clip(y,0,1), \n",
    "                    labels[None,...].double()\n",
    "                    #labels.double()\n",
    "                )\n",
    "                #print(loss.item())\n",
    "                #print(labels[None,...].double())\n",
    "                \n",
    "                #writer.add_scalar(\"Loss/train\", loss, i)\n",
    "\n",
    "                if phase == \"train\":\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                loss_sum += loss.item() * X.shape[0] # We need to multiple by batch size as loss is the mean loss of the samples in the batch\n",
    "                samples += X.shape[0]\n",
    "                num_corrects = torch.sum((y >= 0.5).float() == labels[0].float())\n",
    "                correct_sum += num_corrects\n",
    "                \n",
    "                # Print batch statistics every 50 batches\n",
    "                if j % 50 == 49 and phase == \"train\":\n",
    "                    print(\"{}:{} - loss: {}, acc: {}\".format(\n",
    "                        i + 1, \n",
    "                        j + 1, \n",
    "                        float(loss_sum) / float(samples), \n",
    "                        float(correct_sum) / float(samples)\n",
    "                    ))\n",
    "                \n",
    "        # Print epoch statistics\n",
    "        epoch_acc = float(correct_sum) / float(samples)\n",
    "        epoch_loss = float(loss_sum) / float(samples)\n",
    "        print(\"epoch: {} - {} loss: {}, {} acc: {}\".format(i + 1, phase, epoch_loss, phase, epoch_acc))\n",
    "        \n",
    "        # Deep copy the model\n",
    "        if phase == \"test\" and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, \"resnet50.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cd7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
